{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Ii069NmDFT8u"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 41.8 MB/s \n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting huggingface-hub\u003c1.0,\u003e=0.10.0\n","  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 71.7 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Collecting tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1\n","  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 56.0 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.10.0-\u003etransformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003etransformers) (3.0.9)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers) (3.10.0)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (1.24.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2022.9.24)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.7.0-py3-none-any.whl (451 kB)\n","\u001b[K     |████████████████████████████████| 451 kB 38.1 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: dill\u003c0.3.7 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: fsspec[http]\u003e=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.10.0)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Collecting xxhash\n","  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 73.9 MB/s \n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Collecting responses\u003c0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: pyarrow\u003e=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n","Requirement already satisfied: requests\u003e=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: huggingface-hub\u003c1.0.0,\u003e=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.11.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 67.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003edatasets) (22.1.0)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003edatasets) (1.8.1)\n","Requirement already satisfied: charset-normalizer\u003c3.0,\u003e=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003edatasets) (2.1.1)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003edatasets) (1.3.1)\n","Requirement already satisfied: async-timeout\u003c5.0,\u003e=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003edatasets) (4.0.2)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003edatasets) (6.0.2)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003edatasets) (0.13.0)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003edatasets) (1.3.3)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003edatasets) (4.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.2.0-\u003edatasets) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-\u003edatasets) (3.0.9)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (2022.9.24)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (2.10)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (1.24.3)\n","Collecting urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 68.9 MB/s \n","\u001b[?25hRequirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003edatasets) (3.10.0)\n","Requirement already satisfied: python-dateutil\u003e=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas-\u003edatasets) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas-\u003edatasets) (2022.6)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil\u003e=2.7.3-\u003epandas-\u003edatasets) (1.15.0)\n","Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.7.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n"]}],"source":["!pip install transformers\n","!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PIYgmK7JGApG"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ndair6dzFBl1"},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from transformers import DataCollatorWithPadding\n","from transformers import TrainingArguments, Trainer\n","from datasets import load_metric\n","import datetime\n","from torch import nn\n","from transformers import AutoConfig\n","from transformers import AutoModel\n","from tqdm.auto import tqdm\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_B13Be1POha"},"outputs":[],"source":["class CFG:\n","    str_now = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n","    basic_lr=1e-3\n","    train = True\n","    debug = False\n","    offline = False\n","    models_path = \"bert-base-uncased\"\n","    epochs = 50\n","    save_all_models = False\n","    apex = True\n","    print_freq = 20\n","    num_workers = 4\n","    model = \"bert-base-uncased\"\n","    loss_func = 'SmoothL1'\n","    scheduler = 'cosine'\n","    batch_scheduler = True\n","    num_cycles = 0.5\n","    num_warmup_steps = 0\n","    encoder_lr = 2e-5\n","    decoder_lr = 2e-5\n","    min_lr = 1e-6\n","    llrd = True\n","    layerwise_lr = 5e-5\n","    layerwise_lr_decay = 0.9\n","    layerwise_weight_decay = 0.01\n","    layerwise_adam_epsilon = 1e-6\n","    layerwise_use_bertadam = False\n","    #pooling\n","    pooling = 'mean' # mean, max, min, attention, weightedlayer\n","    layer_start = 4\n","    #init_weight\n","    init_weight = 'normal' # normal, xavier_uniform, xavier_normal, kaiming_uniform, kaiming_normal, orthogonal\n","    #re-init\n","    reinit = True\n","    reinit_n = 1\n","    #adversarial\n","    fgm = False\n","    awp = False\n","    adv_lr = 1\n","    adv_eps = 0.2\n","    unscale = False\n","    eps = 1e-6\n","    betas = (0.9, 0.999)\n","    max_len = 512\n","    weight_decay = 0.01\n","    gradient_accumulation_steps = 1\n","    max_grad_norm = 1000\n","    target_cols = ['EI', 'SN', 'TF', 'JP']\n","    seed = 42\n","    cv_seed = 42\n","    n_fold = 4\n","    trn_fold = list(range(n_fold))\n","    batch_size = 5\n","    n_targets = 4\n","    gpu_id = 0\n","    device = f'cuda:{gpu_id}'\n","cfg=CFG()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tcMZxYMsFBl4"},"outputs":[],"source":["\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification\n","tokenizer=BertTokenizer.from_pretrained(cfg.model)\n","#the dataset class for the first dataset, tokenized, and labeled\n","class Ds_EI(Dataset):\n","    def __init__(self, path, tokenizer, max_token_len=cfg.max_len):\n","        self.df = pd.read_csv(path).dropna()\n","        self.tokenizer=tokenizer\n","        self.max_token_len=max_token_len\n","        self.labelstrdicts={1:\"ESTJ\", 0:\"INFP\"}\n","        self.loc=0  #EI at index 0 in mbti\n","    def __len__(self):\n","        return (len(self.df))\n","    def __getitem__(self, index):\n","        item=self.df.iloc[index]\n","        text=item[\"post\"]\n","        type=item[\"type\"]\n","        labels=self.str2label(type)\n","        try:\n","          tokens=self.tokenizer(text,return_tensors=\"pt\", truncation=True, max_length=self.max_token_len, padding=\"max_length\")\n","        except:\n","          print(text)\n","          quit()\n","        return {\"input_ids\": torch.squeeze(tokens.input_ids), \"attention_mask\":torch.squeeze(tokens.attention_mask), \"labels\":labels}\n","    def str2label(self, string):\n","        letter=string[self.loc]\n","        if letter in \"ESTJ\":\n","            return 1.\n","        else:\n","            return 0.\n","    def label2str(self, label):\n","        return self.labelstrdicts[label][self.loc]\n","\n","class Ds_SN(Dataset):\n","    def __init__(self, path, tokenizer, max_token_len=cfg.max_len):\n","        self.df = pd.read_csv(path).dropna()\n","        self.tokenizer=tokenizer\n","        self.max_token_len=max_token_len\n","        self.labelstrdicts={1:\"ESTJ\", 0:\"INFP\"}\n","        self.loc=1  #EI at index 0 in mbti\n","    def __len__(self):\n","        return (len(self.df))\n","    def __getitem__(self, index):\n","        item=self.df.iloc[index]\n","        text=item[\"post\"]\n","        type=item[\"type\"]\n","        labels=self.str2label(type)\n","        try:\n","          tokens=self.tokenizer(text,return_tensors=\"pt\", truncation=True, max_length=self.max_token_len, padding=\"max_length\")\n","        except:\n","          print(text)\n","          quit()\n","        return {\"input_ids\": torch.squeeze(tokens.input_ids), \"attention_mask\":torch.squeeze(tokens.attention_mask), \"labels\":labels}\n","    def str2label(self, string):\n","        letter=string[self.loc]\n","        if letter in \"ESTJ\":\n","            return 1.\n","        else:\n","            return 0.\n","    def label2str(self, label):\n","        return self.labelstrdicts[label][self.loc]\n","\n","class Ds_TF(Dataset):\n","    def __init__(self, path, tokenizer, max_token_len=cfg.max_len):\n","        self.df = pd.read_csv(path).dropna()\n","        self.tokenizer=tokenizer\n","        self.max_token_len=max_token_len\n","        self.labelstrdicts={1:\"ESTJ\", 0:\"INFP\"}\n","        self.loc=2  #EI at index 0 in mbti\n","    def __len__(self):\n","        return (len(self.df))\n","    def __getitem__(self, index):\n","        item=self.df.iloc[index]\n","        text=item[\"post\"]\n","        type=item[\"type\"]\n","        labels=self.str2label(type)\n","        try:\n","          tokens=self.tokenizer(text,return_tensors=\"pt\", truncation=True, max_length=self.max_token_len, padding=\"max_length\")\n","        except:\n","          print(text)\n","          quit()\n","        return {\"input_ids\": torch.squeeze(tokens.input_ids), \"attention_mask\":torch.squeeze(tokens.attention_mask), \"labels\":labels}\n","    def str2label(self, string):\n","        letter=string[self.loc]\n","        if letter in \"ESTJ\":\n","            return 1.\n","        else:\n","            return 0.\n","    def label2str(self, label):\n","        return self.labelstrdicts[label][self.loc]\n","\n","class Ds_JP(Dataset):\n","    def __init__(self, path, tokenizer, max_token_len=cfg.max_len):\n","        self.df = pd.read_csv(path).dropna()\n","        self.tokenizer=tokenizer\n","        self.max_token_len=max_token_len\n","        self.labelstrdicts={1:\"ESTJ\", 0:\"INFP\"}\n","        self.loc=3  #EI at index 0 in mbti\n","    def __len__(self):\n","        return (len(self.df))\n","    def __getitem__(self, index):\n","        item=self.df.iloc[index]\n","        text=item[\"post\"]\n","        type=item[\"type\"]\n","        labels=self.str2label(type)\n","        try:\n","          tokens=self.tokenizer(text,return_tensors=\"pt\", truncation=True, max_length=self.max_token_len, padding=\"max_length\")\n","        except:\n","          print(text)\n","          quit()\n","        return {\"input_ids\": torch.squeeze(tokens.input_ids), \"attention_mask\":torch.squeeze(tokens.attention_mask), \"labels\":labels}\n","    def str2label(self, string):\n","        letter=string[self.loc]\n","        if letter in \"ESTJ\":\n","            return 1.\n","        else:\n","            return 0.\n","    def label2str(self, label):\n","        return self.labelstrdicts[label][self.loc]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ddPb2H0pFBl7"},"outputs":[],"source":["path=\"/content/drive/MyDrive/nlpproject/dataset2(sep).csv\"\n","#print(dataset[0])\n","data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n","def getdl(ds):\n","    total_len=len(ds)\n","    train_len=int(len(ds)*0.8)\n","    val_len=int((total_len-train_len)/2)\n","    test_len=total_len-train_len-val_len\n","    [train_ds, val_ds, test_ds]=torch.utils.data.random_split(ds, [train_len, val_len, test_len])\n","    #return (training dataloader, validation dataloader, test dataloader)\n","    return DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=data_collator), DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=data_collator), DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=data_collator)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Y7Vn2lUl15T"},"outputs":[],"source":["dataset_EI=Ds_EI(path, tokenizer)\n","train_dl_EI, val_dl_EI, test_dl_EI=getdl(dataset_EI)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qSy4KGQaFBl9"},"outputs":[],"source":["class myModel(nn.Module):\n","    def __init__(self, CFG, pretrained = True):\n","        super().__init__()\n","        self.CFG = CFG\n","        self.config = AutoConfig.from_pretrained(CFG.model, ouput_hidden_states = True)\n","        self.config.hidden_dropout = 0.\n","        self.config.hidden_dropout_prob = 0.\n","        self.config.attention_dropout = 0.\n","        self.config.attention_probs_dropout_prob = 0.\n","        self.config.max_length=self.CFG.max_len\n","        \n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(CFG.model, config=self.config)\n","        else:\n","            self.model = AutoModel(self.config)            \n","        self.fc1 = nn.Linear(self.config.hidden_size, 64)\n","        self.dropout = nn.Dropout(p=0.2, inplace=False)\n","        self.fc2 = nn.Linear(64, 1)\n","        self.sig = nn.Sigmoid()\n","        self._init_weights(self.fc1)\n","        self._init_weights(self.fc2)\n","        \n","        if 'bert-base' in CFG.model:\n","            self.model.embeddings.requires_grad_(False)\n","            self.model.encoder.layer[:11].requires_grad_(False)\n","            self.model.encoder.layer[11:].requires_grad_(True)\n","        \n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            if CFG.init_weight == 'normal':\n","                module.weight.data.normal_(mean = 0.0, std = self.config.initializer_range)\n","            elif CFG.init_weight == 'xavier_uniform':\n","                module.weight.data = nn.init.xavier_uniform_(module.weight.data)\n","            elif CFG.init_weight == 'xavier_normal':\n","                module.weight.data = nn.init.xavier_normal_(module.weight.data)\n","            elif CFG.init_weight == 'kaiming_uniform':\n","                module.weight.data = nn.init.kaiming_uniform_(module.weight.data)\n","            elif CFG.init_weight == 'kaiming_normal':\n","                module.weight.data = nn.init.kaiming_normal_(module.weight.data)\n","            elif CFG.init_weight == 'orthogonal':\n","                module.weight.data = nn.init.orthogonal_(module.weight.data)\n","                \n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            if CFG.init_weight == 'normal':\n","                module.weight.data.normal_(mean = 0.0, std = self.config.initializer_range)\n","            elif CFG.init_weight == 'xavier_uniform':\n","                module.weight.data = nn.init.xavier_uniform_(module.weight.data)\n","            elif CFG.init_weight == 'xavier_normal':\n","                module.weight.data = nn.init.xavier_normal_(module.weight.data)\n","            elif CFG.init_weight == 'kaiming_uniform':\n","                module.weight.data = nn.init.kaiming_uniform_(module.weight.data)\n","            elif CFG.init_weight == 'kaiming_normal':\n","                module.weight.data = nn.init.kaiming_normal_(module.weight.data)\n","            elif CFG.init_weight == 'orthogonal':\n","                module.weight.data = nn.init.orthogonal_(module.weight.data)\n","                \n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","    \n","    def forward(self, inputs):\n","        feature = self.model(input_ids=inputs[\"input_ids\"],attention_mask=inputs[\"attention_mask\"]).last_hidden_state[:,0]\n","        output = self.dropout(feature)\n","        output = self.fc1(output)\n","        output = self.dropout(output)\n","        output = self.fc2(output)\n","        output = self.sig(output)\n","        output = torch.squeeze(output)\n","        return output #2 number between 0 and 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4jmJV1hKhQt"},"outputs":[],"source":["def evaluate(labels, outputs):\n","  answers=(outputs\u003e=0.5)\n","  batch_size=len(labels)\n","  allcorrect=torch.sum(answers==labels)\n","  return allcorrect"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fJUdV0euFBl-"},"outputs":[],"source":["from torch.optim import lr_scheduler\n","from torch import nn\n","from torch.optim import Adam\n","from tqdm.notebook import tqdm\n","def train(train_ds, eval_ds, model, epochs, cfg, type):\n","    if torch.cuda.is_available():  \n","        dev = \"cuda:0\" \n","    else:  \n","        dev = \"cpu\" \n","    device = torch.device(dev)\n","    criterion = nn.BCELoss()\n","    optimizer = Adam(model.parameters(), lr=cfg.basic_lr)\n","    scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=500, \n","                                                                 eta_min=1e-10)\n","    model = model.to(device)\n","    criterion = criterion.to(device)\n","    \n","    for e in range(epochs):\n","        totaltrainloss=0\n","        for i,batch in enumerate(train_ds):\n","            #if i%100==0:\n","              #print(i)\n","            batch.to(device)\n","            labels=batch[\"labels\"]\n","            outputs=model(inputs=batch)\n","            bloss=criterion(outputs, labels)\n","            totaltrainloss+=bloss.item()\n","            model.zero_grad()\n","            bloss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","        totalevalloss=0\n","        totalcorrect=0\n","        totaldata=0\n","        with torch.no_grad():\n","            for batch in eval_ds:\n","                batch.to(device)\n","                labels=batch[\"labels\"]\n","                outputs=model(inputs=batch)\n","                eloss=criterion(outputs, labels).item()\n","                totalevalloss+=eloss\n","                totalcorrect+=evaluate(labels, outputs)\n","                totaldata+=len(labels)\n","        totalcorrect=(totalcorrect/(totaldata))\n","        print(\"probability that our prediction of \", type, \" is correct: \", totalcorrect)\n","        print(f'Epoch: {e+ 1} | Train Loss: {totaltrainloss / len(train_ds): .3f} | Val Loss: {totalevalloss / len(eval_ds): .3f}') "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCSoyiclG2w3"},"outputs":[],"source":["model=myModel(cfg, pretrained = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MtdE1ZeaFBl_"},"outputs":[],"source":["train(train_dl_EI, val_dl_EI, model, epochs=cfg.epochs, cfg=cfg, type=\"EI\")"]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"vscode":{"interpreter":{"hash":"47e86d731e077963188d400b641a1f5cee6401b89b8a1175acb1a082248e2517"}}},"nbformat":4,"nbformat_minor":0}