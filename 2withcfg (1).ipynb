{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Ii069NmDFT8u"},"outputs":[],"source":["!pip install transformers\n","!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PIYgmK7JGApG"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ndair6dzFBl1"},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from transformers import DataCollatorWithPadding\n","from transformers import TrainingArguments, Trainer\n","from datasets import load_metric\n","import datetime\n","from torch import nn\n","from transformers import AutoConfig\n","from transformers import AutoModel\n","from tqdm.auto import tqdm\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_B13Be1POha"},"outputs":[],"source":["class CFG:\n","    str_now = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n","    basic_lr=1e-3\n","    train = True\n","    debug = False\n","    offline = False\n","    models_path = \"bert-base-uncased\"\n","    epochs = 50\n","    save_all_models = False\n","    apex = True\n","    print_freq = 20\n","    num_workers = 4\n","    model = \"bert-base-uncased\"\n","    loss_func = 'SmoothL1'\n","    scheduler = 'cosine'\n","    batch_scheduler = True\n","    num_cycles = 0.5\n","    num_warmup_steps = 0\n","    encoder_lr = 2e-5\n","    decoder_lr = 2e-5\n","    min_lr = 1e-6\n","    llrd = True\n","    layerwise_lr = 5e-5\n","    layerwise_lr_decay = 0.9\n","    layerwise_weight_decay = 0.01\n","    layerwise_adam_epsilon = 1e-6\n","    layerwise_use_bertadam = False\n","    #pooling\n","    pooling = 'mean' # mean, max, min, attention, weightedlayer\n","    layer_start = 4\n","    #init_weight\n","    init_weight = 'normal' # normal, xavier_uniform, xavier_normal, kaiming_uniform, kaiming_normal, orthogonal\n","    #re-init\n","    reinit = True\n","    reinit_n = 1\n","    #adversarial\n","    fgm = False\n","    awp = False\n","    adv_lr = 1\n","    adv_eps = 0.2\n","    unscale = False\n","    eps = 1e-6\n","    betas = (0.9, 0.999)\n","    max_len = 50\n","    weight_decay = 0.01\n","    gradient_accumulation_steps = 1\n","    max_grad_norm = 1000\n","    target_cols = ['EI', 'SN', 'TF', 'JP']\n","    seed = 42\n","    cv_seed = 42\n","    n_fold = 4\n","    trn_fold = list(range(n_fold))\n","    batch_size = 50\n","    n_targets = 4\n","    gpu_id = 0\n","    device = f'cuda:{gpu_id}'\n","cfg=CFG()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tcMZxYMsFBl4"},"outputs":[],"source":["\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification\n","tokenizer=BertTokenizer.from_pretrained(cfg.model)\n","#the dataset class for the first dataset, tokenized, and labeled\n","class Ds1(Dataset):\n","    def __init__(self, path, tokenizer, max_token_len=cfg.max_len):\n","        self.df = pd.read_csv(path).dropna()\n","        self.tokenizer=tokenizer\n","        self.max_token_len=max_token_len\n","        self.labelstrdicts={1:\"ESTJ\", 0:\"INFP\"}\n","    def __len__(self):\n","        return (len(self.df))\n","    def __getitem__(self, index):\n","        item=self.df.iloc[index]\n","        text=item[\"post\"]\n","        type=item[\"type\"]\n","        labels=self.str2label(type)\n","        try:\n","          tokens=self.tokenizer(text,return_tensors=\"pt\", truncation=True, max_length=self.max_token_len, padding=\"max_length\")\n","        except:\n","          print(text)\n","          quit()\n","        return {\"input_ids\": torch.squeeze(tokens.input_ids), \"attention_mask\":torch.squeeze(tokens.attention_mask), \"labels\":labels}\n","    def str2label(self, string):\n","        label=[]\n","        for letter in string:\n","            if letter in \"ESTJ\":\n","                label.append(1.)\n","            else:\n","                label.append(0.)\n","        return label\n","    def label2str(self, label):\n","        string=[]\n","        for index,number in enumerate(label):\n","            string.append(self.labelstrdicts[number][index])\n","        return string\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ddPb2H0pFBl7"},"outputs":[],"source":["path=\"/content/drive/MyDrive/nlpproject/dataset2.csv\"\n","dataset=Ds1(path, tokenizer)\n","#print(dataset[0])\n","data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n","def getdl(ds):\n","    total_len=len(ds)\n","    train_len=int(len(ds)*0.8)\n","    val_len=int((total_len-train_len)/2)\n","    test_len=total_len-train_len-val_len\n","    [train_ds, val_ds, test_ds]=torch.utils.data.random_split(ds, [train_len, val_len, test_len])\n","    #return (training dataloader, validation dataloader, test dataloader)\n","    return DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=data_collator), DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=data_collator), DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=data_collator)\n","train_dl, val_dl, test_dl=getdl(dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qSy4KGQaFBl9"},"outputs":[],"source":["class myModel(nn.Module):\n","    def __init__(self, CFG, pretrained = True):\n","        super().__init__()\n","        self.CFG = CFG\n","        self.config = AutoConfig.from_pretrained(CFG.model, ouput_hidden_states = True)\n","        self.config.hidden_dropout = 0.\n","        self.config.hidden_dropout_prob = 0.\n","        self.config.attention_dropout = 0.\n","        self.config.attention_probs_dropout_prob = 0.\n","        self.config.max_length=self.CFG.max_len\n","        \n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(CFG.model, config=self.config)\n","        else:\n","            self.model = AutoModel(self.config)            \n","        self.fc = nn.Linear(self.config.hidden_size, self.CFG.n_targets)\n","        self.sig = nn.Sigmoid()\n","        self._init_weights(self.fc)\n","        \n","        if 'bert-base' in CFG.model:\n","            self.model.embeddings.requires_grad_(False)\n","            self.model.encoder.layer[:12].requires_grad_(False)\n","        \n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            if CFG.init_weight == 'normal':\n","                module.weight.data.normal_(mean = 0.0, std = self.config.initializer_range)\n","            elif CFG.init_weight == 'xavier_uniform':\n","                module.weight.data = nn.init.xavier_uniform_(module.weight.data)\n","            elif CFG.init_weight == 'xavier_normal':\n","                module.weight.data = nn.init.xavier_normal_(module.weight.data)\n","            elif CFG.init_weight == 'kaiming_uniform':\n","                module.weight.data = nn.init.kaiming_uniform_(module.weight.data)\n","            elif CFG.init_weight == 'kaiming_normal':\n","                module.weight.data = nn.init.kaiming_normal_(module.weight.data)\n","            elif CFG.init_weight == 'orthogonal':\n","                module.weight.data = nn.init.orthogonal_(module.weight.data)\n","                \n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            if CFG.init_weight == 'normal':\n","                module.weight.data.normal_(mean = 0.0, std = self.config.initializer_range)\n","            elif CFG.init_weight == 'xavier_uniform':\n","                module.weight.data = nn.init.xavier_uniform_(module.weight.data)\n","            elif CFG.init_weight == 'xavier_normal':\n","                module.weight.data = nn.init.xavier_normal_(module.weight.data)\n","            elif CFG.init_weight == 'kaiming_uniform':\n","                module.weight.data = nn.init.kaiming_uniform_(module.weight.data)\n","            elif CFG.init_weight == 'kaiming_normal':\n","                module.weight.data = nn.init.kaiming_normal_(module.weight.data)\n","            elif CFG.init_weight == 'orthogonal':\n","                module.weight.data = nn.init.orthogonal_(module.weight.data)\n","                \n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","    \n","    def forward(self, inputs):\n","        feature = self.model(input_ids=inputs[\"input_ids\"],attention_mask=inputs[\"attention_mask\"]).last_hidden_state[:,0]\n","        fourlogits = self.fc(feature)\n","        output = self.sig(fourlogits)\n","        return output #4 number between 0 and 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fJUdV0euFBl-"},"outputs":[],"source":["from torch.optim import lr_scheduler\n","from torch import nn\n","from torch.optim import Adam\n","from tqdm.notebook import tqdm\n","def train(train_ds, eval_ds, model, epochs, cfg):\n","    if torch.cuda.is_available():  \n","        dev = \"cuda:0\" \n","    else:  \n","        dev = \"cpu\" \n","    device = torch.device(dev)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = Adam(model.parameters(), lr=cfg.basic_lr)\n","    scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=500, \n","                                                                 eta_min=1e-6)\n","    model = model.to(device)\n","    criterion = criterion.to(device)\n","    \n","    for e in range(epochs):\n","        totaltrainloss=0\n","        for i,batch in enumerate(train_ds):\n","            #if i%100==0:\n","              #print(i)\n","            batch.to(device)\n","            labels=batch[\"labels\"]\n","            outputs=model(inputs=batch)\n","            bloss=criterion(outputs, labels)\n","            totaltrainloss+=bloss.item()\n","            model.zero_grad()\n","            bloss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","        totalevalloss=0\n","        totalcorrectrate=torch.tensor([0.,0.,0.,0.,0.])\n","        with torch.no_grad():\n","            for batch in eval_ds:\n","                batch.to(device)\n","                labels=batch[\"labels\"]\n","                outputs=model(inputs=batch)\n","                eloss=criterion(outputs, labels).item()\n","                totalevalloss+=eloss\n","                totalcorrectrate+=evaluate(labels, outputs)\n","        totalcorrectrate=(totalcorrectrate/len(eval_ds)).tolist()\n","        print(\"probability that our prediction is correct: \", totalcorrectrate[0])\n","        print(\"probability that our prediction of EI is correct: \", totalcorrectrate[1])\n","        print(\"probability that our prediction of SN is correct: \", totalcorrectrate[2])\n","        print(\"probability that our prediction of TF is correct: \", totalcorrectrate[3])\n","        print(\"probability that our prediction of JP is correct: \", totalcorrectrate[4])\n","        print(f'Epoch: {e+ 1} | Train Loss: {totaltrainloss / len(train_ds): .3f} | Val Loss: {totalevalloss / len(eval_ds): .3f}') "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbx3T9cRTWOu"},"outputs":[],"source":["def evaluate(labels, outputs):\n","  answers=torch.round(outputs)\n","  batch_size=len(labels)\n","  allcorrect=torch.sum(answers==labels)/(batch_size*4)\n","  sum=(torch.sum(answers==labels, dim=0)/batch_size).tolist()\n","  EIcorrect, SNcorrect, TFcorrect, JPcorrect= sum[0], sum[1], sum[2], sum[3]\n","  return torch.tensor([allcorrect, EIcorrect, SNcorrect, TFcorrect, JPcorrect])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCSoyiclG2w3"},"outputs":[],"source":["model=myModel(cfg, pretrained = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ONIKRdxIOv_"},"outputs":[],"source":["totalcorrectrate=torch.tensor([0.,0.,0.,0.,0.])\n","for batch in train_dl:\n","  labels=batch[\"labels\"]\n","  outputs=model(inputs=batch)\n","  totalcorrectrate+=evaluate(labels, outputs)\n","  break\n","print(totalcorrectrate)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"MtdE1ZeaFBl_","outputId":"29529e11-038b-4da4-a34f-96a07d87f052"},"outputs":[{"name":"stdout","output_type":"stream","text":["probability that our prediction is correct:  0.6122736930847168\n","probability that our prediction of EI is correct:  0.6811087727546692\n","probability that our prediction of SN is correct:  0.865970253944397\n","probability that our prediction of TF is correct:  0.4745592474937439\n","probability that our prediction of JP is correct:  0.4274559020996094\n","Epoch: 1 | Train Loss:  1.577 | Val Loss:  1.575\n","probability that our prediction is correct:  0.6150123476982117\n","probability that our prediction of EI is correct:  0.6716381907463074\n","probability that our prediction of SN is correct:  0.865970253944397\n","probability that our prediction of TF is correct:  0.4750882089138031\n","probability that our prediction of JP is correct:  0.44735535979270935\n","Epoch: 2 | Train Loss:  1.573 | Val Loss:  1.575\n","probability that our prediction is correct:  0.6177704334259033\n","probability that our prediction of EI is correct:  0.683929443359375\n","probability that our prediction of SN is correct:  0.8658946752548218\n","probability that our prediction of TF is correct:  0.47420644760131836\n","probability that our prediction of JP is correct:  0.44705304503440857\n","Epoch: 3 | Train Loss:  1.571 | Val Loss:  1.574\n"]}],"source":["train(train_dl, val_dl, model, epochs=cfg.epochs, cfg=cfg)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"vscode":{"interpreter":{"hash":"47e86d731e077963188d400b641a1f5cee6401b89b8a1175acb1a082248e2517"}}},"nbformat":4,"nbformat_minor":0}