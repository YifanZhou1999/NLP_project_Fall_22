{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GBa2geiof8Eu","executionInfo":{"status":"ok","timestamp":1669477528040,"user_tz":300,"elapsed":12927,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"f3ed920c-8ed0-4f46-ac33-1d5933ff3948"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import pandas as pd\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","torch.manual_seed(1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dX6ABpQdgFXi","executionInfo":{"status":"ok","timestamp":1669477530685,"user_tz":300,"elapsed":2648,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"3a6dc6fd-0be4-471f-c78f-7face9d59601"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fbc15327330>"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["foldername= \"/content/drive/My Drive/nlpproject/\"\n","path = foldername+\"dataset2.csv\""],"metadata":{"id":"dUuXs_MJgl3i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(path).dropna()\n","items = df.iloc[:][\"post\"]\n","old_word_to_ix = dict()\n","for i,sent in enumerate(items):\n","    for word in sent.split():\n","        if word not in old_word_to_ix: \n","            old_word_to_ix[word] = 1\n","        else:\n","            old_word_to_ix[word] = old_word_to_ix[word]+1\n","# Add unknown word \"UNK\" to word_to_ix.\n","# Doing this in case you find an unknown word in testing.\n","word_to_ix = {}\n","for word in old_word_to_ix:\n","  count=old_word_to_ix[word]\n","  if count>10:\n","    word_to_ix[word]=len(word_to_ix)\n","word_to_ix[\"UNK\"] = len(word_to_ix)\n","print(len(word_to_ix))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"flPDNf5El6xw","executionInfo":{"status":"ok","timestamp":1669477536189,"user_tz":300,"elapsed":5508,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"48042c15-8391-4627-8acd-7e9a9a9b4e0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["33842\n"]}]},{"cell_type":"code","source":["def prepare_sequence(seq, to_ix):\n","    idxs = [to_ix[w] if w in to_ix else to_ix[\"UNK\"] for w in seq]\n","    return torch.tensor(idxs, dtype=torch.long)"],"metadata":{"id":"y_P0yqSZm2xz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#the dataset class for the first dataset, tokenized, and labeled\n","class Ds(Dataset):\n","    def __init__(self, path, id_mbti, word_to_ix):\n","        self.df = pd.read_csv(path).dropna()\n","        self.labelstrdicts={1:\"ESTJ\", 0:\"INFP\"}\n","        self.id_mbti=0  \n","        self.word_to_ix=word_to_ix\n","    def __len__(self):\n","        return (len(self.df))\n","    def __getitem__(self, index):\n","        item=self.df.iloc[index]\n","        text=item[\"post\"]\n","        type=item[\"type\"]\n","        labels=self.str2label(type)\n","        try:\n","          tokens=text.split()\n","        except:\n","          print(text)\n","          quit()\n","        return {\"input_ids\": prepare_sequence(text,self.word_to_ix), \"labels\":labels}\n","    def str2label(self, string):\n","        letter=string[self.id_mbti]\n","        if letter in \"ESTJ\":\n","            return 1\n","        else:\n","            return 0\n","    def label2str(self, label):\n","        return self.labelstrdicts[label][self.id_mbti]"],"metadata":{"id":"ZGfZtxz-hjaL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.nn.utils.rnn import pad_sequence\n","def c(data): \n","    inputs = [torch.tensor(d['input_ids']) for d in data] \n","    labels = [d['labels'] for d in data]\n","    inputs = pad_sequence(inputs, batch_first=True)\n","    labels = torch.tensor(labels) \n","    return { \n","        'input_ids': inputs, \n","        'labels': labels\n","    }"],"metadata":{"id":"zq_hCMITC6Ko"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def getdl(ds):\n","    total_len=len(ds)\n","    train_len=int(len(ds)*0.9)\n","    val_len=int((total_len-train_len)/2)\n","    test_len=total_len-train_len-val_len\n","    [train_ds, val_ds, test_ds]=torch.utils.data.random_split(ds, [train_len, val_len, test_len])\n","    #return (training dataloader, validation dataloader, test dataloader)\n","    return len(train_ds), len(val_ds), len(test_ds), DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,collate_fn=c), DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False,collate_fn=c), DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=c)\n","    #return DataLoader(ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=data_collator)"],"metadata":{"id":"Pf8QlQbTjO_y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CFG():\n","  lr=1e-4\n","  min_lr=1e-10\n","  batch_size=1024\n","  epoch=1000\n","  embedding_dim=512\n","  hidden_dim=256\n","  drop=0.15\n","cfg=CFG()"],"metadata":{"id":"lvJJ283IknxZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ds=Ds(path,0,word_to_ix)\n","len_train, len_val, len_test, train_dl_EI,val_dl_EI, test_dl_EI=getdl(ds)"],"metadata":{"id":"8knuo9Mxjhrx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["l=[x['labels'] for x in ds]"],"metadata":{"id":"KLZiWkGYVwqk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_one=torch.count_nonzero(torch.tensor(l), dim=0)\n","num_zero=len(l)-num_one\n","\n","print(num_zero)\n","print(num_one)\n","LABEL_RATIO=torch.tensor([num_one/num_one, num_zero/num_one]).to(\"cuda\")\n","print(LABEL_RATIO)\n","print(len(ds))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VrCxhy3XWBPd","executionInfo":{"status":"ok","timestamp":1669477587552,"user_tz":300,"elapsed":3757,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"41d95657-66b2-4015-c8d5-3e631f1b4bb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(303868)\n","tensor(92655)\n","tensor([1.0000, 3.2796], device='cuda:0')\n","396523\n"]}]},{"cell_type":"code","source":["def evaluate(labels, outputs):\n","  answers=(torch.argmax(outputs, dim=1))\n","  allcorrect=torch.sum(answers==labels)\n","  return allcorrect"],"metadata":{"id":"vL59KdrLqYBI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Model(nn.Module):\n","\n","    def __init__(self, embedding_dim, hidden_dim, vocab_size, drop):\n","        super(Model, self).__init__()\n","        self.hidden_dim = hidden_dim\n","\n","        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n","\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n","        self.dropout= nn.Dropout(drop)\n","        self.relu = nn.ReLU()\n","        self.fc1= nn.Linear(hidden_dim, 128)\n","        self.fc2= nn.Linear(128, 32)\n","        self.fc3= nn.Linear(32, 8)\n","        self.fc_final= nn.Linear(8, 2)\n","    def forward(self, input_ids):\n","        o =  self.word_embeddings(input_ids)\n","        o,_ = self.lstm(o)\n","        o = o[:,-1]\n","        o = self.fc1(o)\n","        o = self.dropout(self.relu(o))\n","        o = self.fc2(o)\n","        o = self.dropout(self.relu(o))\n","        o = self.fc3(o)\n","        o = self.dropout(self.relu(o))\n","        o = self.fc_final(o)\n","        return o"],"metadata":{"id":"5RoIueXEkX1q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.optim import lr_scheduler\n","from torch.nn import CrossEntropyLoss\n","from torch import optim\n","from torch.optim import Adam\n","from tqdm.notebook import tqdm\n","def train(train_ds, eval_ds, model, epochs, cfg, type, lr, loss=None):\n","    if torch.cuda.is_available():  \n","        dev = \"cuda:0\" \n","    else:  \n","        dev = \"cpu\" \n","    device = torch.device(dev)\n","    model = model.to(device)\n","\n","    #weights=torch.tensor([1., 3.]).cuda()\n","    #criterion = nn.MSELoss()\n","    criterion = CrossEntropyLoss(weight=LABEL_RATIO)\n","    #criterion = CrossEntropyLoss()\n","\n","    criterion.to(device)\n","    #criterion = loss\n","    \n","    optimizer = Adam(model.parameters(), lr=lr)\n","    #optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n","    scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=500, eta_min=1e-15)\n","    #scheduler= optim.lr_scheduler.ExponentialLR(optimizer, 0.99, last_epoch=- 1, verbose=False)\n","        \n","    totalevalloss=0\n","    totalcorrect=0\n","    totaldata=0\n","    with torch.no_grad():\n","        model.eval()\n","        for batch in eval_ds:\n","            blabels=batch[\"labels\"].to(device)\n","            input_ids=batch[\"input_ids\"].to(device)\n","            outputs=model(input_ids=input_ids)\n","            eloss=criterion(outputs, blabels).item()\n","            totalevalloss+=eloss\n","            totalcorrect+=evaluate(blabels, outputs)\n","            totaldata+=len(blabels)\n","    totalcorrect_rate=(totalcorrect/(totaldata))\n","    print(\"probability that our prediction of \", type, \" is correct: \", totalcorrect_rate)\n","    #print(f'Initial Val Loss: {totalevalloss / len(eval_ds): .3f} ' ) \n","    print(f'Initial Val Loss: {totalevalloss / len(eval_ds): .3f} | current lr: {scheduler.get_last_lr()}' ) \n","    \n","    for e in range(epochs):\n","        totaltrainloss=0\n","        totaltraincorrect=0\n","        totaltraindata=0\n","        for i,batch in enumerate(train_ds):\n","            model.train()\n","            optimizer.zero_grad()\n","\n","            blabels=batch[\"labels\"].to(device)\n","            input_ids=batch[\"input_ids\"].to(device)\n","            outputs=model(input_ids=input_ids)\n","\n","            bloss=criterion(outputs, blabels)\n","            bloss.backward()\n","            optimizer.step()\n","            totaltrainloss+=bloss.item()\n","            totaltraincorrect+=evaluate(blabels, outputs)\n","            totaltraindata+=len(blabels)\n","        scheduler.step()\n","        totalevalloss=0\n","        totalcorrect=0\n","        totaldata=0\n","        with torch.no_grad():\n","            model.eval()\n","            for batch in eval_ds:\n","                blabels=batch[\"labels\"].to(device)\n","                input_ids=batch[\"input_ids\"].to(device)\n","                outputs=model(input_ids=input_ids)\n","                eloss=criterion(outputs, blabels).item()\n","                totalevalloss+=eloss\n","                totalcorrect+=evaluate(blabels, outputs)\n","                totaldata+=len(blabels)\n","        totalcorrect_rate=(totalcorrect/(totaldata))\n","        totaltraincorrect_rate = (totaltraincorrect/(totaltraindata))\n","        print(\"probability that our prediction of \", type, \" is correct: \", totalcorrect_rate)\n","        print(\"probability that our prediction of \", type, \" is correct in training dataset: \", totaltraincorrect_rate)\n","        #print(f'Epoch: {e+ 1} | Train Loss: {totaltrainloss / len(train_ds): .8f} | Val Loss: {totalevalloss / len(eval_ds): .3f}' ) \n","        print(f'Epoch: {e+ 1} | Train Loss: {totaltrainloss / len(train_ds): .8f} | Val Loss: {totalevalloss / len(eval_ds): .8f} | current lr: {scheduler.get_last_lr()}' ) "],"metadata":{"id":"zKOVNHF095SJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Model(cfg.embedding_dim, cfg.hidden_dim, len(word_to_ix),drop=cfg.drop)\n","print(model)\n","train(train_dl_EI, val_dl_EI, model, epochs=cfg.epoch, cfg=cfg, type=\"EI\", lr=1e-3, loss=None)"],"metadata":{"id":"dCnAufuk97OO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"47e56e79-86a0-4498-d20a-b165d622d217"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Model(\n","  (word_embeddings): Embedding(33842, 256)\n","  (lstm): LSTM(256, 512)\n","  (dropout): Dropout(p=0.15, inplace=False)\n","  (relu): ReLU()\n","  (fc1): Linear(in_features=512, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=32, bias=True)\n","  (fc3): Linear(in_features=32, out_features=8, bias=True)\n","  (fc_final): Linear(in_features=8, out_features=2, bias=True)\n",")\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["probability that our prediction of  EI  is correct:  tensor(0.2363, device='cuda:0')\n","Initial Val Loss:  0.717 | current lr: [0.001]\n"]}]},{"cell_type":"code","source":["criterion = CrossEntropyLoss()\n","model = Model(cfg.embedding_dim, cfg.hidden_dim, len(word_to_ix))\n","model.to(\"cuda\")\n","print(model)\n","with torch.no_grad():\n","  device=\"cuda\"\n","  for batch in train_dl_EI:\n","    print(batch)\n","    blabels=batch[\"labels\"].to(device)\n","    print(blabels)\n","    input_ids=batch[\"input_ids\"].to(device)\n","    print(input_ids)\n","    outputs=model(input_ids=input_ids)\n","    print(outputs)\n","    print(blabels)\n","    eloss=criterion(outputs, blabels).item()\n","    break"],"metadata":{"id":"ue_ge-uYxALn"},"execution_count":null,"outputs":[]}]}