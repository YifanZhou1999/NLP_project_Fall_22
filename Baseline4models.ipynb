{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec9ea621",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f56e05",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "365b4708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.24.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "torch.manual_seed(1)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b6f0e1",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39840b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define default Tokenizer\n",
    "defaultTokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# Parameters: Path to dataset csv, tokenizer object, maximum token length\n",
    "# Label_id is a number 0-4, indicate which set of EI/SN/TF/JP will be the label\n",
    "class Dset(Dataset):\n",
    "    # Constructor\n",
    "    def __init__(self, path, label_id, tokenizer=defaultTokenizer, max_token_len=500):\n",
    "        # Initialize some variables\n",
    "        self.df = pd.read_csv(path).dropna()\n",
    "        self.label_id = label_id\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "        \n",
    "        # in order to convert MBTI labels into numbers\n",
    "        self.labelstrdicts={1:\"ESTJ\", 0:\"INFP\"}\n",
    "    \n",
    "    # Override __len__ \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    # Override __getitem__\n",
    "    def __getitem__(self, index):\n",
    "        # get dataframe row\n",
    "        item = self.df.iloc[index]\n",
    "        # get the text and label from dataframe row\n",
    "        text = item[\"post\"]\n",
    "        ptype = item[\"type\"] # Personality Type\n",
    "        # and for labels, turn them into list of numbers (1/0s)\n",
    "        labels = self.str2label(ptype)[self.label_id]\n",
    "        \n",
    "        # Now try tokenize with the BERT Tokenizer\n",
    "        try:\n",
    "          tokens=self.tokenizer(text,return_tensors=\"pt\", truncation=True, max_length=self.max_token_len, padding=\"max_length\")\n",
    "            # return_tensors -> Return \"pt\" pytorch \"torch.Tensor\" objects instead of python int list\n",
    "            # truncation -> \"true\": Truncate to maximum length specified with argument \"max_length\"\n",
    "            # padding -> \"max_length\": Pad to a maximum length specified witht eh argument \"max_length\" \n",
    "        except:\n",
    "          print(text)\n",
    "          quit()\n",
    "        return {\"input_ids\": torch.squeeze(tokens.input_ids), \\\n",
    "                \"attention_mask\":torch.squeeze(tokens.attention_mask), \\\n",
    "                \"labels\":labels}\n",
    "    \n",
    "    # Auxiliary Functions\n",
    "    def str2label(self, string):\n",
    "        label=[]\n",
    "        for letter in string:\n",
    "            if letter in \"ESTJ\":\n",
    "                label.append(1)\n",
    "            else:\n",
    "                label.append(0)\n",
    "        return label\n",
    "    def label2str(self, label):\n",
    "        string=[]\n",
    "        for index,number in enumerate(label):\n",
    "            string.append(self.labelstrdicts[number][index])\n",
    "        return string\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a477dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to automatically split and return train/validation/test dataset object\n",
    "data_collator=DataCollatorWithPadding(tokenizer=defaultTokenizer)\n",
    "def getdl(ds, batch_size):\n",
    "    total_len=len(ds)\n",
    "    train_len=int(len(ds)*0.8)\n",
    "    val_len=int((total_len-train_len)/2)\n",
    "    test_len=total_len-train_len-val_len\n",
    "    [train_ds, val_ds, test_ds]=torch.utils.data.random_split(ds, [train_len, val_len, test_len])\n",
    "    #return (training dataloader, validation dataloader, test dataloader)\n",
    "    return DataLoader(train_ds, batch_size=batch_size, shuffle=False, collate_fn=data_collator),\\\n",
    "        DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=data_collator),\\\n",
    "        DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b98462f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now getting all three datasets at once\n",
    "path=\"./dataset2(sep).csv\"\n",
    "dataset_EI_all=Dset(path, 0) # use default tokenizer\n",
    "dataset_SN_all=Dset(path, 1)\n",
    "dataset_TF_all=Dset(path, 2)\n",
    "dataset_JP_all=Dset(path, 3)\n",
    "# Split into three dataloaders\n",
    "train_dl_EI, val_dl_EI, test_dl_EI=getdl(dataset_EI_all, batch_size=50)\n",
    "train_dl_SN, val_dl_SN, test_dl_SN=getdl(dataset_SN_all, batch_size=50)\n",
    "train_dl_TF, val_dl_TF, test_dl_TF=getdl(dataset_TF_all, batch_size=50)\n",
    "train_dl_JP, val_dl_JP, test_dl_JP=getdl(dataset_JP_all, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f3f84b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 500])\n",
      "tensor([[  101,  2667,  2000,  ...,  1037,  6180,   102],\n",
      "        [  101,  1045,  2066,  ...,  2004,  2115,   102],\n",
      "        [  101,  1005, 21862,  ..., 20919, 16078,   102],\n",
      "        ...,\n",
      "        [  101,  1005,  7632,  ...,  2488,  2005,   102],\n",
      "        [  101,  1005,  1034,  ...,  4557,  1006,   102],\n",
      "        [  101,  1005,  2002,  ...,  1005,  1049,   102]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Test Print\n",
    "for i,batch in enumerate(train_dl_EI):\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    print(batch[\"input_ids\"])\n",
    "    print(batch[\"labels\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad89a108",
   "metadata": {},
   "source": [
    "## Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c15634",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f7fda59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad445aa6f0294d11a11c4881be8cb86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163950e2d98a4b5680e6d4467f28e0d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.4.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.4.attention.self.key.weight', 'pooler.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'classifier.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'classifier.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.4.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.4.attention.self.key.weight', 'pooler.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'classifier.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'classifier.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.4.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.4.attention.self.key.weight', 'pooler.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'classifier.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'classifier.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.4.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.4.attention.self.key.weight', 'pooler.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'classifier.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'classifier.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "baselineModel_EI = BertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    "    return_dict=False\n",
    ")\n",
    "baselineModel_SN = BertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    "    return_dict=False\n",
    ")\n",
    "baselineModel_TF = BertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    "    return_dict=False\n",
    ")\n",
    "baselineModel_JP = BertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    "    return_dict=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1c78168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect Model Structure\n",
    "baselineModel_EI.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26204a5e",
   "metadata": {},
   "source": [
    "## Loss and Optimizer (moved to training function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "# Optimizer\n",
    "#optimizer = optim.Adam(baselineModel.parameters(), lr=1e-5)\n",
    "# Scheduler\n",
    "#scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=500, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c944672d",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe9053aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_tp(preds, labels):\n",
    "  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
    "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fp(preds, labels):\n",
    "  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
    "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_tn(preds, labels):\n",
    "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
    "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fn(preds, labels):\n",
    "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
    "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_metrics(preds, labels):\n",
    "  '''\n",
    "  Returns the following metrics:\n",
    "    - accuracy    = (TP + TN) / N\n",
    "    - precision   = TP / (TP + FP)\n",
    "    - recall      = TP / (TP + FN)\n",
    "    - specificity = TN / (TN + FP)\n",
    "  '''\n",
    "  preds = np.argmax(preds, axis = 1).flatten()\n",
    "  labels = labels.flatten()\n",
    "  tp = b_tp(preds, labels)\n",
    "  tn = b_tn(preds, labels)\n",
    "  fp = b_fp(preds, labels)\n",
    "  fn = b_fn(preds, labels)\n",
    "  b_accuracy = (tp + tn) / len(labels)\n",
    "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
    "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
    "  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
    "  return b_accuracy, b_precision, b_recall, b_specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c8f052",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89500b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_ds, eval_ds, model, epochs):\n",
    "    dev = \"mps:0\"\n",
    "    device = torch.device(dev)\n",
    "    \n",
    "    # Loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "    # Scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=500, eta_min=1e-6)\n",
    "    \n",
    "    # Setup on GPU\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    \n",
    "    for e in range(1, epochs+1):\n",
    "        print(f'Epoch: {e}')\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Tracking variables\n",
    "        tr_loss = 0 # Training Loss\n",
    "        nb_tr_examples = 0 # number of training examples\n",
    "        nb_tr_steps = 0 # number of training examples\n",
    "        \n",
    "        print(f'Total Step: {len(train_ds)}')\n",
    "        \n",
    "        for step, batch in enumerate(train_ds):\n",
    "            print(f'Training Step {step}', end='\\r')\n",
    "            #batch = tuple(t.to(device) for t in batch)\n",
    "            batch.to(device)\n",
    "            b_input_ids = batch[\"input_ids\"]\n",
    "            b_input_mask = batch[\"attention_mask\"]\n",
    "            b_labels = batch[\"labels\"]\n",
    "            optimizer.zero_grad()\n",
    "            # Forward Pass\n",
    "            train_output = model(b_input_ids, \n",
    "                                 token_type_ids = None,\n",
    "                                 attention_mask = b_input_mask, \n",
    "                                 labels = b_labels)\n",
    "\n",
    "            # Backward Pass\n",
    "            bloss = criterion(train_output[1], b_labels)\n",
    "            bloss.backward()\n",
    "            #train_output.loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Update Tracking variables\n",
    "            #tr_loss += train_output.loss.item()\n",
    "            tr_loss += bloss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "        \n",
    "        # Validation ==================================\n",
    "        print(\"Validation....\")\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        # Tracking variables\n",
    "        val_accuracy = []\n",
    "        val_precision = []\n",
    "        val_recall = []\n",
    "        val_specificity = []\n",
    "        \n",
    "        print(f'Valid Total Steps: {len(eval_ds)}')\n",
    "        for batch in eval_ds:\n",
    "            #batch = tuple(t.to(device) for t in batch)\n",
    "            batch.to(device)\n",
    "            b_input_ids = batch[\"input_ids\"]\n",
    "            b_input_mask = batch[\"attention_mask\"]\n",
    "            b_labels = batch[\"labels\"]\n",
    "            with torch.no_grad():\n",
    "                # Forward Pass\n",
    "                eval_output = model(b_input_ids, \n",
    "                                 token_type_ids = None,\n",
    "                                 attention_mask = b_input_mask)\n",
    "            logits = eval_output.logits.detatch().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            \n",
    "            # Calculate Validation metrics\n",
    "            b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
    "            val_accuracy.append(b_accuracy)\n",
    "            # Update the rest tracking variables only when not zero ('nan')\n",
    "            if b_precision != 'nan': val_precision.append(b_precision)\n",
    "            if b_recall != 'nan': val_recall.append(b_recall)\n",
    "            if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
    "        print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
    "        print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
    "        print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
    "        print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
    "        print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08c9dd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Total Step: 139\n",
      "Validation....138\n",
      "Valid Total Steps: 18\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Execute Training:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl_EI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl_EI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaselineModel_EI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [9], line 77\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_ds, eval_ds, model, epochs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# Forward Pass\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     eval_output \u001b[38;5;241m=\u001b[39m model(b_input_ids, \n\u001b[1;32m     75\u001b[0m                      token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     76\u001b[0m                      attention_mask \u001b[38;5;241m=\u001b[39m b_input_mask)\n\u001b[0;32m---> 77\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43meval_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[38;5;241m.\u001b[39mdetatch()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     78\u001b[0m label_ids \u001b[38;5;241m=\u001b[39m b_labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Calculate Validation metrics\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "# Execute Training:\n",
    "epochs = 5\n",
    "train(train_dl_EI, val_dl_EI, baselineModel_EI, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c4ec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Training:\n",
    "epochs = 5\n",
    "train(train_dl_SN, val_dl_SN, baselineModel_SN, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Training:\n",
    "epochs = 5\n",
    "train(train_dl_TF, val_dl_TF, baselineModel_TF, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c886283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Training:\n",
    "epochs = 5\n",
    "train(train_dl_JP, eval_dl_JP, baselineModel_JP, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
