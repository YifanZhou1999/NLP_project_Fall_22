{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXJrIVqiHlED",
        "outputId": "22286ddf-e453-44a8-cbcf-1222a94214c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPIH-Qinj0fS"
      },
      "outputs": [],
      "source": [
        "df = pd.read_pickle(\"/content/drive/My Drive/Project/EIbinary.pkl\") "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    # line = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    line = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
        "    return ' '.join(map(str,line))\n",
        "\n",
        "df['post'] = df.post.apply(lemmatize_text)\n",
        "# df['post'] = df['post'].str.lower()"
      ],
      "metadata": {
        "id": "zWwWNdCsf1LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyDxWvA65lS4"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "def cleanReview(subject):\n",
        "    beau = BeautifulSoup(subject)\n",
        "    newSubject = beau.get_text()\n",
        "    newSubject = newSubject.replace(\"\\\\\", \"\").replace(\"\\'\", \"\").replace('/', '').replace('\"', '').replace(',', '').replace('.', '').replace('?', '').replace('(', '').replace(')', '').replace('|||', '')\n",
        "    newSubject = newSubject.strip().split(\" \")\n",
        "    newSubject = [word.lower() for word in newSubject]\n",
        "    newSubject = \" \".join(newSubject)\n",
        "    \n",
        "    return newSubject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "Gy8Qyw0B5oCK",
        "outputId": "12296290-6ccc-4569-96c8-6ea15b6aed3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/bs4/__init__.py:270: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bs4/__init__.py:270: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bs4/__init__.py:270: UserWarning: \"b'///'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d1e3829338a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_x\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleanReview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4356\u001b[0m         \"\"\"\n\u001b[0;32m-> 4357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4359\u001b[0m     def _reduce(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-73479758e98c>\u001b[0m in \u001b[0;36mcleanReview\u001b[0;34m(subject)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcleanReview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mbeau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnewSubject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeau\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnewSubject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewSubject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'('\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m')'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'|||'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mParserRejectedMarkup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_x = df['post'].apply(cleanReview)\n",
        "df=pd.concat([train_x, df['type']], axis=0) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEmSCrX05yhD"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"/content/drive/My Drive/Project/wordEmbedding.txt\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Word2Vec and Bi-LSTM"
      ],
      "metadata": {
        "id": "0EGV8XFEA-8u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kobBUxwt57nv"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "sentences = word2vec.LineSentence(\"/content/drive/MyDrive/Project/wordEmbedding.txt\")\n",
        "# model = gensim.models.Word2Vec(sentences, size=1000, sg=1, iter=8)  \n",
        "# model.wv.save_word2vec_format(\"/content/drive/My Drive/Project/word2Vec\" + \".bin\", binary=True) \n",
        "wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/Project/word2Vec.bin\", binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7QZEzzuTiaH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import json\n",
        "\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from math import sqrt\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJfy6LPITkGA"
      },
      "outputs": [],
      "source": [
        "class TrainingConfig(object):\n",
        "    epoches = 5\n",
        "    evaluateEvery = 100\n",
        "    checkpointEvery = 100\n",
        "    learningRate = 0.001\n",
        "    \n",
        "class ModelConfig(object):\n",
        "    embeddingSize = 1000\n",
        "    hiddenSizes = [256, 256]\n",
        "    dropoutKeepProb = 0.5\n",
        "    l2RegLambda = 0.0\n",
        "    \n",
        "class Config(object):\n",
        "    sequenceLength = 200 \n",
        "    batchSize = 64\n",
        "    dataSource = \"/content/drive/MyDrive/Project/EIonly.csv\"\n",
        "    stopWordSource = \"/content/drive/MyDrive/Project/english.txt\"\n",
        "    numClasses = 1  \n",
        "    rate = 0.9  \n",
        "    training = TrainingConfig()\n",
        "    model = ModelConfig()\n",
        "    \n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvLyiQJsUp7b"
      },
      "outputs": [],
      "source": [
        "class Dataset(object):\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self._dataSource = config.dataSource\n",
        "        self._stopWordSource = config.stopWordSource  \n",
        "        self._sequenceLength = config.sequenceLength  \n",
        "        self._embeddingSize = config.model.embeddingSize\n",
        "        self._batchSize = config.batchSize\n",
        "        self._rate = config.rate\n",
        "        self._stopWordDict = {}\n",
        "        self.trainReviews = []\n",
        "        self.trainLabels = []\n",
        "        self.evalReviews = []\n",
        "        self.evalLabels = []\n",
        "        self.wordEmbedding = None\n",
        "        self.labelList = []\n",
        "        \n",
        "    def _readData(self, filePath):\n",
        "        df = pd.read_csv(filePath)\n",
        "        labels = df[\"type\"].tolist()\n",
        "        review = df[\"post\"].tolist()\n",
        "        reviews = [line.strip().split() for line in review]\n",
        "        return reviews, labels\n",
        "    \n",
        "    def _labelToIndex(self, labels, label2idx):\n",
        "        labelIds = [label2idx[label] for label in labels]\n",
        "        return labelIds\n",
        "    \n",
        "    def _wordToIndex(self, reviews, word2idx):\n",
        "        reviewIds = [[word2idx.get(item, word2idx[\"UNK\"]) for item in review] for review in reviews]\n",
        "        return reviewIds\n",
        "        \n",
        "    def _genTrainEvalData(self, x, y, word2idx, rate):\n",
        "        reviews = []\n",
        "        for review in x:\n",
        "            if len(review) >= self._sequenceLength:\n",
        "                reviews.append(review[:self._sequenceLength])\n",
        "            else:\n",
        "                reviews.append(review + [word2idx[\"PAD\"]] * (self._sequenceLength - len(review)))\n",
        "        trainIndex = int(len(x) * rate)\n",
        "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
        "        trainLabels = np.array(y[:trainIndex], dtype=\"float32\")\n",
        "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
        "        evalLabels = np.array(y[trainIndex:], dtype=\"float32\")\n",
        "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
        "        \n",
        "    def _genVocabulary(self, reviews, labels):\n",
        "        allWords = [word for review in reviews for word in review]\n",
        "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
        "        wordCount = Counter(subWords) \n",
        "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
        "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
        "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
        "        self.wordEmbedding = wordEmbedding\n",
        "        word2idx = dict(zip(vocab, list(range(len(vocab)))))\n",
        "        uniqueLabel = list(set(labels))\n",
        "        label2idx = dict(zip(uniqueLabel, list(range(len(uniqueLabel)))))\n",
        "        self.labelList = list(range(len(uniqueLabel)))\n",
        "        with open(\"/content/drive/MyDrive/Project/wordJson/word2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(word2idx, f)\n",
        "        with open(\"/content/drive/MyDrive/Project/wordJson/label2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(label2idx, f)\n",
        "        return word2idx, label2idx\n",
        "            \n",
        "    def _getWordEmbedding(self, words):\n",
        "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/Project/word2Vec.bin\", binary=True)\n",
        "        vocab = []\n",
        "        wordEmbedding = []\n",
        "        vocab.append(\"PAD\")\n",
        "        vocab.append(\"UNK\")\n",
        "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
        "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
        "        for word in words:\n",
        "            try:\n",
        "                vector = wordVec.wv[word]\n",
        "                vocab.append(word)\n",
        "                wordEmbedding.append(vector)\n",
        "            except:\n",
        "                continue        \n",
        "        return vocab, np.array(wordEmbedding)\n",
        "    \n",
        "    def _readStopWord(self, stopWordPath):\n",
        "        with open(stopWordPath, \"r\") as f:\n",
        "            stopWords = f.read()\n",
        "            stopWordList = stopWords.splitlines()\n",
        "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
        "            \n",
        "    def dataGen(self):\n",
        "        self._readStopWord(self._stopWordSource)\n",
        "        reviews, labels = self._readData(self._dataSource)\n",
        "        word2idx, label2idx = self._genVocabulary(reviews, labels)\n",
        "        labelIds = self._labelToIndex(labels, label2idx)\n",
        "        reviewIds = self._wordToIndex(reviews, word2idx)\n",
        "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviewIds, labelIds, word2idx, self._rate)\n",
        "        self.trainReviews = trainReviews\n",
        "        self.trainLabels = trainLabels\n",
        "        self.evalReviews = evalReviews\n",
        "        self.evalLabels = evalLabels\n",
        "        \n",
        "data = Dataset(config)\n",
        "data.dataGen()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TM2I_IWIW7wC"
      },
      "outputs": [],
      "source": [
        "def nextBatch(x, y, batchSize):    \n",
        "    perm = np.arange(len(x))\n",
        "    np.random.shuffle(perm)\n",
        "    x = x[perm]\n",
        "    y = y[perm]\n",
        "    numBatches = len(x) // batchSize\n",
        "    for i in range(numBatches):\n",
        "        start = i * batchSize\n",
        "        end = start + batchSize\n",
        "        batchX = np.array(x[start: end], dtype=\"int64\")\n",
        "        batchY = np.array(y[start: end], dtype=\"float32\")\n",
        "        yield batchX, batchY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7zjE5aeW-CQ"
      },
      "outputs": [],
      "source": [
        "class BiLSTM(object):\n",
        "    def __init__(self, config, wordEmbedding):\n",
        "        self.inputX = tf.compat.v1.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
        "        self.inputY = tf.compat.v1.placeholder(tf.int32, [None], name=\"inputY\")\n",
        "        self.dropoutKeepProb = tf.compat.v1.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
        "        l2Loss = tf.constant(0.0)\n",
        "        with tf.name_scope(\"embedding\"):\n",
        "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
        "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
        "            \n",
        "        with tf.name_scope(\"Bi-LSTM\"):\n",
        "            for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
        "                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n",
        "                    lstmFwCell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
        "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
        "                    lstmBwCell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
        "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
        "\n",
        "                    outputs, self.current_state = tf.compat.v1.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell, \n",
        "                                                                                  self.embeddedWords, dtype=tf.float32,\n",
        "                                                                                  scope=\"bi-lstm\" + str(idx))\n",
        "                    self.embeddedWords = tf.concat(outputs, 2)\n",
        "        \n",
        "        finalOutput = self.embeddedWords[:, 0, :]\n",
        "        outputSize = config.model.hiddenSizes[-1] * 2 \n",
        "        output = tf.reshape(finalOutput, [-1, outputSize])  \n",
        "        \n",
        "        with tf.name_scope(\"output\"):\n",
        "            outputW = tf.compat.v1.get_variable(\n",
        "                \"outputW\",\n",
        "                shape=[outputSize, config.numClasses],\n",
        "                initializer=tf.keras.initializers.glorot_normal)\n",
        "            \n",
        "            outputB= tf.Variable(tf.constant(0.1, shape=[config.numClasses]), name=\"outputB\")\n",
        "            l2Loss += tf.nn.l2_loss(outputW)\n",
        "            l2Loss += tf.nn.l2_loss(outputB)\n",
        "            self.logits = tf.compat.v1.nn.xw_plus_b(output, outputW, outputB, name=\"logits\")\n",
        "            if config.numClasses == 1:\n",
        "                self.predictions = tf.cast(tf.greater_equal(self.logits, 0.0), tf.float32, name=\"predictions\")\n",
        "            elif config.numClasses > 1:\n",
        "                self.predictions = tf.argmax(self.logits, axis=-1, name=\"predictions\")\n",
        "        \n",
        "        with tf.name_scope(\"loss\"):    \n",
        "            if config.numClasses == 1:\n",
        "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=tf.cast(tf.reshape(self.inputY, [-1, 1]), \n",
        "                                                                                                    dtype=tf.float32))\n",
        "            elif config.numClasses > 1:\n",
        "                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.inputY)\n",
        "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "In-swnEjXCLH"
      },
      "outputs": [],
      "source": [
        "def mean(item: list) -> float:\n",
        "    res = sum(item) / len(item) if len(item) > 0 else 0\n",
        "    return res\n",
        "\n",
        "\n",
        "def accuracy(pred_y, true_y):\n",
        "    if isinstance(pred_y[0], list):\n",
        "        pred_y = [item[0] for item in pred_y]\n",
        "    corr = 0\n",
        "    for i in range(len(pred_y)):\n",
        "        if pred_y[i] == true_y[i]:\n",
        "            corr += 1\n",
        "    acc = corr / len(pred_y) if len(pred_y) > 0 else 0\n",
        "    return acc\n",
        "\n",
        "\n",
        "def binary_precision(pred_y, true_y, positive=1):\n",
        "    corr = 0\n",
        "    pred_corr = 0\n",
        "    for i in range(len(pred_y)):\n",
        "        if pred_y[i] == positive:\n",
        "            pred_corr += 1\n",
        "            if pred_y[i] == true_y[i]:\n",
        "                corr += 1\n",
        "\n",
        "    prec = corr / pred_corr if pred_corr > 0 else 0\n",
        "    return prec\n",
        "\n",
        "\n",
        "def binary_recall(pred_y, true_y, positive=1):\n",
        "    corr = 0\n",
        "    true_corr = 0\n",
        "    for i in range(len(pred_y)):\n",
        "        if true_y[i] == positive:\n",
        "            true_corr += 1\n",
        "            if pred_y[i] == true_y[i]:\n",
        "                corr += 1\n",
        "\n",
        "    rec = corr / true_corr if true_corr > 0 else 0\n",
        "    return rec\n",
        "\n",
        "\n",
        "def binary_f_beta(pred_y, true_y, beta=1.0, positive=1):\n",
        "    precision = binary_precision(pred_y, true_y, positive)\n",
        "    recall = binary_recall(pred_y, true_y, positive)\n",
        "    try:\n",
        "        f_b = (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)\n",
        "    except:\n",
        "        f_b = 0\n",
        "    return f_b\n",
        "\n",
        "\n",
        "def multi_precision(pred_y, true_y, labels):\n",
        "    \"\"\"\n",
        "    多类的精确率\n",
        "    :param pred_y: 预测结果\n",
        "    :param true_y: 真实结果\n",
        "    :param labels: 标签列表\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if isinstance(pred_y[0], list):\n",
        "        pred_y = [item[0] for item in pred_y]\n",
        "\n",
        "    precisions = [binary_precision(pred_y, true_y, label) for label in labels]\n",
        "    prec = mean(precisions)\n",
        "    return prec\n",
        "\n",
        "\n",
        "def multi_recall(pred_y, true_y, labels):\n",
        "    \"\"\"\n",
        "    多类的召回率\n",
        "    :param pred_y: 预测结果\n",
        "    :param true_y: 真实结果\n",
        "    :param labels: 标签列表\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if isinstance(pred_y[0], list):\n",
        "        pred_y = [item[0] for item in pred_y]\n",
        "\n",
        "    recalls = [binary_recall(pred_y, true_y, label) for label in labels]\n",
        "    rec = mean(recalls)\n",
        "    return rec\n",
        "\n",
        "\n",
        "def multi_f_beta(pred_y, true_y, labels, beta=1.0):\n",
        "    \"\"\"\n",
        "    多类的f beta值\n",
        "    :param pred_y: 预测结果\n",
        "    :param true_y: 真实结果\n",
        "    :param labels: 标签列表\n",
        "    :param beta: beta值\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if isinstance(pred_y[0], list):\n",
        "        pred_y = [item[0] for item in pred_y]\n",
        "\n",
        "    f_betas = [binary_f_beta(pred_y, true_y, beta, label) for label in labels]\n",
        "    f_beta = mean(f_betas)\n",
        "    return f_beta\n",
        "\n",
        "\n",
        "def get_binary_metrics(pred_y, true_y, f_beta=1.0):\n",
        "    acc = accuracy(pred_y, true_y)\n",
        "    recall = binary_recall(pred_y, true_y)\n",
        "    precision = binary_precision(pred_y, true_y)\n",
        "    f_beta = binary_f_beta(pred_y, true_y, f_beta)\n",
        "    return acc, recall, precision, f_beta\n",
        "\n",
        "\n",
        "def get_multi_metrics(pred_y, true_y, labels, f_beta=1.0):\n",
        "    acc = accuracy(pred_y, true_y)\n",
        "    recall = multi_recall(pred_y, true_y, labels)\n",
        "    precision = multi_precision(pred_y, true_y, labels)\n",
        "    f_beta = multi_f_beta(pred_y, true_y, labels, f_beta)\n",
        "    return acc, recall, precision, f_beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x36lPunWXEeF",
        "outputId": "66d1ca69-ffbf-4363-9bec-79c472d1b87c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing to /content/drive/MyDrive/Project/Bi-LSTM/summarys\n",
            "\n",
            "start training model\n",
            "train: step: 13101, loss: 0.5577394962310791, acc: 0.625, recall: 0.35294117647058826, precision: 0.8571428571428571, f_beta: 0.5\n",
            "train: step: 13102, loss: 0.5218223333358765, acc: 0.6875, recall: 0.45714285714285713, precision: 0.9411764705882353, f_beta: 0.6153846153846154\n",
            "train: step: 13103, loss: 0.4933256506919861, acc: 0.75, recall: 0.5862068965517241, precision: 0.8095238095238095, f_beta: 0.68\n",
            "train: step: 13104, loss: 0.5491538643836975, acc: 0.640625, recall: 0.4782608695652174, precision: 0.5, f_beta: 0.4888888888888889\n",
            "train: step: 13105, loss: 0.5740712285041809, acc: 0.703125, recall: 0.5757575757575758, precision: 0.7916666666666666, f_beta: 0.6666666666666667\n",
            "train: step: 13106, loss: 0.5120251774787903, acc: 0.734375, recall: 0.5555555555555556, precision: 0.75, f_beta: 0.6382978723404256\n",
            "train: step: 13107, loss: 0.5779334306716919, acc: 0.671875, recall: 0.6296296296296297, precision: 0.6071428571428571, f_beta: 0.6181818181818182\n",
            "train: step: 13108, loss: 0.5516031980514526, acc: 0.765625, recall: 0.7037037037037037, precision: 0.7307692307692307, f_beta: 0.7169811320754716\n",
            "train: step: 13109, loss: 0.5676782131195068, acc: 0.609375, recall: 0.34615384615384615, precision: 0.5294117647058824, f_beta: 0.41860465116279066\n",
            "train: step: 13110, loss: 0.46987172961235046, acc: 0.734375, recall: 0.6551724137931034, precision: 0.7307692307692307, f_beta: 0.6909090909090909\n",
            "train: step: 13111, loss: 0.4979175925254822, acc: 0.734375, recall: 0.6666666666666666, precision: 0.7407407407407407, f_beta: 0.7017543859649122\n",
            "train: step: 13112, loss: 0.5476164221763611, acc: 0.6875, recall: 0.6, precision: 0.6923076923076923, f_beta: 0.6428571428571429\n",
            "train: step: 13113, loss: 0.6042271852493286, acc: 0.671875, recall: 0.5, precision: 0.5238095238095238, f_beta: 0.5116279069767442\n",
            "train: step: 13114, loss: 0.5404372811317444, acc: 0.65625, recall: 0.6538461538461539, precision: 0.5666666666666667, f_beta: 0.6071428571428571\n",
            "train: step: 13115, loss: 0.5245261192321777, acc: 0.71875, recall: 0.6956521739130435, precision: 0.5925925925925926, f_beta: 0.6399999999999999\n",
            "train: step: 13116, loss: 0.45123323798179626, acc: 0.71875, recall: 0.5769230769230769, precision: 0.6818181818181818, f_beta: 0.6249999999999999\n",
            "train: step: 13117, loss: 0.6637824177742004, acc: 0.53125, recall: 0.42424242424242425, precision: 0.56, f_beta: 0.4827586206896552\n",
            "train: step: 13118, loss: 0.5604147911071777, acc: 0.703125, recall: 0.6451612903225806, precision: 0.7142857142857143, f_beta: 0.6779661016949152\n",
            "train: step: 13119, loss: 0.4823059141635895, acc: 0.765625, recall: 0.5454545454545454, precision: 0.7058823529411765, f_beta: 0.6153846153846153\n",
            "train: step: 13120, loss: 0.5779247283935547, acc: 0.640625, recall: 0.44, precision: 0.55, f_beta: 0.48888888888888893\n",
            "train: step: 13121, loss: 0.5034147500991821, acc: 0.703125, recall: 0.5161290322580645, precision: 0.8, f_beta: 0.6274509803921569\n",
            "train: step: 13122, loss: 0.5373055934906006, acc: 0.734375, recall: 0.5925925925925926, precision: 0.7272727272727273, f_beta: 0.6530612244897959\n",
            "train: step: 13123, loss: 0.5805739164352417, acc: 0.703125, recall: 0.6111111111111112, precision: 0.8148148148148148, f_beta: 0.6984126984126984\n",
            "train: step: 13124, loss: 0.6076991558074951, acc: 0.59375, recall: 0.375, precision: 0.6666666666666666, f_beta: 0.4800000000000001\n",
            "train: step: 13125, loss: 0.5716003775596619, acc: 0.578125, recall: 0.30303030303030304, precision: 0.7142857142857143, f_beta: 0.42553191489361697\n",
            "train: step: 13126, loss: 0.657411515712738, acc: 0.546875, recall: 0.35294117647058826, precision: 0.631578947368421, f_beta: 0.45283018867924524\n",
            "train: step: 13127, loss: 0.6130669713020325, acc: 0.703125, recall: 0.6206896551724138, precision: 0.6923076923076923, f_beta: 0.6545454545454545\n",
            "train: step: 13128, loss: 0.5993906259536743, acc: 0.6875, recall: 0.5, precision: 0.8, f_beta: 0.6153846153846154\n",
            "train: step: 13129, loss: 0.5553262233734131, acc: 0.703125, recall: 0.5384615384615384, precision: 0.6666666666666666, f_beta: 0.5957446808510638\n",
            "train: step: 13130, loss: 0.581910252571106, acc: 0.65625, recall: 0.42857142857142855, precision: 0.6666666666666666, f_beta: 0.5217391304347826\n",
            "train: step: 13131, loss: 0.628105640411377, acc: 0.625, recall: 0.5, precision: 0.6666666666666666, f_beta: 0.5714285714285715\n",
            "train: step: 13132, loss: 0.5252680778503418, acc: 0.734375, recall: 0.6333333333333333, precision: 0.76, f_beta: 0.6909090909090909\n",
            "train: step: 13133, loss: 0.5401971936225891, acc: 0.75, recall: 0.6896551724137931, precision: 0.7407407407407407, f_beta: 0.7142857142857143\n",
            "train: step: 13134, loss: 0.542866587638855, acc: 0.671875, recall: 0.5806451612903226, precision: 0.6923076923076923, f_beta: 0.631578947368421\n",
            "train: step: 13135, loss: 0.6290282011032104, acc: 0.609375, recall: 0.5151515151515151, precision: 0.6538461538461539, f_beta: 0.576271186440678\n",
            "train: step: 13136, loss: 0.597335696220398, acc: 0.59375, recall: 0.5714285714285714, precision: 0.5333333333333333, f_beta: 0.5517241379310344\n",
            "train: step: 13137, loss: 0.46542462706565857, acc: 0.75, recall: 0.7692307692307693, precision: 0.6666666666666666, f_beta: 0.7142857142857142\n",
            "train: step: 13138, loss: 0.545947790145874, acc: 0.71875, recall: 0.6896551724137931, precision: 0.6896551724137931, f_beta: 0.6896551724137931\n",
            "train: step: 13139, loss: 0.559237539768219, acc: 0.609375, recall: 0.38235294117647056, precision: 0.7647058823529411, f_beta: 0.5098039215686274\n",
            "train: step: 13140, loss: 0.519179105758667, acc: 0.765625, recall: 0.7142857142857143, precision: 0.7407407407407407, f_beta: 0.7272727272727273\n",
            "train: step: 13141, loss: 0.6806027889251709, acc: 0.578125, recall: 0.5666666666666667, precision: 0.5483870967741935, f_beta: 0.5573770491803278\n",
            "train: step: 13142, loss: 0.5079067945480347, acc: 0.75, recall: 0.75, precision: 0.7, f_beta: 0.7241379310344827\n",
            "train: step: 13143, loss: 0.48765242099761963, acc: 0.734375, recall: 0.625, precision: 0.6521739130434783, f_beta: 0.6382978723404256\n",
            "train: step: 13144, loss: 0.5310848951339722, acc: 0.65625, recall: 0.6875, precision: 0.6470588235294118, f_beta: 0.6666666666666667\n",
            "train: step: 13145, loss: 0.592788577079773, acc: 0.65625, recall: 0.5517241379310345, precision: 0.64, f_beta: 0.5925925925925927\n",
            "train: step: 13146, loss: 0.5940343141555786, acc: 0.6875, recall: 0.6551724137931034, precision: 0.6551724137931034, f_beta: 0.6551724137931034\n",
            "train: step: 13147, loss: 0.5177140235900879, acc: 0.71875, recall: 0.5862068965517241, precision: 0.7391304347826086, f_beta: 0.6538461538461539\n",
            "train: step: 13148, loss: 0.4589536488056183, acc: 0.78125, recall: 0.7083333333333334, precision: 0.7083333333333334, f_beta: 0.7083333333333334\n",
            "train: step: 13149, loss: 0.574930727481842, acc: 0.703125, recall: 0.5714285714285714, precision: 0.8333333333333334, f_beta: 0.6779661016949152\n",
            "train: step: 13150, loss: 0.5621800422668457, acc: 0.6875, recall: 0.6451612903225806, precision: 0.6896551724137931, f_beta: 0.6666666666666667\n",
            "train: step: 13151, loss: 0.5446546673774719, acc: 0.6875, recall: 0.5, precision: 0.65, f_beta: 0.5652173913043479\n",
            "train: step: 13152, loss: 0.5819315314292908, acc: 0.65625, recall: 0.5135135135135135, precision: 0.8260869565217391, f_beta: 0.6333333333333333\n",
            "train: step: 13153, loss: 0.5832933187484741, acc: 0.703125, recall: 0.6071428571428571, precision: 0.68, f_beta: 0.6415094339622641\n",
            "train: step: 13154, loss: 0.5894614458084106, acc: 0.6875, recall: 0.44, precision: 0.6470588235294118, f_beta: 0.5238095238095238\n",
            "train: step: 13155, loss: 0.5081527829170227, acc: 0.703125, recall: 0.5151515151515151, precision: 0.85, f_beta: 0.6415094339622641\n",
            "train: step: 13156, loss: 0.6226627230644226, acc: 0.640625, recall: 0.5142857142857142, precision: 0.75, f_beta: 0.6101694915254237\n",
            "train: step: 13157, loss: 0.5375884771347046, acc: 0.671875, recall: 0.5, precision: 0.8095238095238095, f_beta: 0.6181818181818182\n",
            "train: step: 13158, loss: 0.5403509736061096, acc: 0.703125, recall: 0.5185185185185185, precision: 0.7, f_beta: 0.5957446808510639\n",
            "train: step: 13159, loss: 0.6058163642883301, acc: 0.59375, recall: 0.5666666666666667, precision: 0.5666666666666667, f_beta: 0.5666666666666667\n",
            "train: step: 13160, loss: 0.6071360111236572, acc: 0.671875, recall: 0.41379310344827586, precision: 0.75, f_beta: 0.5333333333333333\n",
            "train: step: 13161, loss: 0.5154199004173279, acc: 0.6875, recall: 0.6333333333333333, precision: 0.6785714285714286, f_beta: 0.6551724137931035\n",
            "train: step: 13162, loss: 0.532024621963501, acc: 0.65625, recall: 0.39285714285714285, precision: 0.6875, f_beta: 0.5\n",
            "train: step: 13163, loss: 0.49178242683410645, acc: 0.6875, recall: 0.5294117647058824, precision: 0.8181818181818182, f_beta: 0.6428571428571428\n",
            "train: step: 13164, loss: 0.5782617330551147, acc: 0.59375, recall: 0.43333333333333335, precision: 0.5909090909090909, f_beta: 0.5\n",
            "train: step: 13165, loss: 0.6547247171401978, acc: 0.625, recall: 0.5, precision: 0.5416666666666666, f_beta: 0.52\n",
            "train: step: 13166, loss: 0.5397781133651733, acc: 0.640625, recall: 0.4666666666666667, precision: 0.6666666666666666, f_beta: 0.5490196078431373\n",
            "train: step: 13167, loss: 0.5114801526069641, acc: 0.640625, recall: 0.5357142857142857, precision: 0.6, f_beta: 0.5660377358490566\n",
            "train: step: 13168, loss: 0.6336382031440735, acc: 0.609375, recall: 0.53125, precision: 0.6296296296296297, f_beta: 0.5762711864406779\n",
            "train: step: 13169, loss: 0.5599695444107056, acc: 0.703125, recall: 0.5517241379310345, precision: 0.7272727272727273, f_beta: 0.6274509803921569\n",
            "train: step: 13170, loss: 0.5406309962272644, acc: 0.71875, recall: 0.5172413793103449, precision: 0.7894736842105263, f_beta: 0.625\n",
            "train: step: 13171, loss: 0.5887470245361328, acc: 0.640625, recall: 0.5675675675675675, precision: 0.75, f_beta: 0.6461538461538462\n",
            "train: step: 13172, loss: 0.5960777997970581, acc: 0.65625, recall: 0.48, precision: 0.5714285714285714, f_beta: 0.5217391304347826\n",
            "train: step: 13173, loss: 0.4967663288116455, acc: 0.765625, recall: 0.8076923076923077, precision: 0.6774193548387096, f_beta: 0.7368421052631579\n",
            "train: step: 13174, loss: 0.4704630970954895, acc: 0.78125, recall: 0.7037037037037037, precision: 0.76, f_beta: 0.7307692307692308\n",
            "train: step: 13175, loss: 0.5621994733810425, acc: 0.625, recall: 0.5454545454545454, precision: 0.6666666666666666, f_beta: 0.6\n",
            "train: step: 13176, loss: 0.5707584023475647, acc: 0.671875, recall: 0.64, precision: 0.5714285714285714, f_beta: 0.6037735849056605\n",
            "train: step: 13177, loss: 0.5581148862838745, acc: 0.71875, recall: 0.625, precision: 0.625, f_beta: 0.625\n",
            "train: step: 13178, loss: 0.46678605675697327, acc: 0.6875, recall: 0.4666666666666667, precision: 0.7777777777777778, f_beta: 0.5833333333333334\n",
            "train: step: 13179, loss: 0.5397538542747498, acc: 0.703125, recall: 0.59375, precision: 0.76, f_beta: 0.6666666666666666\n",
            "train: step: 13180, loss: 0.5761092901229858, acc: 0.71875, recall: 0.5357142857142857, precision: 0.75, f_beta: 0.6250000000000001\n",
            "train: step: 13181, loss: 0.4994468092918396, acc: 0.6875, recall: 0.4827586206896552, precision: 0.7368421052631579, f_beta: 0.5833333333333334\n",
            "train: step: 13182, loss: 0.528863251209259, acc: 0.71875, recall: 0.5625, precision: 0.8181818181818182, f_beta: 0.6666666666666666\n",
            "train: step: 13183, loss: 0.5244169235229492, acc: 0.703125, recall: 0.4482758620689655, precision: 0.8125, f_beta: 0.5777777777777777\n",
            "train: step: 13184, loss: 0.4805411696434021, acc: 0.71875, recall: 0.5263157894736842, precision: 0.5263157894736842, f_beta: 0.5263157894736842\n",
            "train: step: 13185, loss: 0.6355728507041931, acc: 0.65625, recall: 0.4444444444444444, precision: 0.631578947368421, f_beta: 0.5217391304347826\n",
            "train: step: 13186, loss: 0.6606872081756592, acc: 0.609375, recall: 0.4482758620689655, precision: 0.5909090909090909, f_beta: 0.5098039215686274\n",
            "train: step: 13187, loss: 0.5679633617401123, acc: 0.6875, recall: 0.5806451612903226, precision: 0.72, f_beta: 0.6428571428571428\n",
            "train: step: 13188, loss: 0.5899343490600586, acc: 0.640625, recall: 0.36666666666666664, precision: 0.7333333333333333, f_beta: 0.4888888888888889\n",
            "train: step: 13189, loss: 0.6728089451789856, acc: 0.609375, recall: 0.4166666666666667, precision: 0.47619047619047616, f_beta: 0.4444444444444445\n",
            "train: step: 13190, loss: 0.5073556900024414, acc: 0.671875, recall: 0.5, precision: 0.5238095238095238, f_beta: 0.5116279069767442\n",
            "train: step: 13191, loss: 0.49260032176971436, acc: 0.78125, recall: 0.625, precision: 0.9090909090909091, f_beta: 0.7407407407407406\n",
            "train: step: 13192, loss: 0.6223381757736206, acc: 0.625, recall: 0.3870967741935484, precision: 0.7058823529411765, f_beta: 0.5\n",
            "train: step: 13193, loss: 0.5325737595558167, acc: 0.75, recall: 0.5, precision: 0.75, f_beta: 0.6\n",
            "train: step: 13194, loss: 0.5867105722427368, acc: 0.734375, recall: 0.5172413793103449, precision: 0.8333333333333334, f_beta: 0.6382978723404256\n",
            "train: step: 13195, loss: 0.6075345277786255, acc: 0.671875, recall: 0.48484848484848486, precision: 0.8, f_beta: 0.6037735849056605\n",
            "train: step: 13196, loss: 0.6079933047294617, acc: 0.671875, recall: 0.42424242424242425, precision: 0.875, f_beta: 0.5714285714285714\n",
            "train: step: 13197, loss: 0.4916066527366638, acc: 0.71875, recall: 0.5, precision: 0.6666666666666666, f_beta: 0.5714285714285715\n",
            "train: step: 13198, loss: 0.5099045634269714, acc: 0.734375, recall: 0.696969696969697, precision: 0.7666666666666667, f_beta: 0.7301587301587302\n",
            "train: step: 13199, loss: 0.5324866771697998, acc: 0.640625, recall: 0.3333333333333333, precision: 0.5333333333333333, f_beta: 0.4102564102564102\n",
            "train: step: 13200, loss: 0.5764234066009521, acc: 0.671875, recall: 0.4, precision: 0.625, f_beta: 0.48780487804878053\n",
            "\n",
            "Evaluation:\n",
            "2022-12-03T03:05:25.560238, step: 13200, loss: 0.950571485044222, acc: 0.364132785467128,precision: 0.364132785467128, recall: 1.0, f_beta: 0.5314414089361633\n",
            "Saved model checkpoint to /content/drive/MyDrive/Project/Bi-LSTM/model/my-model-13200\n",
            "\n",
            "train: step: 13201, loss: 0.5370593667030334, acc: 0.6875, recall: 0.6, precision: 0.6923076923076923, f_beta: 0.6428571428571429\n",
            "train: step: 13202, loss: 0.6459336280822754, acc: 0.609375, recall: 0.53125, precision: 0.6296296296296297, f_beta: 0.5762711864406779\n",
            "train: step: 13203, loss: 0.6211508512496948, acc: 0.640625, recall: 0.5925925925925926, precision: 0.5714285714285714, f_beta: 0.5818181818181818\n",
            "train: step: 13204, loss: 0.5621250867843628, acc: 0.59375, recall: 0.4444444444444444, precision: 0.5217391304347826, f_beta: 0.48\n",
            "train: step: 13205, loss: 0.4707906246185303, acc: 0.734375, recall: 0.7307692307692307, precision: 0.6551724137931034, f_beta: 0.6909090909090909\n",
            "train: step: 13206, loss: 0.5126821994781494, acc: 0.6875, recall: 0.48148148148148145, precision: 0.6842105263157895, f_beta: 0.5652173913043478\n",
            "train: step: 13207, loss: 0.5772136449813843, acc: 0.71875, recall: 0.6451612903225806, precision: 0.7407407407407407, f_beta: 0.689655172413793\n",
            "train: step: 13208, loss: 0.5511215329170227, acc: 0.6875, recall: 0.4642857142857143, precision: 0.7222222222222222, f_beta: 0.5652173913043479\n",
            "train: step: 13209, loss: 0.5321323871612549, acc: 0.6875, recall: 0.5625, precision: 0.75, f_beta: 0.6428571428571429\n",
            "train: step: 13210, loss: 0.5872052907943726, acc: 0.640625, recall: 0.5454545454545454, precision: 0.6923076923076923, f_beta: 0.6101694915254238\n",
            "train: step: 13211, loss: 0.4746030271053314, acc: 0.75, recall: 0.6060606060606061, precision: 0.8695652173913043, f_beta: 0.7142857142857143\n",
            "train: step: 13212, loss: 0.5871677398681641, acc: 0.609375, recall: 0.47368421052631576, precision: 0.782608695652174, f_beta: 0.5901639344262294\n",
            "train: step: 13213, loss: 0.6131412982940674, acc: 0.59375, recall: 0.4090909090909091, precision: 0.4090909090909091, f_beta: 0.4090909090909091\n",
            "train: step: 13214, loss: 0.7568498849868774, acc: 0.5, recall: 0.2962962962962963, precision: 0.38095238095238093, f_beta: 0.3333333333333333\n",
            "train: step: 13215, loss: 0.49551427364349365, acc: 0.734375, recall: 0.6896551724137931, precision: 0.7142857142857143, f_beta: 0.7017543859649122\n",
            "train: step: 13216, loss: 0.6276395916938782, acc: 0.6875, recall: 0.47058823529411764, precision: 0.8888888888888888, f_beta: 0.6153846153846153\n",
            "train: step: 13217, loss: 0.5944128036499023, acc: 0.65625, recall: 0.45, precision: 0.45, f_beta: 0.45\n",
            "train: step: 13218, loss: 0.5929837822914124, acc: 0.59375, recall: 0.45454545454545453, precision: 0.6521739130434783, f_beta: 0.5357142857142857\n",
            "train: step: 13219, loss: 0.6401745080947876, acc: 0.625, recall: 0.41379310344827586, precision: 0.631578947368421, f_beta: 0.5\n",
            "train: step: 13220, loss: 0.546934962272644, acc: 0.71875, recall: 0.52, precision: 0.6842105263157895, f_beta: 0.5909090909090909\n",
            "train: step: 13221, loss: 0.5020668506622314, acc: 0.703125, recall: 0.6296296296296297, precision: 0.6538461538461539, f_beta: 0.6415094339622641\n",
            "train: step: 13222, loss: 0.4920339286327362, acc: 0.71875, recall: 0.48148148148148145, precision: 0.7647058823529411, f_beta: 0.5909090909090909\n",
            "train: step: 13223, loss: 0.5653442740440369, acc: 0.671875, recall: 0.5666666666666667, precision: 0.68, f_beta: 0.6181818181818183\n",
            "train: step: 13224, loss: 0.654716968536377, acc: 0.609375, recall: 0.4838709677419355, precision: 0.625, f_beta: 0.5454545454545454\n",
            "train: step: 13225, loss: 0.5277970433235168, acc: 0.796875, recall: 0.6774193548387096, precision: 0.875, f_beta: 0.7636363636363636\n",
            "train: step: 13226, loss: 0.5662763714790344, acc: 0.6875, recall: 0.4444444444444444, precision: 0.7058823529411765, f_beta: 0.5454545454545455\n",
            "train: step: 13227, loss: 0.5242573618888855, acc: 0.78125, recall: 0.6666666666666666, precision: 0.782608695652174, f_beta: 0.72\n",
            "train: step: 13228, loss: 0.6555670499801636, acc: 0.609375, recall: 0.45161290322580644, precision: 0.6363636363636364, f_beta: 0.5283018867924528\n",
            "train: step: 13229, loss: 0.47370219230651855, acc: 0.796875, recall: 0.7333333333333333, precision: 0.8148148148148148, f_beta: 0.7719298245614035\n",
            "train: step: 13230, loss: 0.5730047225952148, acc: 0.65625, recall: 0.5, precision: 0.5909090909090909, f_beta: 0.5416666666666667\n",
            "train: step: 13231, loss: 0.5344737768173218, acc: 0.71875, recall: 0.5172413793103449, precision: 0.7894736842105263, f_beta: 0.625\n",
            "train: step: 13232, loss: 0.5554972887039185, acc: 0.6875, recall: 0.4838709677419355, precision: 0.7894736842105263, f_beta: 0.6\n",
            "train: step: 13233, loss: 0.5496885180473328, acc: 0.640625, recall: 0.4482758620689655, precision: 0.65, f_beta: 0.5306122448979592\n",
            "train: step: 13234, loss: 0.5011981725692749, acc: 0.71875, recall: 0.5714285714285714, precision: 0.5714285714285714, f_beta: 0.5714285714285714\n",
            "train: step: 13235, loss: 0.4942712187767029, acc: 0.796875, recall: 0.7419354838709677, precision: 0.8214285714285714, f_beta: 0.7796610169491526\n",
            "train: step: 13236, loss: 0.6709142923355103, acc: 0.625, recall: 0.43333333333333335, precision: 0.65, f_beta: 0.5199999999999999\n",
            "train: step: 13237, loss: 0.5275470018386841, acc: 0.75, recall: 0.6896551724137931, precision: 0.7407407407407407, f_beta: 0.7142857142857143\n",
            "train: step: 13238, loss: 0.6367790699005127, acc: 0.59375, recall: 0.4230769230769231, precision: 0.5, f_beta: 0.4583333333333333\n",
            "train: step: 13239, loss: 0.5568017363548279, acc: 0.703125, recall: 0.5, precision: 0.7894736842105263, f_beta: 0.6122448979591837\n",
            "train: step: 13240, loss: 0.5711035132408142, acc: 0.703125, recall: 0.5517241379310345, precision: 0.7272727272727273, f_beta: 0.6274509803921569\n",
            "train: step: 13241, loss: 0.6214361786842346, acc: 0.640625, recall: 0.6111111111111112, precision: 0.7096774193548387, f_beta: 0.6567164179104478\n",
            "train: step: 13242, loss: 0.6053869724273682, acc: 0.5625, recall: 0.2916666666666667, precision: 0.3888888888888889, f_beta: 0.3333333333333333\n",
            "train: step: 13243, loss: 0.42507123947143555, acc: 0.734375, recall: 0.6551724137931034, precision: 0.7307692307692307, f_beta: 0.6909090909090909\n",
            "train: step: 13244, loss: 0.5677019357681274, acc: 0.671875, recall: 0.5625, precision: 0.72, f_beta: 0.631578947368421\n",
            "train: step: 13245, loss: 0.6082547307014465, acc: 0.59375, recall: 0.4230769230769231, precision: 0.5, f_beta: 0.4583333333333333\n",
            "train: step: 13246, loss: 0.6490716338157654, acc: 0.671875, recall: 0.5517241379310345, precision: 0.6666666666666666, f_beta: 0.6037735849056604\n",
            "train: step: 13247, loss: 0.5600813627243042, acc: 0.6875, recall: 0.6, precision: 0.6, f_beta: 0.6\n",
            "train: step: 13248, loss: 0.6461613178253174, acc: 0.640625, recall: 0.5151515151515151, precision: 0.7083333333333334, f_beta: 0.5964912280701754\n",
            "train: step: 13249, loss: 0.5103038549423218, acc: 0.703125, recall: 0.5, precision: 0.6842105263157895, f_beta: 0.5777777777777778\n",
            "train: step: 13250, loss: 0.5980842709541321, acc: 0.671875, recall: 0.5625, precision: 0.72, f_beta: 0.631578947368421\n",
            "train: step: 13251, loss: 0.5624275803565979, acc: 0.71875, recall: 0.5862068965517241, precision: 0.7391304347826086, f_beta: 0.6538461538461539\n",
            "train: step: 13252, loss: 0.565584659576416, acc: 0.640625, recall: 0.5384615384615384, precision: 0.56, f_beta: 0.5490196078431373\n",
            "train: step: 13253, loss: 0.6239690184593201, acc: 0.671875, recall: 0.5172413793103449, precision: 0.6818181818181818, f_beta: 0.5882352941176471\n",
            "train: step: 13254, loss: 0.5727006196975708, acc: 0.703125, recall: 0.5789473684210527, precision: 0.5, f_beta: 0.5365853658536586\n",
            "train: step: 13255, loss: 0.5927042365074158, acc: 0.5625, recall: 0.41935483870967744, precision: 0.5652173913043478, f_beta: 0.4814814814814815\n",
            "train: step: 13256, loss: 0.5774015188217163, acc: 0.65625, recall: 0.39285714285714285, precision: 0.6875, f_beta: 0.5\n",
            "train: step: 13257, loss: 0.5792645215988159, acc: 0.6875, recall: 0.5142857142857142, precision: 0.8571428571428571, f_beta: 0.6428571428571428\n",
            "train: step: 13258, loss: 0.5505926012992859, acc: 0.6875, recall: 0.625, precision: 0.7142857142857143, f_beta: 0.6666666666666666\n",
            "train: step: 13259, loss: 0.6032679677009583, acc: 0.6875, recall: 0.5357142857142857, precision: 0.6818181818181818, f_beta: 0.6\n",
            "train: step: 13260, loss: 0.6083593368530273, acc: 0.6875, recall: 0.5, precision: 0.7, f_beta: 0.5833333333333334\n",
            "train: step: 13261, loss: 0.5888230800628662, acc: 0.6875, recall: 0.4827586206896552, precision: 0.7368421052631579, f_beta: 0.5833333333333334\n",
            "train: step: 13262, loss: 0.6725148558616638, acc: 0.65625, recall: 0.4642857142857143, precision: 0.65, f_beta: 0.5416666666666667\n",
            "train: step: 13263, loss: 0.47733578085899353, acc: 0.765625, recall: 0.8148148148148148, precision: 0.6875, f_beta: 0.7457627118644067\n",
            "train: step: 13264, loss: 0.5601954460144043, acc: 0.6875, recall: 0.4827586206896552, precision: 0.7368421052631579, f_beta: 0.5833333333333334\n",
            "train: step: 13265, loss: 0.5153182148933411, acc: 0.765625, recall: 0.6896551724137931, precision: 0.7692307692307693, f_beta: 0.7272727272727274\n",
            "train: step: 13266, loss: 0.5617688298225403, acc: 0.640625, recall: 0.38461538461538464, precision: 0.5882352941176471, f_beta: 0.46511627906976744\n",
            "train: step: 13267, loss: 0.654733419418335, acc: 0.609375, recall: 0.5333333333333333, precision: 0.5925925925925926, f_beta: 0.5614035087719299\n",
            "train: step: 13268, loss: 0.5225840210914612, acc: 0.734375, recall: 0.6206896551724138, precision: 0.75, f_beta: 0.679245283018868\n",
            "train: step: 13269, loss: 0.5345032811164856, acc: 0.640625, recall: 0.3939393939393939, precision: 0.8125, f_beta: 0.5306122448979591\n",
            "train: step: 13270, loss: 0.5724173188209534, acc: 0.65625, recall: 0.48148148148148145, precision: 0.6190476190476191, f_beta: 0.5416666666666666\n",
            "train: step: 13271, loss: 0.5861659049987793, acc: 0.734375, recall: 0.5833333333333334, precision: 0.6666666666666666, f_beta: 0.6222222222222222\n",
            "train: step: 13272, loss: 0.6100888252258301, acc: 0.625, recall: 0.41025641025641024, precision: 0.9411764705882353, f_beta: 0.5714285714285713\n",
            "train: step: 13273, loss: 0.6204493045806885, acc: 0.59375, recall: 0.34615384615384615, precision: 0.5, f_beta: 0.40909090909090906\n",
            "train: step: 13274, loss: 0.49617999792099, acc: 0.75, recall: 0.5862068965517241, precision: 0.8095238095238095, f_beta: 0.68\n",
            "train: step: 13275, loss: 0.5523802042007446, acc: 0.6875, recall: 0.46153846153846156, precision: 0.6666666666666666, f_beta: 0.5454545454545455\n",
            "train: step: 13276, loss: 0.5186220407485962, acc: 0.796875, recall: 0.5416666666666666, precision: 0.8666666666666667, f_beta: 0.6666666666666667\n",
            "train: step: 13277, loss: 0.49639707803726196, acc: 0.78125, recall: 0.5652173913043478, precision: 0.7647058823529411, f_beta: 0.65\n",
            "train: step: 13278, loss: 0.5829085111618042, acc: 0.703125, recall: 0.5333333333333333, precision: 0.7619047619047619, f_beta: 0.6274509803921569\n",
            "train: step: 13279, loss: 0.5727801322937012, acc: 0.65625, recall: 0.41379310344827586, precision: 0.7058823529411765, f_beta: 0.5217391304347826\n",
            "train: step: 13280, loss: 0.6656479239463806, acc: 0.5625, recall: 0.3142857142857143, precision: 0.7333333333333333, f_beta: 0.44\n",
            "train: step: 13281, loss: 0.5943465232849121, acc: 0.640625, recall: 0.2608695652173913, precision: 0.5, f_beta: 0.3428571428571428\n",
            "train: step: 13282, loss: 0.5626718997955322, acc: 0.703125, recall: 0.4090909090909091, precision: 0.6, f_beta: 0.4864864864864865\n",
            "train: step: 13283, loss: 0.5130608677864075, acc: 0.703125, recall: 0.4375, precision: 0.9333333333333333, f_beta: 0.5957446808510638\n",
            "train: step: 13284, loss: 0.5267512798309326, acc: 0.6875, recall: 0.5, precision: 0.85, f_beta: 0.6296296296296295\n",
            "train: step: 13285, loss: 0.5890247821807861, acc: 0.71875, recall: 0.4827586206896552, precision: 0.8235294117647058, f_beta: 0.608695652173913\n",
            "train: step: 13286, loss: 0.5904033184051514, acc: 0.6875, recall: 0.35714285714285715, precision: 0.8333333333333334, f_beta: 0.5\n",
            "train: step: 13287, loss: 0.5716245770454407, acc: 0.65625, recall: 0.5161290322580645, precision: 0.6956521739130435, f_beta: 0.5925925925925926\n",
            "train: step: 13288, loss: 0.6583697199821472, acc: 0.578125, recall: 0.36666666666666664, precision: 0.5789473684210527, f_beta: 0.44897959183673464\n",
            "train: step: 13289, loss: 0.5585757493972778, acc: 0.609375, recall: 0.4722222222222222, precision: 0.7391304347826086, f_beta: 0.5762711864406781\n",
            "train: step: 13290, loss: 0.4884865880012512, acc: 0.734375, recall: 0.5333333333333333, precision: 0.8421052631578947, f_beta: 0.653061224489796\n",
            "train: step: 13291, loss: 0.5756305456161499, acc: 0.703125, recall: 0.6129032258064516, precision: 0.7307692307692307, f_beta: 0.6666666666666667\n",
            "train: step: 13292, loss: 0.5052230358123779, acc: 0.734375, recall: 0.5862068965517241, precision: 0.7727272727272727, f_beta: 0.6666666666666667\n",
            "train: step: 13293, loss: 0.5682332515716553, acc: 0.671875, recall: 0.5483870967741935, precision: 0.7083333333333334, f_beta: 0.6181818181818182\n",
            "train: step: 13294, loss: 0.5258695483207703, acc: 0.703125, recall: 0.6666666666666666, precision: 0.6428571428571429, f_beta: 0.6545454545454545\n",
            "train: step: 13295, loss: 0.6687279343605042, acc: 0.5625, recall: 0.41379310344827586, precision: 0.5217391304347826, f_beta: 0.4615384615384615\n",
            "train: step: 13296, loss: 0.4844954013824463, acc: 0.703125, recall: 0.6333333333333333, precision: 0.7037037037037037, f_beta: 0.6666666666666667\n",
            "train: step: 13297, loss: 0.48552149534225464, acc: 0.71875, recall: 0.6296296296296297, precision: 0.68, f_beta: 0.6538461538461539\n",
            "train: step: 13298, loss: 0.569324791431427, acc: 0.75, recall: 0.7307692307692307, precision: 0.6785714285714286, f_beta: 0.7037037037037038\n",
            "train: step: 13299, loss: 0.4844779372215271, acc: 0.71875, recall: 0.6538461538461539, precision: 0.6538461538461539, f_beta: 0.6538461538461539\n",
            "train: step: 13300, loss: 0.5541541576385498, acc: 0.703125, recall: 0.5384615384615384, precision: 0.6666666666666666, f_beta: 0.5957446808510638\n",
            "\n",
            "Evaluation:\n",
            "2022-12-03T03:10:19.464117, step: 13300, loss: 0.9994667776728171, acc: 0.3732698961937716,precision: 0.3732698961937716, recall: 1.0, f_beta: 0.5410020641113016\n",
            "Saved model checkpoint to /content/drive/MyDrive/Project/Bi-LSTM/model/my-model-13300\n",
            "\n",
            "train: step: 13301, loss: 0.5419070720672607, acc: 0.703125, recall: 0.6129032258064516, precision: 0.7307692307692307, f_beta: 0.6666666666666667\n",
            "train: step: 13302, loss: 0.5399723052978516, acc: 0.734375, recall: 0.6071428571428571, precision: 0.7391304347826086, f_beta: 0.6666666666666666\n",
            "train: step: 13303, loss: 0.5341517329216003, acc: 0.734375, recall: 0.52, precision: 0.7222222222222222, f_beta: 0.6046511627906976\n",
            "train: step: 13304, loss: 0.584443211555481, acc: 0.609375, recall: 0.41379310344827586, precision: 0.6, f_beta: 0.4897959183673469\n",
            "train: step: 13305, loss: 0.7369641065597534, acc: 0.515625, recall: 0.375, precision: 0.5217391304347826, f_beta: 0.43636363636363634\n",
            "train: step: 13306, loss: 0.5285000801086426, acc: 0.71875, recall: 0.5357142857142857, precision: 0.75, f_beta: 0.6250000000000001\n",
            "train: step: 13307, loss: 0.582263708114624, acc: 0.640625, recall: 0.46153846153846156, precision: 0.5714285714285714, f_beta: 0.5106382978723405\n",
            "train: step: 13308, loss: 0.6167780160903931, acc: 0.671875, recall: 0.6, precision: 0.5769230769230769, f_beta: 0.5882352941176471\n",
            "train: step: 13309, loss: 0.5273847579956055, acc: 0.734375, recall: 0.6153846153846154, precision: 0.6956521739130435, f_beta: 0.6530612244897959\n",
            "train: step: 13310, loss: 0.5790480375289917, acc: 0.640625, recall: 0.44, precision: 0.55, f_beta: 0.48888888888888893\n",
            "train: step: 13311, loss: 0.6545106172561646, acc: 0.640625, recall: 0.45161290322580644, precision: 0.7, f_beta: 0.5490196078431372\n",
            "train: step: 13312, loss: 0.4495469629764557, acc: 0.734375, recall: 0.5454545454545454, precision: 0.631578947368421, f_beta: 0.5853658536585366\n",
            "train: step: 13313, loss: 0.4676682651042938, acc: 0.765625, recall: 0.6666666666666666, precision: 0.75, f_beta: 0.7058823529411765\n",
            "train: step: 13314, loss: 0.65590500831604, acc: 0.625, recall: 0.5294117647058824, precision: 0.6923076923076923, f_beta: 0.5999999999999999\n",
            "train: step: 13315, loss: 0.6126464605331421, acc: 0.65625, recall: 0.5, precision: 0.6818181818181818, f_beta: 0.576923076923077\n",
            "train: step: 13316, loss: 0.55674809217453, acc: 0.640625, recall: 0.42424242424242425, precision: 0.7777777777777778, f_beta: 0.5490196078431373\n",
            "train: step: 13317, loss: 0.5699028372764587, acc: 0.6875, recall: 0.4827586206896552, precision: 0.7368421052631579, f_beta: 0.5833333333333334\n",
            "train: step: 13318, loss: 0.4979613423347473, acc: 0.703125, recall: 0.45161290322580644, precision: 0.875, f_beta: 0.5957446808510638\n",
            "train: step: 13319, loss: 0.7145079970359802, acc: 0.640625, recall: 0.41935483870967744, precision: 0.7222222222222222, f_beta: 0.5306122448979592\n",
            "train: step: 13320, loss: 0.5571601986885071, acc: 0.703125, recall: 0.47619047619047616, precision: 0.5555555555555556, f_beta: 0.5128205128205129\n",
            "train: step: 13321, loss: 0.5508896708488464, acc: 0.703125, recall: 0.5833333333333334, precision: 0.6086956521739131, f_beta: 0.5957446808510638\n",
            "train: step: 13322, loss: 0.5547724962234497, acc: 0.75, recall: 0.75, precision: 0.75, f_beta: 0.75\n",
            "train: step: 13323, loss: 0.5546221733093262, acc: 0.625, recall: 0.6071428571428571, precision: 0.5666666666666667, f_beta: 0.5862068965517241\n",
            "train: step: 13324, loss: 0.48806047439575195, acc: 0.75, recall: 0.5, precision: 0.6875, f_beta: 0.5789473684210527\n",
            "train: step: 13325, loss: 0.5723835825920105, acc: 0.71875, recall: 0.6666666666666666, precision: 0.6666666666666666, f_beta: 0.6666666666666666\n",
            "train: step: 13326, loss: 0.610558271408081, acc: 0.671875, recall: 0.5517241379310345, precision: 0.6666666666666666, f_beta: 0.6037735849056604\n",
            "train: step: 13327, loss: 0.5329537391662598, acc: 0.765625, recall: 0.6857142857142857, precision: 0.8571428571428571, f_beta: 0.7619047619047619\n",
            "train: step: 13328, loss: 0.5953317880630493, acc: 0.625, recall: 0.5, precision: 0.625, f_beta: 0.5555555555555556\n",
            "train: step: 13329, loss: 0.5703301429748535, acc: 0.6875, recall: 0.6071428571428571, precision: 0.6538461538461539, f_beta: 0.6296296296296297\n",
            "train: step: 13330, loss: 0.510138988494873, acc: 0.75, recall: 0.56, precision: 0.7368421052631579, f_beta: 0.6363636363636364\n",
            "train: step: 13331, loss: 0.5261955857276917, acc: 0.75, recall: 0.6785714285714286, precision: 0.7307692307692307, f_beta: 0.7037037037037038\n",
            "train: step: 13332, loss: 0.487033873796463, acc: 0.75, recall: 0.7037037037037037, precision: 0.7037037037037037, f_beta: 0.7037037037037037\n",
            "train: step: 13333, loss: 0.5539919137954712, acc: 0.75, recall: 0.6538461538461539, precision: 0.7083333333333334, f_beta: 0.68\n",
            "train: step: 13334, loss: 0.5570113062858582, acc: 0.71875, recall: 0.48, precision: 0.7058823529411765, f_beta: 0.5714285714285713\n",
            "train: step: 13335, loss: 0.6155666708946228, acc: 0.546875, recall: 0.41379310344827586, precision: 0.5, f_beta: 0.4528301886792453\n",
            "train: step: 13336, loss: 0.534441351890564, acc: 0.78125, recall: 0.5925925925925926, precision: 0.8421052631578947, f_beta: 0.6956521739130435\n",
            "train: step: 13337, loss: 0.5379830002784729, acc: 0.71875, recall: 0.631578947368421, precision: 0.5217391304347826, f_beta: 0.5714285714285715\n",
            "train: step: 13338, loss: 0.5040498971939087, acc: 0.71875, recall: 0.5555555555555556, precision: 0.7142857142857143, f_beta: 0.6250000000000001\n",
            "train: step: 13339, loss: 0.7732147574424744, acc: 0.59375, recall: 0.43333333333333335, precision: 0.5909090909090909, f_beta: 0.5\n",
            "train: step: 13340, loss: 0.5445396304130554, acc: 0.65625, recall: 0.4827586206896552, precision: 0.6666666666666666, f_beta: 0.56\n",
            "train: step: 13341, loss: 0.5476483106613159, acc: 0.765625, recall: 0.6086956521739131, precision: 0.7, f_beta: 0.6511627906976744\n",
            "train: step: 13342, loss: 0.5261498093605042, acc: 0.71875, recall: 0.6551724137931034, precision: 0.7037037037037037, f_beta: 0.6785714285714286\n",
            "train: step: 13343, loss: 0.5494250655174255, acc: 0.6875, recall: 0.48148148148148145, precision: 0.6842105263157895, f_beta: 0.5652173913043478\n",
            "train: step: 13344, loss: 0.6572427153587341, acc: 0.578125, recall: 0.4230769230769231, precision: 0.4782608695652174, f_beta: 0.44897959183673475\n",
            "train: step: 13345, loss: 0.6294175386428833, acc: 0.578125, recall: 0.3939393939393939, precision: 0.65, f_beta: 0.490566037735849\n",
            "train: step: 13346, loss: 0.5671398639678955, acc: 0.65625, recall: 0.4, precision: 0.5882352941176471, f_beta: 0.4761904761904762\n",
            "train: step: 13347, loss: 0.5572976469993591, acc: 0.75, recall: 0.5769230769230769, precision: 0.75, f_beta: 0.6521739130434783\n",
            "train: step: 13348, loss: 0.6302595734596252, acc: 0.703125, recall: 0.6428571428571429, precision: 0.6666666666666666, f_beta: 0.6545454545454545\n",
            "train: step: 13349, loss: 0.6984096169471741, acc: 0.671875, recall: 0.5769230769230769, precision: 0.6, f_beta: 0.5882352941176471\n",
            "train: step: 13350, loss: 0.5220754146575928, acc: 0.671875, recall: 0.5384615384615384, precision: 0.6086956521739131, f_beta: 0.5714285714285715\n",
            "train: step: 13351, loss: 0.5575205087661743, acc: 0.625, recall: 0.53125, precision: 0.6538461538461539, f_beta: 0.5862068965517242\n",
            "train: step: 13352, loss: 0.5461626648902893, acc: 0.6875, recall: 0.5161290322580645, precision: 0.7619047619047619, f_beta: 0.6153846153846153\n",
            "train: step: 13353, loss: 0.5684009790420532, acc: 0.6875, recall: 0.3333333333333333, precision: 0.8181818181818182, f_beta: 0.4736842105263157\n",
            "train: step: 13354, loss: 0.5699924826622009, acc: 0.640625, recall: 0.46153846153846156, precision: 0.9, f_beta: 0.6101694915254238\n",
            "train: step: 13355, loss: 0.5625026822090149, acc: 0.734375, recall: 0.6, precision: 0.782608695652174, f_beta: 0.6792452830188679\n",
            "train: step: 13356, loss: 0.5348491668701172, acc: 0.6875, recall: 0.46153846153846156, precision: 0.6666666666666666, f_beta: 0.5454545454545455\n",
            "train: step: 13357, loss: 0.5348972082138062, acc: 0.6875, recall: 0.6538461538461539, precision: 0.6071428571428571, f_beta: 0.6296296296296297\n",
            "train: step: 13358, loss: 0.5379658937454224, acc: 0.703125, recall: 0.6206896551724138, precision: 0.6923076923076923, f_beta: 0.6545454545454545\n",
            "train: step: 13359, loss: 0.549728274345398, acc: 0.734375, recall: 0.5454545454545454, precision: 0.631578947368421, f_beta: 0.5853658536585366\n",
            "train: step: 13360, loss: 0.470584511756897, acc: 0.734375, recall: 0.782608695652174, precision: 0.6, f_beta: 0.6792452830188679\n",
            "train: step: 13361, loss: 0.5610142946243286, acc: 0.65625, recall: 0.4666666666666667, precision: 0.7, f_beta: 0.56\n",
            "train: step: 13362, loss: 0.5530556440353394, acc: 0.78125, recall: 0.7222222222222222, precision: 0.5909090909090909, f_beta: 0.65\n",
            "train: step: 13363, loss: 0.5578145980834961, acc: 0.65625, recall: 0.2857142857142857, precision: 0.46153846153846156, f_beta: 0.35294117647058826\n",
            "train: step: 13364, loss: 0.5787444114685059, acc: 0.6875, recall: 0.5555555555555556, precision: 0.8333333333333334, f_beta: 0.6666666666666667\n",
            "train: step: 13365, loss: 0.5346649289131165, acc: 0.6875, recall: 0.48, precision: 0.631578947368421, f_beta: 0.5454545454545454\n",
            "train: step: 13366, loss: 0.625596821308136, acc: 0.59375, recall: 0.4666666666666667, precision: 0.5833333333333334, f_beta: 0.5185185185185186\n",
            "train: step: 13367, loss: 0.5118770599365234, acc: 0.671875, recall: 0.5, precision: 0.7619047619047619, f_beta: 0.6037735849056604\n",
            "train: step: 13368, loss: 0.5053586959838867, acc: 0.65625, recall: 0.4482758620689655, precision: 0.6842105263157895, f_beta: 0.5416666666666666\n",
            "train: step: 13369, loss: 0.50621497631073, acc: 0.703125, recall: 0.6111111111111112, precision: 0.8148148148148148, f_beta: 0.6984126984126984\n",
            "train: step: 13370, loss: 0.615297794342041, acc: 0.65625, recall: 0.48148148148148145, precision: 0.6190476190476191, f_beta: 0.5416666666666666\n",
            "train: step: 13371, loss: 0.49363580346107483, acc: 0.6875, recall: 0.5, precision: 0.7, f_beta: 0.5833333333333334\n",
            "train: step: 13372, loss: 0.5059860348701477, acc: 0.765625, recall: 0.5217391304347826, precision: 0.75, f_beta: 0.6153846153846153\n",
            "train: step: 13373, loss: 0.5693365335464478, acc: 0.6875, recall: 0.4375, precision: 0.875, f_beta: 0.5833333333333334\n",
            "train: step: 13374, loss: 0.5325381755828857, acc: 0.734375, recall: 0.6451612903225806, precision: 0.7692307692307693, f_beta: 0.7017543859649122\n",
            "train: step: 13375, loss: 0.59772789478302, acc: 0.640625, recall: 0.32142857142857145, precision: 0.6923076923076923, f_beta: 0.4390243902439025\n",
            "train: step: 13376, loss: 0.5035464763641357, acc: 0.71875, recall: 0.48148148148148145, precision: 0.7647058823529411, f_beta: 0.5909090909090909\n",
            "train: step: 13377, loss: 0.6213366985321045, acc: 0.6875, recall: 0.48, precision: 0.631578947368421, f_beta: 0.5454545454545454\n",
            "train: step: 13378, loss: 0.4555799067020416, acc: 0.8125, recall: 0.6666666666666666, precision: 0.7368421052631579, f_beta: 0.7\n",
            "train: step: 13379, loss: 0.46507036685943604, acc: 0.765625, recall: 0.6896551724137931, precision: 0.7692307692307693, f_beta: 0.7272727272727274\n",
            "train: step: 13380, loss: 0.5228625535964966, acc: 0.703125, recall: 0.48148148148148145, precision: 0.7222222222222222, f_beta: 0.5777777777777777\n",
            "train: step: 13381, loss: 0.5140070915222168, acc: 0.703125, recall: 0.5862068965517241, precision: 0.7083333333333334, f_beta: 0.6415094339622641\n",
            "train: step: 13382, loss: 0.6362136602401733, acc: 0.640625, recall: 0.5588235294117647, precision: 0.7037037037037037, f_beta: 0.6229508196721312\n",
            "train: step: 13383, loss: 0.5977559089660645, acc: 0.6875, recall: 0.6666666666666666, precision: 0.6666666666666666, f_beta: 0.6666666666666666\n",
            "train: step: 13384, loss: 0.4805765748023987, acc: 0.8125, recall: 0.78125, precision: 0.8333333333333334, f_beta: 0.8064516129032259\n",
            "train: step: 13385, loss: 0.5448566675186157, acc: 0.671875, recall: 0.5454545454545454, precision: 0.75, f_beta: 0.631578947368421\n",
            "train: step: 13386, loss: 0.5347381830215454, acc: 0.703125, recall: 0.5, precision: 0.631578947368421, f_beta: 0.5581395348837209\n",
            "train: step: 13387, loss: 0.5701673030853271, acc: 0.703125, recall: 0.65625, precision: 0.7241379310344828, f_beta: 0.6885245901639345\n",
            "train: step: 13388, loss: 0.6103221774101257, acc: 0.703125, recall: 0.6, precision: 0.625, f_beta: 0.6122448979591836\n",
            "train: step: 13389, loss: 0.5725049376487732, acc: 0.65625, recall: 0.6875, precision: 0.6470588235294118, f_beta: 0.6666666666666667\n",
            "train: step: 13390, loss: 0.5484268665313721, acc: 0.6875, recall: 0.7037037037037037, precision: 0.6129032258064516, f_beta: 0.6551724137931035\n",
            "train: step: 13391, loss: 0.5388078689575195, acc: 0.671875, recall: 0.5161290322580645, precision: 0.7272727272727273, f_beta: 0.6037735849056604\n",
            "train: step: 13392, loss: 0.6797430515289307, acc: 0.65625, recall: 0.4090909090909091, precision: 0.5, f_beta: 0.45\n",
            "train: step: 13393, loss: 0.6292449831962585, acc: 0.609375, recall: 0.4482758620689655, precision: 0.5909090909090909, f_beta: 0.5098039215686274\n",
            "train: step: 13394, loss: 0.6029931306838989, acc: 0.578125, recall: 0.5333333333333333, precision: 0.5517241379310345, f_beta: 0.5423728813559322\n",
            "train: step: 13395, loss: 0.5667192935943604, acc: 0.75, recall: 0.6764705882352942, precision: 0.8214285714285714, f_beta: 0.7419354838709677\n",
            "train: step: 13396, loss: 0.48247021436691284, acc: 0.78125, recall: 0.6176470588235294, precision: 0.9545454545454546, f_beta: 0.75\n",
            "train: step: 13397, loss: 0.5213799476623535, acc: 0.734375, recall: 0.5714285714285714, precision: 0.7619047619047619, f_beta: 0.6530612244897959\n",
            "train: step: 13398, loss: 0.47763872146606445, acc: 0.8125, recall: 0.6538461538461539, precision: 0.85, f_beta: 0.7391304347826088\n",
            "train: step: 13399, loss: 0.5476601123809814, acc: 0.671875, recall: 0.75, precision: 0.6, f_beta: 0.6666666666666665\n",
            "train: step: 13400, loss: 0.511815071105957, acc: 0.71875, recall: 0.48, precision: 0.7058823529411765, f_beta: 0.5714285714285713\n",
            "\n",
            "Evaluation:\n",
            "2022-12-03T03:15:11.497960, step: 13400, loss: 0.9407561312084792, acc: 0.4151167820069204,precision: 0.4151167820069204, recall: 1.0, f_beta: 0.5841324541714136\n",
            "Saved model checkpoint to /content/drive/MyDrive/Project/Bi-LSTM/model/my-model-13400\n",
            "\n",
            "train: step: 13401, loss: 0.44151055812835693, acc: 0.796875, recall: 0.7297297297297297, precision: 0.9, f_beta: 0.8059701492537312\n",
            "train: step: 13402, loss: 0.6819189786911011, acc: 0.625, recall: 0.46153846153846156, precision: 0.5454545454545454, f_beta: 0.4999999999999999\n",
            "train: step: 13403, loss: 0.5246232748031616, acc: 0.703125, recall: 0.6296296296296297, precision: 0.6538461538461539, f_beta: 0.6415094339622641\n",
            "train: step: 13404, loss: 0.507217526435852, acc: 0.734375, recall: 0.6176470588235294, precision: 0.84, f_beta: 0.711864406779661\n",
            "train: step: 13405, loss: 0.4510849714279175, acc: 0.796875, recall: 0.68, precision: 0.7727272727272727, f_beta: 0.7234042553191491\n",
            "train: step: 13406, loss: 0.5343508124351501, acc: 0.609375, recall: 0.4838709677419355, precision: 0.625, f_beta: 0.5454545454545454\n",
            "train: step: 13407, loss: 0.5368850231170654, acc: 0.671875, recall: 0.7, precision: 0.6363636363636364, f_beta: 0.6666666666666666\n",
            "train: step: 13408, loss: 0.6466410160064697, acc: 0.59375, recall: 0.4074074074074074, precision: 0.5238095238095238, f_beta: 0.4583333333333333\n",
            "train: step: 13409, loss: 0.41926145553588867, acc: 0.8125, recall: 0.76, precision: 0.76, f_beta: 0.76\n",
            "train: step: 13410, loss: 0.5876709222793579, acc: 0.625, recall: 0.5172413793103449, precision: 0.6, f_beta: 0.5555555555555556\n",
            "train: step: 13411, loss: 0.44762948155403137, acc: 0.796875, recall: 0.8076923076923077, precision: 0.7241379310344828, f_beta: 0.7636363636363636\n",
            "train: step: 13412, loss: 0.5679997801780701, acc: 0.671875, recall: 0.6451612903225806, precision: 0.6666666666666666, f_beta: 0.6557377049180327\n",
            "train: step: 13413, loss: 0.4995114803314209, acc: 0.765625, recall: 0.7647058823529411, precision: 0.7878787878787878, f_beta: 0.7761194029850745\n",
            "train: step: 13414, loss: 0.7043745517730713, acc: 0.546875, recall: 0.52, precision: 0.43333333333333335, f_beta: 0.4727272727272728\n",
            "train: step: 13415, loss: 0.48476701974868774, acc: 0.765625, recall: 0.7272727272727273, precision: 0.64, f_beta: 0.6808510638297872\n",
            "train: step: 13416, loss: 0.5596581697463989, acc: 0.671875, recall: 0.6538461538461539, precision: 0.5862068965517241, f_beta: 0.6181818181818182\n",
            "train: step: 13417, loss: 0.7477530837059021, acc: 0.578125, recall: 0.45454545454545453, precision: 0.625, f_beta: 0.5263157894736842\n",
            "train: step: 13418, loss: 0.6438384652137756, acc: 0.59375, recall: 0.42857142857142855, precision: 0.5454545454545454, f_beta: 0.4799999999999999\n",
            "train: step: 13419, loss: 0.5293155312538147, acc: 0.71875, recall: 0.7241379310344828, precision: 0.6774193548387096, f_beta: 0.7\n",
            "train: step: 13420, loss: 0.5811755657196045, acc: 0.671875, recall: 0.5, precision: 0.6666666666666666, f_beta: 0.5714285714285715\n",
            "train: step: 13421, loss: 0.6386289596557617, acc: 0.71875, recall: 0.5652173913043478, precision: 0.6190476190476191, f_beta: 0.5909090909090909\n",
            "train: step: 13422, loss: 0.6611897945404053, acc: 0.578125, recall: 0.4666666666666667, precision: 0.56, f_beta: 0.509090909090909\n",
            "train: step: 13423, loss: 0.48275622725486755, acc: 0.765625, recall: 0.625, precision: 0.8695652173913043, f_beta: 0.7272727272727273\n",
            "train: step: 13424, loss: 0.6751813888549805, acc: 0.65625, recall: 0.5675675675675675, precision: 0.7777777777777778, f_beta: 0.65625\n",
            "train: step: 13425, loss: 0.6075769066810608, acc: 0.625, recall: 0.32142857142857145, precision: 0.6428571428571429, f_beta: 0.4285714285714286\n",
            "train: step: 13426, loss: 0.4773566722869873, acc: 0.703125, recall: 0.64, precision: 0.6153846153846154, f_beta: 0.6274509803921569\n",
            "train: step: 13427, loss: 0.5592530369758606, acc: 0.65625, recall: 0.5416666666666666, precision: 0.5416666666666666, f_beta: 0.5416666666666666\n",
            "train: step: 13428, loss: 0.5485692024230957, acc: 0.765625, recall: 0.6086956521739131, precision: 0.7, f_beta: 0.6511627906976744\n",
            "train: step: 13429, loss: 0.601459264755249, acc: 0.65625, recall: 0.3793103448275862, precision: 0.7333333333333333, f_beta: 0.5\n",
            "train: step: 13430, loss: 0.588054895401001, acc: 0.609375, recall: 0.38235294117647056, precision: 0.7647058823529411, f_beta: 0.5098039215686274\n",
            "train: step: 13431, loss: 0.582198441028595, acc: 0.640625, recall: 0.3157894736842105, precision: 0.375, f_beta: 0.34285714285714286\n",
            "train: step: 13432, loss: 0.6012143492698669, acc: 0.671875, recall: 0.41379310344827586, precision: 0.75, f_beta: 0.5333333333333333\n",
            "train: step: 13433, loss: 0.6018345355987549, acc: 0.59375, recall: 0.4117647058823529, precision: 0.7, f_beta: 0.5185185185185185\n",
            "train: step: 13434, loss: 0.601006031036377, acc: 0.640625, recall: 0.34615384615384615, precision: 0.6, f_beta: 0.43902439024390244\n",
            "train: step: 13435, loss: 0.5082377791404724, acc: 0.71875, recall: 0.5333333333333333, precision: 0.8, f_beta: 0.64\n",
            "train: step: 13436, loss: 0.5113928318023682, acc: 0.765625, recall: 0.5714285714285714, precision: 0.8421052631578947, f_beta: 0.6808510638297872\n",
            "train: step: 13437, loss: 0.5935497879981995, acc: 0.65625, recall: 0.3333333333333333, precision: 0.6923076923076923, f_beta: 0.45\n",
            "train: step: 13438, loss: 0.6867979764938354, acc: 0.59375, recall: 0.42857142857142855, precision: 0.5454545454545454, f_beta: 0.4799999999999999\n",
            "train: step: 13439, loss: 0.4692015051841736, acc: 0.765625, recall: 0.6818181818181818, precision: 0.6521739130434783, f_beta: 0.6666666666666666\n",
            "train: step: 13440, loss: 0.5220900774002075, acc: 0.796875, recall: 0.7037037037037037, precision: 0.7916666666666666, f_beta: 0.7450980392156864\n",
            "train: step: 13441, loss: 0.5206458568572998, acc: 0.703125, recall: 0.42857142857142855, precision: 0.5625, f_beta: 0.4864864864864864\n",
            "train: step: 13442, loss: 0.48470792174339294, acc: 0.828125, recall: 0.5714285714285714, precision: 0.8571428571428571, f_beta: 0.6857142857142857\n",
            "train: step: 13443, loss: 0.5633268356323242, acc: 0.703125, recall: 0.43478260869565216, precision: 0.625, f_beta: 0.5128205128205128\n",
            "train: step: 13444, loss: 0.5634735822677612, acc: 0.640625, recall: 0.4722222222222222, precision: 0.8095238095238095, f_beta: 0.5964912280701755\n",
            "train: step: 13445, loss: 0.5391947627067566, acc: 0.71875, recall: 0.5172413793103449, precision: 0.7894736842105263, f_beta: 0.625\n",
            "train: step: 13446, loss: 0.5827365517616272, acc: 0.625, recall: 0.4827586206896552, precision: 0.6086956521739131, f_beta: 0.5384615384615384\n",
            "train: step: 13447, loss: 0.5600284934043884, acc: 0.65625, recall: 0.36363636363636365, precision: 0.5, f_beta: 0.4210526315789474\n",
            "train: step: 13448, loss: 0.5579352974891663, acc: 0.6875, recall: 0.43478260869565216, precision: 0.5882352941176471, f_beta: 0.5\n",
            "train: step: 13449, loss: 0.5943747758865356, acc: 0.59375, recall: 0.43243243243243246, precision: 0.7619047619047619, f_beta: 0.5517241379310345\n",
            "train: step: 13450, loss: 0.5601484179496765, acc: 0.671875, recall: 0.5357142857142857, precision: 0.6521739130434783, f_beta: 0.5882352941176471\n",
            "train: step: 13451, loss: 0.5130207538604736, acc: 0.78125, recall: 0.6666666666666666, precision: 0.782608695652174, f_beta: 0.72\n",
            "train: step: 13452, loss: 0.542022705078125, acc: 0.71875, recall: 0.625, precision: 0.7692307692307693, f_beta: 0.6896551724137931\n",
            "train: step: 13453, loss: 0.564434289932251, acc: 0.640625, recall: 0.45161290322580644, precision: 0.7, f_beta: 0.5490196078431372\n",
            "train: step: 13454, loss: 0.6058317422866821, acc: 0.609375, recall: 0.375, precision: 0.7058823529411765, f_beta: 0.48979591836734687\n",
            "train: step: 13455, loss: 0.4911481440067291, acc: 0.734375, recall: 0.5714285714285714, precision: 0.7619047619047619, f_beta: 0.6530612244897959\n",
            "train: step: 13456, loss: 0.577110767364502, acc: 0.65625, recall: 0.52, precision: 0.5652173913043478, f_beta: 0.5416666666666667\n",
            "train: step: 13457, loss: 0.5022751688957214, acc: 0.765625, recall: 0.5714285714285714, precision: 0.8421052631578947, f_beta: 0.6808510638297872\n",
            "train: step: 13458, loss: 0.5197405219078064, acc: 0.75, recall: 0.7058823529411765, precision: 0.8, f_beta: 0.7500000000000001\n",
            "train: step: 13459, loss: 0.5211910605430603, acc: 0.6875, recall: 0.6, precision: 0.6923076923076923, f_beta: 0.6428571428571429\n",
            "train: step: 13460, loss: 0.5593459010124207, acc: 0.640625, recall: 0.4838709677419355, precision: 0.6818181818181818, f_beta: 0.5660377358490567\n",
            "train: step: 13461, loss: 0.5504137873649597, acc: 0.65625, recall: 0.5384615384615384, precision: 0.5833333333333334, f_beta: 0.5599999999999999\n",
            "train: step: 13462, loss: 0.6223125457763672, acc: 0.640625, recall: 0.5555555555555556, precision: 0.5769230769230769, f_beta: 0.5660377358490566\n",
            "train: step: 13463, loss: 0.5479587316513062, acc: 0.71875, recall: 0.6451612903225806, precision: 0.7407407407407407, f_beta: 0.689655172413793\n",
            "train: step: 13464, loss: 0.5119028091430664, acc: 0.75, recall: 0.6388888888888888, precision: 0.8846153846153846, f_beta: 0.7419354838709676\n",
            "train: step: 13465, loss: 0.5907338857650757, acc: 0.640625, recall: 0.5, precision: 0.6956521739130435, f_beta: 0.5818181818181818\n",
            "train: step: 13466, loss: 0.5605195760726929, acc: 0.6875, recall: 0.6774193548387096, precision: 0.6774193548387096, f_beta: 0.6774193548387096\n",
            "train: step: 13467, loss: 0.5755407810211182, acc: 0.671875, recall: 0.5882352941176471, precision: 0.7407407407407407, f_beta: 0.6557377049180328\n",
            "train: step: 13468, loss: 0.6059874296188354, acc: 0.640625, recall: 0.5428571428571428, precision: 0.7307692307692307, f_beta: 0.6229508196721311\n",
            "train: step: 13469, loss: 0.5177397727966309, acc: 0.71875, recall: 0.7037037037037037, precision: 0.6551724137931034, f_beta: 0.6785714285714286\n",
            "train: step: 13470, loss: 0.5303993821144104, acc: 0.71875, recall: 0.7142857142857143, precision: 0.6666666666666666, f_beta: 0.689655172413793\n",
            "train: step: 13471, loss: 0.6360931396484375, acc: 0.609375, recall: 0.5862068965517241, precision: 0.5666666666666667, f_beta: 0.576271186440678\n",
            "train: step: 13472, loss: 0.561894416809082, acc: 0.71875, recall: 0.625, precision: 0.625, f_beta: 0.625\n",
            "train: step: 13473, loss: 0.4810648262500763, acc: 0.8125, recall: 0.7575757575757576, precision: 0.8620689655172413, f_beta: 0.8064516129032258\n",
            "train: step: 13474, loss: 0.6394146084785461, acc: 0.640625, recall: 0.68, precision: 0.53125, f_beta: 0.5964912280701754\n",
            "train: step: 13475, loss: 0.4998871088027954, acc: 0.71875, recall: 0.7575757575757576, precision: 0.7142857142857143, f_beta: 0.7352941176470589\n",
            "train: step: 13476, loss: 0.5410110950469971, acc: 0.703125, recall: 0.7037037037037037, precision: 0.6333333333333333, f_beta: 0.6666666666666667\n",
            "train: step: 13477, loss: 0.5003290176391602, acc: 0.8125, recall: 0.7948717948717948, precision: 0.8857142857142857, f_beta: 0.8378378378378378\n",
            "train: step: 13478, loss: 0.5101277232170105, acc: 0.703125, recall: 0.7142857142857143, precision: 0.6451612903225806, f_beta: 0.6779661016949152\n",
            "train: step: 13479, loss: 0.6151124835014343, acc: 0.6875, recall: 0.625, precision: 0.5769230769230769, f_beta: 0.6\n",
            "train: step: 13480, loss: 0.5570595264434814, acc: 0.671875, recall: 0.5625, precision: 0.72, f_beta: 0.631578947368421\n",
            "train: step: 13481, loss: 0.5140185356140137, acc: 0.6875, recall: 0.6206896551724138, precision: 0.6666666666666666, f_beta: 0.6428571428571429\n",
            "train: step: 13482, loss: 0.5998201370239258, acc: 0.609375, recall: 0.5294117647058824, precision: 0.6666666666666666, f_beta: 0.5901639344262295\n",
            "train: step: 13483, loss: 0.5820749998092651, acc: 0.6875, recall: 0.5714285714285714, precision: 0.6666666666666666, f_beta: 0.6153846153846153\n",
            "train: step: 13484, loss: 0.5230321884155273, acc: 0.6875, recall: 0.5625, precision: 0.75, f_beta: 0.6428571428571429\n",
            "train: step: 13485, loss: 0.5673708915710449, acc: 0.671875, recall: 0.4666666666666667, precision: 0.7368421052631579, f_beta: 0.5714285714285714\n",
            "train: step: 13486, loss: 0.5437850952148438, acc: 0.765625, recall: 0.6060606060606061, precision: 0.9090909090909091, f_beta: 0.7272727272727273\n",
            "train: step: 13487, loss: 0.6350647211074829, acc: 0.625, recall: 0.4642857142857143, precision: 0.5909090909090909, f_beta: 0.52\n",
            "train: step: 13488, loss: 0.4912526309490204, acc: 0.703125, recall: 0.52, precision: 0.65, f_beta: 0.5777777777777778\n",
            "train: step: 13489, loss: 0.43860000371932983, acc: 0.78125, recall: 0.64, precision: 0.7619047619047619, f_beta: 0.6956521739130435\n",
            "train: step: 13490, loss: 0.5324369072914124, acc: 0.671875, recall: 0.6176470588235294, precision: 0.7241379310344828, f_beta: 0.6666666666666667\n",
            "train: step: 13491, loss: 0.5291306972503662, acc: 0.71875, recall: 0.5, precision: 0.7222222222222222, f_beta: 0.5909090909090908\n",
            "train: step: 13492, loss: 0.5399645566940308, acc: 0.734375, recall: 0.5925925925925926, precision: 0.7272727272727273, f_beta: 0.6530612244897959\n",
            "train: step: 13493, loss: 0.529360830783844, acc: 0.765625, recall: 0.7272727272727273, precision: 0.64, f_beta: 0.6808510638297872\n",
            "train: step: 13494, loss: 0.5400715470314026, acc: 0.734375, recall: 0.6, precision: 0.6818181818181818, f_beta: 0.6382978723404256\n",
            "train: step: 13495, loss: 0.5746504068374634, acc: 0.703125, recall: 0.6764705882352942, precision: 0.7419354838709677, f_beta: 0.7076923076923077\n",
            "train: step: 13496, loss: 0.5429304242134094, acc: 0.734375, recall: 0.5555555555555556, precision: 0.75, f_beta: 0.6382978723404256\n",
            "train: step: 13497, loss: 0.5629390478134155, acc: 0.625, recall: 0.48148148148148145, precision: 0.5652173913043478, f_beta: 0.52\n",
            "train: step: 13498, loss: 0.6006079912185669, acc: 0.65625, recall: 0.56, precision: 0.56, f_beta: 0.56\n",
            "train: step: 13499, loss: 0.5776337385177612, acc: 0.671875, recall: 0.5882352941176471, precision: 0.7407407407407407, f_beta: 0.6557377049180328\n",
            "train: step: 13500, loss: 0.6364964246749878, acc: 0.65625, recall: 0.45161290322580644, precision: 0.7368421052631579, f_beta: 0.56\n",
            "\n",
            "Evaluation:\n",
            "2022-12-03T03:20:02.318934, step: 13500, loss: 0.9964621918836679, acc: 0.3944096020761246,precision: 0.3944096020761246, recall: 1.0, f_beta: 0.5632332525573225\n",
            "Saved model checkpoint to /content/drive/MyDrive/Project/Bi-LSTM/model/my-model-13500\n",
            "\n",
            "train: step: 13501, loss: 0.4494270086288452, acc: 0.75, recall: 0.78125, precision: 0.7352941176470589, f_beta: 0.7575757575757576\n",
            "train: step: 13502, loss: 0.5937782526016235, acc: 0.625, recall: 0.5, precision: 0.5833333333333334, f_beta: 0.5384615384615384\n",
            "train: step: 13503, loss: 0.6044092178344727, acc: 0.5625, recall: 0.4444444444444444, precision: 0.6666666666666666, f_beta: 0.5333333333333333\n",
            "train: step: 13504, loss: 0.5684846639633179, acc: 0.6875, recall: 0.6129032258064516, precision: 0.7037037037037037, f_beta: 0.6551724137931035\n",
            "train: step: 13505, loss: 0.5386033058166504, acc: 0.703125, recall: 0.6, precision: 0.72, f_beta: 0.6545454545454547\n",
            "train: step: 13506, loss: 0.5373177528381348, acc: 0.78125, recall: 0.6666666666666666, precision: 0.6, f_beta: 0.631578947368421\n",
            "train: step: 13507, loss: 0.6057837009429932, acc: 0.625, recall: 0.4838709677419355, precision: 0.6521739130434783, f_beta: 0.5555555555555556\n",
            "train: step: 13508, loss: 0.47723668813705444, acc: 0.71875, recall: 0.5555555555555556, precision: 0.7142857142857143, f_beta: 0.6250000000000001\n",
            "train: step: 13509, loss: 0.5608299374580383, acc: 0.65625, recall: 0.36363636363636365, precision: 0.5, f_beta: 0.4210526315789474\n",
            "train: step: 13510, loss: 0.4843932092189789, acc: 0.703125, recall: 0.5384615384615384, precision: 0.6666666666666666, f_beta: 0.5957446808510638\n",
            "train: step: 13511, loss: 0.615433931350708, acc: 0.671875, recall: 0.5, precision: 0.6666666666666666, f_beta: 0.5714285714285715\n",
            "train: step: 13512, loss: 0.6075548529624939, acc: 0.578125, recall: 0.34285714285714286, precision: 0.75, f_beta: 0.47058823529411753\n",
            "train: step: 13513, loss: 0.6736316680908203, acc: 0.5625, recall: 0.35135135135135137, precision: 0.7647058823529411, f_beta: 0.48148148148148157\n",
            "train: step: 13514, loss: 0.4805835783481598, acc: 0.671875, recall: 0.5625, precision: 0.72, f_beta: 0.631578947368421\n",
            "train: step: 13515, loss: 0.5859736204147339, acc: 0.609375, recall: 0.3870967741935484, precision: 0.6666666666666666, f_beta: 0.4897959183673469\n",
            "train: step: 13516, loss: 0.6017558574676514, acc: 0.671875, recall: 0.5454545454545454, precision: 0.5217391304347826, f_beta: 0.5333333333333332\n",
            "train: step: 13517, loss: 0.5979834794998169, acc: 0.71875, recall: 0.5483870967741935, precision: 0.8095238095238095, f_beta: 0.6538461538461537\n",
            "train: step: 13518, loss: 0.5449108481407166, acc: 0.625, recall: 0.4666666666666667, precision: 0.6363636363636364, f_beta: 0.5384615384615385\n",
            "train: step: 13519, loss: 0.508633017539978, acc: 0.71875, recall: 0.6060606060606061, precision: 0.8, f_beta: 0.689655172413793\n",
            "train: step: 13520, loss: 0.6416357159614563, acc: 0.625, recall: 0.42857142857142855, precision: 0.6, f_beta: 0.5\n",
            "train: step: 13521, loss: 0.5890071392059326, acc: 0.6875, recall: 0.5833333333333334, precision: 0.5833333333333334, f_beta: 0.5833333333333334\n",
            "train: step: 13522, loss: 0.5240195393562317, acc: 0.71875, recall: 0.6538461538461539, precision: 0.6538461538461539, f_beta: 0.6538461538461539\n",
            "train: step: 13523, loss: 0.5504552125930786, acc: 0.65625, recall: 0.5161290322580645, precision: 0.6956521739130435, f_beta: 0.5925925925925926\n",
            "train: step: 13524, loss: 0.519275963306427, acc: 0.6875, recall: 0.46153846153846156, precision: 0.6666666666666666, f_beta: 0.5454545454545455\n",
            "train: step: 13525, loss: 0.5583786964416504, acc: 0.640625, recall: 0.38461538461538464, precision: 0.5882352941176471, f_beta: 0.46511627906976744\n",
            "train: step: 13526, loss: 0.5978512167930603, acc: 0.640625, recall: 0.36, precision: 0.5625, f_beta: 0.43902439024390244\n",
            "train: step: 13527, loss: 0.5050227046012878, acc: 0.71875, recall: 0.6666666666666666, precision: 0.7142857142857143, f_beta: 0.689655172413793\n",
            "train: step: 13528, loss: 0.6211515665054321, acc: 0.6875, recall: 0.4838709677419355, precision: 0.7894736842105263, f_beta: 0.6\n",
            "train: step: 13529, loss: 0.5155675411224365, acc: 0.65625, recall: 0.5, precision: 0.6363636363636364, f_beta: 0.56\n",
            "train: step: 13530, loss: 0.6360924243927002, acc: 0.625, recall: 0.39285714285714285, precision: 0.6111111111111112, f_beta: 0.4782608695652174\n",
            "train: step: 13531, loss: 0.5743310451507568, acc: 0.671875, recall: 0.5, precision: 0.8095238095238095, f_beta: 0.6181818181818182\n",
            "train: step: 13532, loss: 0.5968854427337646, acc: 0.671875, recall: 0.5, precision: 0.9047619047619048, f_beta: 0.6440677966101696\n",
            "train: step: 13533, loss: 0.6550664901733398, acc: 0.609375, recall: 0.3939393939393939, precision: 0.7222222222222222, f_beta: 0.5098039215686274\n",
            "train: step: 13534, loss: 0.49519723653793335, acc: 0.734375, recall: 0.5416666666666666, precision: 0.6842105263157895, f_beta: 0.6046511627906976\n",
            "train: step: 13535, loss: 0.6216346025466919, acc: 0.640625, recall: 0.48484848484848486, precision: 0.7272727272727273, f_beta: 0.5818181818181818\n",
            "train: step: 13536, loss: 0.5911369919776917, acc: 0.578125, recall: 0.46875, precision: 0.6, f_beta: 0.5263157894736842\n",
            "train: step: 13537, loss: 0.6405943632125854, acc: 0.6875, recall: 0.5517241379310345, precision: 0.6956521739130435, f_beta: 0.6153846153846154\n",
            "train: step: 13538, loss: 0.5408670902252197, acc: 0.671875, recall: 0.4857142857142857, precision: 0.85, f_beta: 0.6181818181818183\n",
            "train: step: 13539, loss: 0.46088600158691406, acc: 0.78125, recall: 0.6538461538461539, precision: 0.7727272727272727, f_beta: 0.7083333333333333\n",
            "train: step: 13540, loss: 0.6237562298774719, acc: 0.625, recall: 0.64, precision: 0.5161290322580645, f_beta: 0.5714285714285714\n",
            "train: step: 13541, loss: 0.5701459050178528, acc: 0.65625, recall: 0.48, precision: 0.5714285714285714, f_beta: 0.5217391304347826\n",
            "train: step: 13542, loss: 0.5819527506828308, acc: 0.625, recall: 0.5666666666666667, precision: 0.6071428571428571, f_beta: 0.5862068965517241\n",
            "train: step: 13543, loss: 0.6311877369880676, acc: 0.6875, recall: 0.68, precision: 0.5862068965517241, f_beta: 0.6296296296296295\n",
            "train: step: 13544, loss: 0.6653467416763306, acc: 0.640625, recall: 0.5483870967741935, precision: 0.6538461538461539, f_beta: 0.5964912280701755\n",
            "train: step: 13545, loss: 0.576920747756958, acc: 0.703125, recall: 0.5925925925925926, precision: 0.6666666666666666, f_beta: 0.627450980392157\n",
            "train: step: 13546, loss: 0.6377174258232117, acc: 0.578125, recall: 0.46875, precision: 0.6, f_beta: 0.5263157894736842\n",
            "train: step: 13547, loss: 0.5628676414489746, acc: 0.671875, recall: 0.5217391304347826, precision: 0.5454545454545454, f_beta: 0.5333333333333332\n",
            "train: step: 13548, loss: 0.5197256803512573, acc: 0.734375, recall: 0.625, precision: 0.6521739130434783, f_beta: 0.6382978723404256\n",
            "train: step: 13549, loss: 0.605064868927002, acc: 0.65625, recall: 0.5483870967741935, precision: 0.68, f_beta: 0.6071428571428571\n",
            "train: step: 13550, loss: 0.5430179834365845, acc: 0.625, recall: 0.391304347826087, precision: 0.47368421052631576, f_beta: 0.42857142857142855\n",
            "train: step: 13551, loss: 0.5474179983139038, acc: 0.734375, recall: 0.5909090909090909, precision: 0.6190476190476191, f_beta: 0.6046511627906977\n",
            "train: step: 13552, loss: 0.6082540154457092, acc: 0.734375, recall: 0.5333333333333333, precision: 0.8421052631578947, f_beta: 0.653061224489796\n",
            "train: step: 13553, loss: 0.4797907769680023, acc: 0.734375, recall: 0.5238095238095238, precision: 0.6111111111111112, f_beta: 0.5641025641025642\n",
            "train: step: 13554, loss: 0.4652698040008545, acc: 0.765625, recall: 0.6428571428571429, precision: 0.782608695652174, f_beta: 0.7058823529411765\n",
            "train: step: 13555, loss: 0.5713924169540405, acc: 0.640625, recall: 0.5238095238095238, precision: 0.88, f_beta: 0.6567164179104478\n",
            "train: step: 13556, loss: 0.49168917536735535, acc: 0.671875, recall: 0.5, precision: 0.6666666666666666, f_beta: 0.5714285714285715\n",
            "train: step: 13557, loss: 0.6339308023452759, acc: 0.53125, recall: 0.2727272727272727, precision: 0.6, f_beta: 0.37499999999999994\n",
            "train: step: 13558, loss: 0.5170961618423462, acc: 0.765625, recall: 0.5769230769230769, precision: 0.7894736842105263, f_beta: 0.6666666666666666\n",
            "train: step: 13559, loss: 0.5441770553588867, acc: 0.734375, recall: 0.6060606060606061, precision: 0.8333333333333334, f_beta: 0.7017543859649124\n",
            "train: step: 13560, loss: 0.5044200420379639, acc: 0.671875, recall: 0.4827586206896552, precision: 0.7, f_beta: 0.5714285714285714\n",
            "train: step: 13561, loss: 0.6418614983558655, acc: 0.65625, recall: 0.45454545454545453, precision: 0.7894736842105263, f_beta: 0.5769230769230769\n",
            "train: step: 13562, loss: 0.4776598811149597, acc: 0.75, recall: 0.625, precision: 0.6818181818181818, f_beta: 0.6521739130434783\n",
            "train: step: 13563, loss: 0.5063175559043884, acc: 0.734375, recall: 0.45454545454545453, precision: 0.6666666666666666, f_beta: 0.5405405405405405\n",
            "train: step: 13564, loss: 0.5230884552001953, acc: 0.734375, recall: 0.42857142857142855, precision: 0.6428571428571429, f_beta: 0.5142857142857143\n",
            "train: step: 13565, loss: 0.552882194519043, acc: 0.71875, recall: 0.5161290322580645, precision: 0.8421052631578947, f_beta: 0.6399999999999999\n",
            "train: step: 13566, loss: 0.6191401481628418, acc: 0.65625, recall: 0.4074074074074074, precision: 0.6470588235294118, f_beta: 0.5\n",
            "train: step: 13567, loss: 0.5902746319770813, acc: 0.625, recall: 0.5151515151515151, precision: 0.68, f_beta: 0.5862068965517241\n",
            "train: step: 13568, loss: 0.5224317312240601, acc: 0.609375, recall: 0.27586206896551724, precision: 0.6666666666666666, f_beta: 0.3902439024390244\n",
            "train: step: 13569, loss: 0.5821542143821716, acc: 0.625, recall: 0.5294117647058824, precision: 0.6923076923076923, f_beta: 0.5999999999999999\n",
            "train: step: 13570, loss: 0.4967947006225586, acc: 0.734375, recall: 0.4782608695652174, precision: 0.6875, f_beta: 0.5641025641025642\n",
            "train: step: 13571, loss: 0.5705984830856323, acc: 0.765625, recall: 0.6428571428571429, precision: 0.782608695652174, f_beta: 0.7058823529411765\n",
            "train: step: 13572, loss: 0.5136494636535645, acc: 0.71875, recall: 0.6818181818181818, precision: 0.5769230769230769, f_beta: 0.6249999999999999\n",
            "train: step: 13573, loss: 0.5632392168045044, acc: 0.65625, recall: 0.4782608695652174, precision: 0.5238095238095238, f_beta: 0.5\n",
            "train: step: 13574, loss: 0.5929485559463501, acc: 0.703125, recall: 0.5, precision: 0.6842105263157895, f_beta: 0.5777777777777778\n",
            "train: step: 13575, loss: 0.5562097430229187, acc: 0.6875, recall: 0.5588235294117647, precision: 0.7916666666666666, f_beta: 0.6551724137931034\n",
            "train: step: 13576, loss: 0.6015201210975647, acc: 0.609375, recall: 0.5, precision: 0.52, f_beta: 0.5098039215686274\n",
            "train: step: 13577, loss: 0.5793426632881165, acc: 0.71875, recall: 0.4642857142857143, precision: 0.8125, f_beta: 0.5909090909090908\n",
            "train: step: 13578, loss: 0.5135126709938049, acc: 0.703125, recall: 0.6, precision: 0.72, f_beta: 0.6545454545454547\n",
            "train: step: 13579, loss: 0.522706151008606, acc: 0.703125, recall: 0.6129032258064516, precision: 0.7307692307692307, f_beta: 0.6666666666666667\n",
            "train: step: 13580, loss: 0.5936300754547119, acc: 0.609375, recall: 0.6, precision: 0.5, f_beta: 0.5454545454545454\n",
            "train: step: 13581, loss: 0.7580265998840332, acc: 0.625, recall: 0.5161290322580645, precision: 0.64, f_beta: 0.5714285714285714\n",
            "train: step: 13582, loss: 0.5003366470336914, acc: 0.703125, recall: 0.5666666666666667, precision: 0.7391304347826086, f_beta: 0.6415094339622641\n",
            "train: step: 13583, loss: 0.5722312927246094, acc: 0.625, recall: 0.48717948717948717, precision: 0.8260869565217391, f_beta: 0.6129032258064516\n",
            "train: step: 13584, loss: 0.5422050952911377, acc: 0.71875, recall: 0.52, precision: 0.6842105263157895, f_beta: 0.5909090909090909\n",
            "train: step: 13585, loss: 0.4890132248401642, acc: 0.6875, recall: 0.4838709677419355, precision: 0.7894736842105263, f_beta: 0.6\n",
            "train: step: 13586, loss: 0.48203927278518677, acc: 0.796875, recall: 0.7058823529411765, precision: 0.8888888888888888, f_beta: 0.7868852459016393\n",
            "train: step: 13587, loss: 0.5416542887687683, acc: 0.6875, recall: 0.48148148148148145, precision: 0.6842105263157895, f_beta: 0.5652173913043478\n",
            "train: step: 13588, loss: 0.6486291885375977, acc: 0.640625, recall: 0.48148148148148145, precision: 0.5909090909090909, f_beta: 0.5306122448979591\n",
            "train: step: 13589, loss: 0.5638844966888428, acc: 0.734375, recall: 0.5909090909090909, precision: 0.6190476190476191, f_beta: 0.6046511627906977\n",
            "train: step: 13590, loss: 0.6289364695549011, acc: 0.671875, recall: 0.5454545454545454, precision: 0.75, f_beta: 0.631578947368421\n",
            "train: step: 13591, loss: 0.7001291513442993, acc: 0.640625, recall: 0.48484848484848486, precision: 0.7272727272727273, f_beta: 0.5818181818181818\n",
            "train: step: 13592, loss: 0.4756779670715332, acc: 0.75, recall: 0.6538461538461539, precision: 0.7083333333333334, f_beta: 0.68\n",
            "train: step: 13593, loss: 0.6365604400634766, acc: 0.625, recall: 0.43333333333333335, precision: 0.65, f_beta: 0.5199999999999999\n",
            "train: step: 13594, loss: 0.560675323009491, acc: 0.703125, recall: 0.48148148148148145, precision: 0.7222222222222222, f_beta: 0.5777777777777777\n",
            "train: step: 13595, loss: 0.5364949703216553, acc: 0.640625, recall: 0.3448275862068966, precision: 0.7142857142857143, f_beta: 0.46511627906976755\n",
            "train: step: 13596, loss: 0.5350667238235474, acc: 0.78125, recall: 0.7, precision: 0.6363636363636364, f_beta: 0.6666666666666666\n",
            "train: step: 13597, loss: 0.5504021048545837, acc: 0.703125, recall: 0.5675675675675675, precision: 0.875, f_beta: 0.6885245901639344\n",
            "train: step: 13598, loss: 0.5410717129707336, acc: 0.71875, recall: 0.44, precision: 0.7333333333333333, f_beta: 0.5499999999999999\n",
            "train: step: 13599, loss: 0.5423264503479004, acc: 0.703125, recall: 0.41379310344827586, precision: 0.8571428571428571, f_beta: 0.5581395348837208\n",
            "train: step: 13600, loss: 0.5111403465270996, acc: 0.65625, recall: 0.36666666666666664, precision: 0.7857142857142857, f_beta: 0.5\n",
            "\n",
            "Evaluation:\n",
            "2022-12-03T03:24:52.350039, step: 13600, loss: 1.0409480054898246, acc: 0.3388840830449827,precision: 0.3388840830449827, recall: 1.0, f_beta: 0.5033754734049628\n",
            "Saved model checkpoint to /content/drive/MyDrive/Project/Bi-LSTM/model/my-model-13600\n",
            "\n",
            "train: step: 13601, loss: 0.587690532207489, acc: 0.765625, recall: 0.625, precision: 0.7142857142857143, f_beta: 0.6666666666666666\n",
            "train: step: 13602, loss: 0.5941078066825867, acc: 0.609375, recall: 0.34375, precision: 0.7333333333333333, f_beta: 0.4680851063829787\n",
            "train: step: 13603, loss: 0.5910642147064209, acc: 0.6875, recall: 0.4166666666666667, precision: 0.625, f_beta: 0.5\n",
            "train: step: 13604, loss: 0.5793876051902771, acc: 0.703125, recall: 0.5142857142857142, precision: 0.9, f_beta: 0.6545454545454545\n",
            "train: step: 13605, loss: 0.6047695875167847, acc: 0.65625, recall: 0.4827586206896552, precision: 0.6666666666666666, f_beta: 0.56\n",
            "train: step: 13606, loss: 0.5812576413154602, acc: 0.65625, recall: 0.43478260869565216, precision: 0.5263157894736842, f_beta: 0.47619047619047616\n",
            "train: step: 13607, loss: 0.547870934009552, acc: 0.671875, recall: 0.4166666666666667, precision: 0.5882352941176471, f_beta: 0.48780487804878053\n",
            "train: step: 13608, loss: 0.5702241063117981, acc: 0.6875, recall: 0.53125, precision: 0.7727272727272727, f_beta: 0.6296296296296297\n",
            "train: step: 13609, loss: 0.5400998592376709, acc: 0.6875, recall: 0.53125, precision: 0.7727272727272727, f_beta: 0.6296296296296297\n",
            "train: step: 13610, loss: 0.6463500261306763, acc: 0.609375, recall: 0.4722222222222222, precision: 0.7391304347826086, f_beta: 0.5762711864406781\n",
            "train: step: 13611, loss: 0.6384937763214111, acc: 0.5625, recall: 0.32142857142857145, precision: 0.5, f_beta: 0.391304347826087\n",
            "train: step: 13612, loss: 0.5725037455558777, acc: 0.71875, recall: 0.6666666666666666, precision: 0.7142857142857143, f_beta: 0.689655172413793\n",
            "train: step: 13613, loss: 0.5579967498779297, acc: 0.703125, recall: 0.5384615384615384, precision: 0.6666666666666666, f_beta: 0.5957446808510638\n",
            "train: step: 13614, loss: 0.6432864665985107, acc: 0.609375, recall: 0.45454545454545453, precision: 0.6818181818181818, f_beta: 0.5454545454545454\n",
            "train: step: 13615, loss: 0.5412807464599609, acc: 0.6875, recall: 0.5161290322580645, precision: 0.7619047619047619, f_beta: 0.6153846153846153\n",
            "train: step: 13616, loss: 0.5804102420806885, acc: 0.734375, recall: 0.6, precision: 0.5714285714285714, f_beta: 0.5853658536585366\n",
            "train: step: 13617, loss: 0.5306782722473145, acc: 0.65625, recall: 0.5142857142857142, precision: 0.782608695652174, f_beta: 0.6206896551724138\n",
            "train: step: 13618, loss: 0.4980662763118744, acc: 0.75, recall: 0.6, precision: 0.7142857142857143, f_beta: 0.6521739130434783\n",
            "train: step: 13619, loss: 0.5396329760551453, acc: 0.71875, recall: 0.6129032258064516, precision: 0.76, f_beta: 0.6785714285714285\n",
            "train: step: 13620, loss: 0.5623359680175781, acc: 0.671875, recall: 0.48, precision: 0.6, f_beta: 0.5333333333333332\n",
            "train: step: 13621, loss: 0.5691688656806946, acc: 0.6875, recall: 0.5714285714285714, precision: 0.5217391304347826, f_beta: 0.5454545454545454\n",
            "train: step: 13622, loss: 0.5704885721206665, acc: 0.671875, recall: 0.5263157894736842, precision: 0.8695652173913043, f_beta: 0.6557377049180327\n",
            "train: step: 13623, loss: 0.4756926894187927, acc: 0.765625, recall: 0.7586206896551724, precision: 0.7333333333333333, f_beta: 0.7457627118644068\n",
            "train: step: 13624, loss: 0.5551944375038147, acc: 0.71875, recall: 0.5384615384615384, precision: 0.7, f_beta: 0.608695652173913\n",
            "train: step: 13625, loss: 0.6049346923828125, acc: 0.703125, recall: 0.43478260869565216, precision: 0.625, f_beta: 0.5128205128205128\n",
            "train: step: 13626, loss: 0.5578532218933105, acc: 0.71875, recall: 0.59375, precision: 0.7916666666666666, f_beta: 0.6785714285714286\n",
            "train: step: 13627, loss: 0.5119305849075317, acc: 0.734375, recall: 0.5172413793103449, precision: 0.8333333333333334, f_beta: 0.6382978723404256\n",
            "train: step: 13628, loss: 0.6385447978973389, acc: 0.625, recall: 0.45454545454545453, precision: 0.7142857142857143, f_beta: 0.5555555555555556\n",
            "train: step: 13629, loss: 0.4976786971092224, acc: 0.8125, recall: 0.6875, precision: 0.6111111111111112, f_beta: 0.6470588235294118\n",
            "train: step: 13630, loss: 0.5380997061729431, acc: 0.625, recall: 0.4358974358974359, precision: 0.8947368421052632, f_beta: 0.5862068965517242\n",
            "train: step: 13631, loss: 0.4749149680137634, acc: 0.734375, recall: 0.56, precision: 0.7, f_beta: 0.6222222222222222\n",
            "train: step: 13632, loss: 0.6084210872650146, acc: 0.671875, recall: 0.5666666666666667, precision: 0.68, f_beta: 0.6181818181818183\n",
            "train: step: 13633, loss: 0.401079922914505, acc: 0.765625, recall: 0.3157894736842105, precision: 0.75, f_beta: 0.44444444444444436\n",
            "train: step: 13634, loss: 0.523141622543335, acc: 0.734375, recall: 0.59375, precision: 0.8260869565217391, f_beta: 0.6909090909090908\n",
            "train: step: 13635, loss: 0.5311949253082275, acc: 0.765625, recall: 0.5652173913043478, precision: 0.7222222222222222, f_beta: 0.6341463414634146\n",
            "train: step: 13636, loss: 0.5491151809692383, acc: 0.671875, recall: 0.3333333333333333, precision: 0.5, f_beta: 0.4\n",
            "train: step: 13637, loss: 0.5026542544364929, acc: 0.734375, recall: 0.6206896551724138, precision: 0.75, f_beta: 0.679245283018868\n",
            "train: step: 13638, loss: 0.578217625617981, acc: 0.734375, recall: 0.4, precision: 0.6153846153846154, f_beta: 0.4848484848484849\n",
            "train: step: 13639, loss: 0.5525754690170288, acc: 0.71875, recall: 0.42857142857142855, precision: 0.6, f_beta: 0.5\n",
            "train: step: 13640, loss: 0.5824921727180481, acc: 0.640625, recall: 0.40625, precision: 0.7647058823529411, f_beta: 0.5306122448979592\n",
            "train: step: 13641, loss: 0.5589820742607117, acc: 0.734375, recall: 0.53125, precision: 0.8947368421052632, f_beta: 0.6666666666666666\n",
            "train: step: 13642, loss: 0.6087093353271484, acc: 0.65625, recall: 0.5, precision: 0.5909090909090909, f_beta: 0.5416666666666667\n",
            "train: step: 13643, loss: 0.564520001411438, acc: 0.6875, recall: 0.45714285714285713, precision: 0.9411764705882353, f_beta: 0.6153846153846154\n",
            "train: step: 13644, loss: 0.5634722709655762, acc: 0.625, recall: 0.45161290322580644, precision: 0.6666666666666666, f_beta: 0.5384615384615384\n",
            "train: step: 13645, loss: 0.48624566197395325, acc: 0.75, recall: 0.5555555555555556, precision: 0.7894736842105263, f_beta: 0.6521739130434783\n",
            "train: step: 13646, loss: 0.6210823655128479, acc: 0.640625, recall: 0.3793103448275862, precision: 0.6875, f_beta: 0.4888888888888889\n",
            "train: step: 13647, loss: 0.46136999130249023, acc: 0.78125, recall: 0.6666666666666666, precision: 0.6666666666666666, f_beta: 0.6666666666666666\n",
            "train: step: 13648, loss: 0.4562971591949463, acc: 0.828125, recall: 0.7037037037037037, precision: 0.8636363636363636, f_beta: 0.7755102040816326\n",
            "train: step: 13649, loss: 0.5580886006355286, acc: 0.65625, recall: 0.4594594594594595, precision: 0.8947368421052632, f_beta: 0.6071428571428572\n",
            "train: step: 13650, loss: 0.6349570751190186, acc: 0.71875, recall: 0.5483870967741935, precision: 0.8095238095238095, f_beta: 0.6538461538461537\n",
            "train: step: 13651, loss: 0.5739865303039551, acc: 0.625, recall: 0.5483870967741935, precision: 0.6296296296296297, f_beta: 0.5862068965517241\n",
            "train: step: 13652, loss: 0.5644233822822571, acc: 0.59375, recall: 0.5517241379310345, precision: 0.5517241379310345, f_beta: 0.5517241379310345\n",
            "train: step: 13653, loss: 0.5944663286209106, acc: 0.625, recall: 0.5666666666666667, precision: 0.6071428571428571, f_beta: 0.5862068965517241\n",
            "train: step: 13654, loss: 0.5651873350143433, acc: 0.71875, recall: 0.6111111111111112, precision: 0.8461538461538461, f_beta: 0.7096774193548387\n",
            "train: step: 13655, loss: 0.4866011142730713, acc: 0.765625, recall: 0.7241379310344828, precision: 0.75, f_beta: 0.736842105263158\n",
            "train: step: 13656, loss: 0.5123072862625122, acc: 0.75, recall: 0.6956521739130435, precision: 0.64, f_beta: 0.6666666666666666\n",
            "train: step: 13657, loss: 0.5554078817367554, acc: 0.71875, recall: 0.6296296296296297, precision: 0.68, f_beta: 0.6538461538461539\n",
            "train: step: 13658, loss: 0.7047349810600281, acc: 0.515625, recall: 0.5357142857142857, precision: 0.45454545454545453, f_beta: 0.49180327868852464\n",
            "train: step: 13659, loss: 0.57127845287323, acc: 0.703125, recall: 0.7941176470588235, precision: 0.6923076923076923, f_beta: 0.7397260273972601\n",
            "train: step: 13660, loss: 0.562603235244751, acc: 0.671875, recall: 0.64, precision: 0.5714285714285714, f_beta: 0.6037735849056605\n",
            "train: step: 13661, loss: 0.5500863790512085, acc: 0.65625, recall: 0.7027027027027027, precision: 0.7027027027027027, f_beta: 0.7027027027027027\n",
            "train: step: 13662, loss: 0.5172093510627747, acc: 0.765625, recall: 0.8181818181818182, precision: 0.75, f_beta: 0.7826086956521738\n",
            "train: step: 13663, loss: 0.5468542575836182, acc: 0.65625, recall: 0.6470588235294118, precision: 0.6875, f_beta: 0.6666666666666667\n",
            "train: step: 13664, loss: 0.5303472876548767, acc: 0.734375, recall: 0.7096774193548387, precision: 0.7333333333333333, f_beta: 0.7213114754098361\n",
            "train: step: 13665, loss: 0.5473807454109192, acc: 0.703125, recall: 0.7241379310344828, precision: 0.65625, f_beta: 0.6885245901639345\n",
            "train: step: 13666, loss: 0.5275262594223022, acc: 0.671875, recall: 0.5882352941176471, precision: 0.7407407407407407, f_beta: 0.6557377049180328\n",
            "train: step: 13667, loss: 0.6955889463424683, acc: 0.5625, recall: 0.5833333333333334, precision: 0.4375, f_beta: 0.5\n",
            "train: step: 13668, loss: 0.6334843635559082, acc: 0.609375, recall: 0.6666666666666666, precision: 0.5294117647058824, f_beta: 0.5901639344262295\n",
            "train: step: 13669, loss: 0.6478590965270996, acc: 0.578125, recall: 0.6551724137931034, precision: 0.5277777777777778, f_beta: 0.5846153846153845\n",
            "train: step: 13670, loss: 0.5790673494338989, acc: 0.65625, recall: 0.5714285714285714, precision: 0.7407407407407407, f_beta: 0.6451612903225806\n",
            "train: step: 13671, loss: 0.5213255286216736, acc: 0.8125, recall: 0.8928571428571429, precision: 0.7352941176470589, f_beta: 0.806451612903226\n",
            "train: step: 13672, loss: 0.6391130685806274, acc: 0.640625, recall: 0.59375, precision: 0.6551724137931034, f_beta: 0.6229508196721311\n",
            "train: step: 13673, loss: 0.6016792058944702, acc: 0.65625, recall: 0.6470588235294118, precision: 0.6875, f_beta: 0.6666666666666667\n",
            "train: step: 13674, loss: 0.4919910132884979, acc: 0.796875, recall: 0.7142857142857143, precision: 0.8, f_beta: 0.7547169811320756\n",
            "train: step: 13675, loss: 0.5637611150741577, acc: 0.71875, recall: 0.75, precision: 0.7058823529411765, f_beta: 0.7272727272727272\n",
            "train: step: 13676, loss: 0.552521288394928, acc: 0.71875, recall: 0.6153846153846154, precision: 0.6666666666666666, f_beta: 0.64\n",
            "train: step: 13677, loss: 0.6417343616485596, acc: 0.65625, recall: 0.5151515151515151, precision: 0.7391304347826086, f_beta: 0.6071428571428571\n",
            "train: step: 13678, loss: 0.6335209608078003, acc: 0.6875, recall: 0.5161290322580645, precision: 0.7619047619047619, f_beta: 0.6153846153846153\n",
            "train: step: 13679, loss: 0.6336860656738281, acc: 0.765625, recall: 0.5384615384615384, precision: 0.8235294117647058, f_beta: 0.6511627906976744\n",
            "train: step: 13680, loss: 0.6148843765258789, acc: 0.71875, recall: 0.5357142857142857, precision: 0.75, f_beta: 0.6250000000000001\n",
            "train: step: 13681, loss: 0.5361655950546265, acc: 0.703125, recall: 0.4230769230769231, precision: 0.7333333333333333, f_beta: 0.5365853658536585\n",
            "train: step: 13682, loss: 0.5904840230941772, acc: 0.65625, recall: 0.6, precision: 0.46153846153846156, f_beta: 0.5217391304347826\n",
            "train: step: 13683, loss: 0.5644476413726807, acc: 0.734375, recall: 0.6296296296296297, precision: 0.7083333333333334, f_beta: 0.6666666666666667\n",
            "train: step: 13684, loss: 0.6417496800422668, acc: 0.671875, recall: 0.5909090909090909, precision: 0.52, f_beta: 0.5531914893617023\n",
            "train: step: 13685, loss: 0.5103583335876465, acc: 0.78125, recall: 0.6666666666666666, precision: 0.6, f_beta: 0.631578947368421\n",
            "train: step: 13686, loss: 0.4769490361213684, acc: 0.78125, recall: 0.7142857142857143, precision: 0.6521739130434783, f_beta: 0.6818181818181819\n",
            "train: step: 13687, loss: 0.602458655834198, acc: 0.609375, recall: 0.3448275862068966, precision: 0.625, f_beta: 0.4444444444444445\n",
            "train: step: 13688, loss: 0.49172070622444153, acc: 0.75, recall: 0.7391304347826086, precision: 0.6296296296296297, f_beta: 0.68\n",
            "train: step: 13689, loss: 0.5024667382240295, acc: 0.71875, recall: 0.5714285714285714, precision: 0.7272727272727273, f_beta: 0.64\n",
            "train: step: 13690, loss: 0.469789057970047, acc: 0.796875, recall: 0.6206896551724138, precision: 0.9, f_beta: 0.7346938775510204\n",
            "train: step: 13691, loss: 0.5381089448928833, acc: 0.65625, recall: 0.3870967741935484, precision: 0.8, f_beta: 0.5217391304347827\n",
            "train: step: 13692, loss: 0.5502855777740479, acc: 0.671875, recall: 0.43333333333333335, precision: 0.7647058823529411, f_beta: 0.5531914893617021\n",
            "train: step: 13693, loss: 0.567857027053833, acc: 0.640625, recall: 0.4, precision: 0.875, f_beta: 0.5490196078431373\n",
            "train: step: 13694, loss: 0.5869930982589722, acc: 0.671875, recall: 0.43333333333333335, precision: 0.7647058823529411, f_beta: 0.5531914893617021\n",
            "train: step: 13695, loss: 0.592073917388916, acc: 0.71875, recall: 0.5172413793103449, precision: 0.7894736842105263, f_beta: 0.625\n",
            "train: step: 13696, loss: 0.5521448254585266, acc: 0.671875, recall: 0.45161290322580644, precision: 0.7777777777777778, f_beta: 0.5714285714285714\n",
            "train: step: 13697, loss: 0.6314622163772583, acc: 0.625, recall: 0.3333333333333333, precision: 0.5, f_beta: 0.4\n",
            "train: step: 13698, loss: 0.5743898749351501, acc: 0.609375, recall: 0.3103448275862069, precision: 0.6428571428571429, f_beta: 0.4186046511627907\n",
            "train: step: 13699, loss: 0.6718767881393433, acc: 0.625, recall: 0.5142857142857142, precision: 0.72, f_beta: 0.6\n",
            "train: step: 13700, loss: 0.5900863409042358, acc: 0.640625, recall: 0.41379310344827586, precision: 0.6666666666666666, f_beta: 0.5106382978723404\n",
            "\n",
            "Evaluation:\n",
            "2022-12-03T03:29:46.091531, step: 13700, loss: 1.0357787646224341, acc: 0.33926254325259514,precision: 0.33926254325259514, recall: 1.0, f_beta: 0.5037000135234679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py:1066: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model checkpoint to /content/drive/MyDrive/Project/Bi-LSTM/model/my-model-13700\n",
            "\n",
            "train: step: 13701, loss: 0.5855984687805176, acc: 0.71875, recall: 0.6666666666666666, precision: 0.7142857142857143, f_beta: 0.689655172413793\n",
            "train: step: 13702, loss: 0.48769986629486084, acc: 0.75, recall: 0.5, precision: 0.75, f_beta: 0.6\n",
            "train: step: 13703, loss: 0.5835672616958618, acc: 0.71875, recall: 0.5483870967741935, precision: 0.8095238095238095, f_beta: 0.6538461538461537\n",
            "train: step: 13704, loss: 0.566433310508728, acc: 0.625, recall: 0.5142857142857142, precision: 0.72, f_beta: 0.6\n",
            "train: step: 13705, loss: 0.611892580986023, acc: 0.734375, recall: 0.5666666666666667, precision: 0.8095238095238095, f_beta: 0.6666666666666666\n",
            "train: step: 13706, loss: 0.5776333808898926, acc: 0.640625, recall: 0.48, precision: 0.5454545454545454, f_beta: 0.5106382978723404\n",
            "train: step: 13707, loss: 0.5132969617843628, acc: 0.71875, recall: 0.6785714285714286, precision: 0.6785714285714286, f_beta: 0.6785714285714286\n",
            "train: step: 13708, loss: 0.6758630275726318, acc: 0.578125, recall: 0.34615384615384615, precision: 0.47368421052631576, f_beta: 0.39999999999999997\n",
            "train: step: 13709, loss: 0.5030632615089417, acc: 0.796875, recall: 0.6785714285714286, precision: 0.8260869565217391, f_beta: 0.7450980392156864\n",
            "train: step: 13710, loss: 0.5665817856788635, acc: 0.703125, recall: 0.5925925925925926, precision: 0.6666666666666666, f_beta: 0.627450980392157\n",
            "train: step: 13711, loss: 0.5275339484214783, acc: 0.765625, recall: 0.7083333333333334, precision: 0.68, f_beta: 0.6938775510204083\n",
            "train: step: 13712, loss: 0.5523329377174377, acc: 0.734375, recall: 0.72, precision: 0.6428571428571429, f_beta: 0.6792452830188679\n",
            "train: step: 13713, loss: 0.6674647927284241, acc: 0.671875, recall: 0.5833333333333334, precision: 0.56, f_beta: 0.5714285714285714\n",
            "train: step: 13714, loss: 0.6385823488235474, acc: 0.5625, recall: 0.5, precision: 0.39285714285714285, f_beta: 0.44\n",
            "train: step: 13715, loss: 0.5686978697776794, acc: 0.6875, recall: 0.56, precision: 0.6086956521739131, f_beta: 0.5833333333333334\n",
            "train: step: 13716, loss: 0.5067712068557739, acc: 0.734375, recall: 0.5416666666666666, precision: 0.6842105263157895, f_beta: 0.6046511627906976\n",
            "train: step: 13717, loss: 0.5642704963684082, acc: 0.640625, recall: 0.36666666666666664, precision: 0.7333333333333333, f_beta: 0.4888888888888889\n",
            "train: step: 13718, loss: 0.5700641870498657, acc: 0.65625, recall: 0.5454545454545454, precision: 0.72, f_beta: 0.6206896551724138\n",
            "train: step: 13719, loss: 0.5619329214096069, acc: 0.734375, recall: 0.6, precision: 0.782608695652174, f_beta: 0.6792452830188679\n",
            "train: step: 13720, loss: 0.4568893015384674, acc: 0.78125, recall: 0.6071428571428571, precision: 0.85, f_beta: 0.7083333333333333\n",
            "train: step: 13721, loss: 0.6212607026100159, acc: 0.65625, recall: 0.5, precision: 0.8636363636363636, f_beta: 0.6333333333333333\n",
            "train: step: 13722, loss: 0.5425010919570923, acc: 0.734375, recall: 0.4444444444444444, precision: 0.8571428571428571, f_beta: 0.5853658536585367\n",
            "train: step: 13723, loss: 0.4862399697303772, acc: 0.765625, recall: 0.46153846153846156, precision: 0.9230769230769231, f_beta: 0.6153846153846155\n",
            "train: step: 13724, loss: 0.5741428732872009, acc: 0.671875, recall: 0.48484848484848486, precision: 0.8, f_beta: 0.6037735849056605\n",
            "train: step: 13725, loss: 0.5399360060691833, acc: 0.671875, recall: 0.5, precision: 0.5238095238095238, f_beta: 0.5116279069767442\n",
            "train: step: 13726, loss: 0.553605318069458, acc: 0.6875, recall: 0.42857142857142855, precision: 0.75, f_beta: 0.5454545454545454\n",
            "train: step: 13727, loss: 0.6285015344619751, acc: 0.625, recall: 0.41379310344827586, precision: 0.631578947368421, f_beta: 0.5\n",
            "train: step: 13728, loss: 0.6560465693473816, acc: 0.671875, recall: 0.23809523809523808, precision: 0.5, f_beta: 0.3225806451612903\n",
            "train: step: 13729, loss: 0.5179516077041626, acc: 0.75, recall: 0.65625, precision: 0.8076923076923077, f_beta: 0.7241379310344829\n",
            "train: step: 13730, loss: 0.5998578667640686, acc: 0.71875, recall: 0.4642857142857143, precision: 0.8125, f_beta: 0.5909090909090908\n",
            "train: step: 13731, loss: 0.6229006052017212, acc: 0.625, recall: 0.41379310344827586, precision: 0.631578947368421, f_beta: 0.5\n",
            "train: step: 13732, loss: 0.678525447845459, acc: 0.625, recall: 0.3870967741935484, precision: 0.7058823529411765, f_beta: 0.5\n",
            "train: step: 13733, loss: 0.48491591215133667, acc: 0.8125, recall: 0.64, precision: 0.8421052631578947, f_beta: 0.7272727272727272\n",
            "train: step: 13734, loss: 0.583077609539032, acc: 0.671875, recall: 0.5, precision: 0.7142857142857143, f_beta: 0.588235294117647\n",
            "train: step: 13735, loss: 0.5133918523788452, acc: 0.78125, recall: 0.6, precision: 0.7894736842105263, f_beta: 0.6818181818181819\n",
            "train: step: 13736, loss: 0.5315057635307312, acc: 0.75, recall: 0.56, precision: 0.7368421052631579, f_beta: 0.6363636363636364\n",
            "train: step: 13737, loss: 0.6349973678588867, acc: 0.625, recall: 0.45161290322580644, precision: 0.6666666666666666, f_beta: 0.5384615384615384\n",
            "train: step: 13738, loss: 0.567447304725647, acc: 0.625, recall: 0.5454545454545454, precision: 0.6666666666666666, f_beta: 0.6\n",
            "train: step: 13739, loss: 0.5623972415924072, acc: 0.625, recall: 0.35714285714285715, precision: 0.625, f_beta: 0.45454545454545453\n",
            "train: step: 13740, loss: 0.6529341340065002, acc: 0.578125, recall: 0.35, precision: 0.3333333333333333, f_beta: 0.3414634146341463\n",
            "train: step: 13741, loss: 0.5519874095916748, acc: 0.75, recall: 0.6153846153846154, precision: 0.7272727272727273, f_beta: 0.6666666666666667\n",
            "train: step: 13742, loss: 0.6733349561691284, acc: 0.609375, recall: 0.4827586206896552, precision: 0.5833333333333334, f_beta: 0.5283018867924529\n",
            "train: step: 13743, loss: 0.5142302513122559, acc: 0.6875, recall: 0.48484848484848486, precision: 0.8421052631578947, f_beta: 0.6153846153846154\n",
            "train: step: 13744, loss: 0.5923886299133301, acc: 0.71875, recall: 0.5714285714285714, precision: 0.7272727272727273, f_beta: 0.64\n",
            "train: step: 13745, loss: 0.5654597282409668, acc: 0.625, recall: 0.4782608695652174, precision: 0.4782608695652174, f_beta: 0.4782608695652174\n",
            "train: step: 13746, loss: 0.6303245425224304, acc: 0.65625, recall: 0.5555555555555556, precision: 0.6, f_beta: 0.576923076923077\n",
            "train: step: 13747, loss: 0.5554403066635132, acc: 0.6875, recall: 0.34615384615384615, precision: 0.75, f_beta: 0.4736842105263157\n",
            "train: step: 13748, loss: 0.6261475086212158, acc: 0.71875, recall: 0.46153846153846156, precision: 0.75, f_beta: 0.5714285714285714\n",
            "train: step: 13749, loss: 0.5694777965545654, acc: 0.625, recall: 0.2903225806451613, precision: 0.8181818181818182, f_beta: 0.4285714285714286\n",
            "train: step: 13750, loss: 0.5321928262710571, acc: 0.6875, recall: 0.5454545454545454, precision: 0.5454545454545454, f_beta: 0.5454545454545454\n",
            "train: step: 13751, loss: 0.58343505859375, acc: 0.65625, recall: 0.48148148148148145, precision: 0.6190476190476191, f_beta: 0.5416666666666666\n",
            "train: step: 13752, loss: 0.6070573925971985, acc: 0.59375, recall: 0.4827586206896552, precision: 0.56, f_beta: 0.5185185185185185\n",
            "train: step: 13753, loss: 0.5890698432922363, acc: 0.65625, recall: 0.5294117647058824, precision: 0.75, f_beta: 0.6206896551724139\n",
            "train: step: 13754, loss: 0.6072840690612793, acc: 0.59375, recall: 0.4375, precision: 0.6363636363636364, f_beta: 0.5185185185185185\n",
            "train: step: 13755, loss: 0.5340706706047058, acc: 0.703125, recall: 0.4482758620689655, precision: 0.8125, f_beta: 0.5777777777777777\n"
          ]
        }
      ],
      "source": [
        "trainReviews = data.trainReviews\n",
        "trainLabels = data.trainLabels\n",
        "evalReviews = data.evalReviews\n",
        "evalLabels = data.evalLabels\n",
        "\n",
        "wordEmbedding = data.wordEmbedding\n",
        "labelList = data.labelList\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    session_conf = tf.compat.v1.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
        "    session_conf.gpu_options.allow_growth=True\n",
        "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  \n",
        "    sess = tf.compat.v1.Session(config=session_conf)\n",
        "    \n",
        "    with sess.as_default():\n",
        "        lstm = BiLSTM(config, wordEmbedding)\n",
        "        \n",
        "        globalStep = tf.compat.v1.train.get_or_create_global_step()\n",
        "        optimizer = tf.compat.v1.train.AdamOptimizer(config.training.learningRate)\n",
        "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
        "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
        "        \n",
        "        gradSummaries = []\n",
        "        for g, v in gradsAndVars:\n",
        "            if g is not None:\n",
        "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
        "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
        "        \n",
        "        outDir = os.path.abspath(\"/content/drive/MyDrive/Project/Bi-LSTM/summarys\")\n",
        "        print(\"Writing to {}\\n\".format(outDir))\n",
        "        \n",
        "        lossSummary = tf.compat.v1.summary.scalar(\"loss\", lstm.loss)\n",
        "        summaryOp = tf.compat.v1.summary.merge_all()\n",
        "\n",
        "        \n",
        "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
        "        trainSummaryWriter = tf.compat.v1.summary.FileWriter(trainSummaryDir, sess.graph)\n",
        "        \n",
        "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
        "        evalSummaryWriter = tf.compat.v1.summary.FileWriter(evalSummaryDir, sess.graph)\n",
        "        \n",
        "        \n",
        "        saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables(), max_to_keep=5)\n",
        "        saver = tf.compat.v1.train.import_meta_graph('/content/drive/MyDrive/Project/Bi-LSTM/model/my-model-13100.meta')\n",
        "        saver.restore(sess\n",
        "                      , tf.compat.v1.train.latest_checkpoint('/content/drive/MyDrive/Project/Bi-LSTM/model'))\n",
        "        \n",
        "        graph = tf.compat.v1.get_default_graph()\n",
        "        inputX = graph.get_tensor_by_name(\"inputX:0\")\n",
        "        inputY = graph.get_tensor_by_name(\"inputY:0\")\n",
        "        dropoutKeepProb = graph.get_tensor_by_name(\"dropoutKeepProb:0\")\n",
        "\n",
        "        feed_dict = {inputX, inputY,dropoutKeepProb}\n",
        "\n",
        "        def trainStep(batchX, batchY):\n",
        "            \"\"\"\n",
        "            训练函数\n",
        "            \"\"\"   \n",
        "            feed_dict = {\n",
        "              lstm.inputX: batchX,\n",
        "              lstm.inputY: batchY,\n",
        "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
        "            }\n",
        "            _, summary, step, loss, predictions = sess.run([trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions], feed_dict)\n",
        "            \n",
        "            timeStr = datetime.datetime.now().isoformat()\n",
        "            \n",
        "            if config.numClasses == 1:\n",
        "                acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
        "                \n",
        "            elif config.numClasses > 1:\n",
        "                acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY,\n",
        "                                                              labels=labelList)\n",
        "                \n",
        "            trainSummaryWriter.add_summary(summary, step)\n",
        "            \n",
        "            return loss, acc, prec, recall, f_beta\n",
        "\n",
        "        def devStep(batchX, batchY):\n",
        "            feed_dict = {\n",
        "              lstm.inputX: batchX,\n",
        "              lstm.inputY: batchY,\n",
        "              lstm.dropoutKeepProb: 1.0\n",
        "            }\n",
        "            summary, step, loss, predictions = sess.run(\n",
        "                [summaryOp, globalStep, lstm.loss, lstm.predictions],\n",
        "                feed_dict)\n",
        "            \n",
        "            if config.numClasses == 1:\n",
        "            \n",
        "                acc, precision, recall, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
        "            elif config.numClasses > 1:\n",
        "                acc, precision, recall, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY, labels=labelList)\n",
        "            \n",
        "            evalSummaryWriter.add_summary(summary, step)\n",
        "            \n",
        "            return loss, acc, precision, recall, f_beta\n",
        "        \n",
        "        for i in range(config.training.epoches):\n",
        "            print(\"start training model\")\n",
        "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
        "                loss, acc, prec, recall, f_beta = trainStep(batchTrain[0], batchTrain[1])\n",
        "                \n",
        "                \n",
        "                currentStep = tf.compat.v1.train.global_step(sess, globalStep) \n",
        "                print(\"train: step: {}, loss: {}, acc: {}, recall: {}, precision: {}, f_beta: {}\".format(\n",
        "                    currentStep, loss, acc, recall, prec, f_beta))\n",
        "                if currentStep % config.training.evaluateEvery == 0:\n",
        "                    print(\"\\nEvaluation:\")\n",
        "                    \n",
        "                    losses = []\n",
        "                    accs = []\n",
        "                    f_betas = []\n",
        "                    precisions = []\n",
        "                    recalls = []\n",
        "                    \n",
        "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
        "                        loss, acc, precision, recall, f_beta = devStep(batchEval[0], batchEval[1])\n",
        "                        losses.append(loss)\n",
        "                        accs.append(acc)\n",
        "                        f_betas.append(f_beta)\n",
        "                        precisions.append(precision)\n",
        "                        recalls.append(recall)\n",
        "                        \n",
        "                    time_str = datetime.datetime.now().isoformat()\n",
        "                    print(\"{}, step: {}, loss: {}, acc: {},precision: {}, recall: {}, f_beta: {}\".format(time_str, currentStep, mean(losses), \n",
        "                                                                                                       mean(accs), mean(precisions),\n",
        "                                                                                                       mean(recalls), mean(f_betas)))\n",
        "                    \n",
        "                if currentStep % config.training.checkpointEvery == 0:\n",
        "                    path = saver.save(sess, \"/content/drive/MyDrive/Project/Bi-LSTM/model/my-model\", global_step=currentStep)\n",
        "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
        "                    \n",
        "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
        "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
        "\n",
        "        outputs = {\"predictions\": tf.saved_model.utils.build_tensor_info(lstm.predictions)}\n",
        "\n",
        "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
        "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
        "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
        "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
        "                                            signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
        "\n",
        "        builder.save()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2RJEc4FiIlVY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}