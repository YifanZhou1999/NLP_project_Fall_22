{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drh-27y0P52u"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNzkjH7-P0HD"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYzjfZHxP69V"
      },
      "outputs": [],
      "source": [
        "#foldername= \"/content/drive/My Drive/nlpproject/bertsequenceclassification/\"\n",
        "foldername= \"/content/drive/My Drive/Project/extractedfeaturewithbert/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZguGQVMP8J9"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorWithPadding\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from datasets import load_metric\n",
        "import datetime\n",
        "from torch import nn\n",
        "from transformers import AutoConfig\n",
        "from transformers import AutoModel\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWp9GiTPP9UU"
      },
      "outputs": [],
      "source": [
        "class CFG():\n",
        "  max_len = 512\n",
        "  #model = \"bert-base-uncased\"\n",
        "  model = \"distilbert-base-uncased\"\n",
        "  lr=1e-4\n",
        "  min_lr=1e-10\n",
        "  batch_size=16\n",
        "  epoch=1000\n",
        "  embedding_dim=512\n",
        "  hidden_dim=256\n",
        "  drop=0.15\n",
        "cfg=CFG()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ-y7ugzP-vV"
      },
      "outputs": [],
      "source": [
        "class Ds(Dataset):\n",
        "    def __init__(self, path, tokenizer, i, max_token_len=cfg.max_len):\n",
        "        self.df = pd.read_csv(path).dropna()\n",
        "        self.tokenizer=tokenizer\n",
        "        self.max_token_len=max_token_len\n",
        "        self.labelstrdicts={1:\"ESTJ\", 0:\"INFP\"}\n",
        "        self.loc=i #EI at index 0 in mbti\n",
        "    def __len__(self):\n",
        "        return (len(self.df))\n",
        "    def __getitem__(self, index):\n",
        "        item=self.df.iloc[index]\n",
        "        text=item[\"post\"]\n",
        "        t=item[\"type\"]\n",
        "        labels=self.str2label(t)\n",
        "        try:\n",
        "          tokens=self.tokenizer(text,return_tensors=\"pt\", truncation=True, max_length=self.max_token_len, padding=\"max_length\")\n",
        "        except:\n",
        "          print(text)\n",
        "          quit()\n",
        "        return {\"input_ids\": torch.squeeze(tokens.input_ids), \"attention_mask\":torch.squeeze(tokens.attention_mask), \"labels\":labels}\n",
        "    def str2label(self, string):\n",
        "        letter=string[self.loc]\n",
        "        if letter in \"ESTJ\":\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    def label2str(self, label):\n",
        "        return self.labelstrdicts[label][self.loc]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqOkdZKoQy7u"
      },
      "outputs": [],
      "source": [
        "#from transformers import BertTokenizer, BertForSequenceClassification\n",
        "#tokenizer=BertTokenizer.from_pretrained(cfg.model)\n",
        "#bertmodel=AutoModel.from_pretrained(cfg.model)\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "tokenizer=BertTokenizer.from_pretrained(cfg.model)\n",
        "bertmodel=AutoModel.from_pretrained(cfg.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6fRjEa4R6MP"
      },
      "outputs": [],
      "source": [
        "path=foldername+\"dataset2.csv\"\n",
        "#print(dataset[0])\n",
        "data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "def getdl(ds):\n",
        "    total_len=len(ds)\n",
        "    train_len=int(len(ds)*0.9)\n",
        "    val_len=int((total_len-train_len)/2)\n",
        "    test_len=total_len-train_len-val_len\n",
        "    [train_ds, val_ds, test_ds]=torch.utils.data.random_split(ds, [train_len, val_len, test_len])\n",
        "    #return (training dataloader, validation dataloader, test dataloader)\n",
        "    return len(train_ds), len(val_ds), len(test_ds), DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=data_collator), DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=data_collator), DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=data_collator)\n",
        "    #return DataLoader(ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=data_collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBbUTmveQIo8"
      },
      "outputs": [],
      "source": [
        "dataset_EI=Ds(path, tokenizer, i=0)\n",
        "len_train, len_val, len_test, train_dl_EI,val_dl_EI, test_dl_EI=getdl(dataset_EI)\n",
        "\n",
        "#dataset_SN=Ds_SN(path, tokenizer)\n",
        "#dl_SN=getdl(dataset_SN)\n",
        "\n",
        "#dataset_TF=Ds_TF(path, tokenizer)\n",
        "#dl_TF=getdl(dataset_TF)\n",
        "\n",
        "#dataset_JP=Ds_JP(path, tokenizer)\n",
        "#dl_JP=getdl(dataset_JP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg6u14WvPMec"
      },
      "outputs": [],
      "source": [
        "l=[x['labels'] for x in dataset_EI]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df_IPSO8PPxG"
      },
      "outputs": [],
      "source": [
        "num_one=torch.count_nonzero(torch.tensor(l), dim=0)\n",
        "num_zero=len(l)-num_one\n",
        "\n",
        "print(num_zero)\n",
        "print(num_one)\n",
        "LABEL_RATIO=torch.tensor([num_one/num_one, num_zero/num_one]).to(\"cuda\")\n",
        "print(LABEL_RATIO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvg1SblfCXOb"
      },
      "outputs": [],
      "source": [
        "def evaluate(labels, outputs):\n",
        "  answers=(torch.argmax(outputs, dim=1))\n",
        "  allcorrect=torch.sum(answers==labels)\n",
        "  return allcorrect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMYkoAk0QCC7"
      },
      "outputs": [],
      "source": [
        "class myModel(nn.Module):\n",
        "    def __init__(self, CFG, dropoutrate, model):\n",
        "        super().__init__()\n",
        "        self.CFG = CFG\n",
        "        self.relu = nn.ReLU()\n",
        "        self.t = nn.Tanh()\n",
        "        #self.bert = model\n",
        "        self.distilbert = model\n",
        "        self.fc1 = nn.Linear(768, 64)\n",
        "        self.fc2 = nn.Linear(64, 16)\n",
        "        #self.fc3 = nn.Linear(64, 16)\n",
        "        #self.fc4 = nn.Linear(64, 16)\n",
        "        #self.fc5 = nn.Linear(512, 256)\n",
        "        #self.fc6 = nn.Linear(256, 128)\n",
        "        #self.fc7 = nn.Linear(128, 64)\n",
        "        #self.fc8 = nn.Linear(64, 32)\n",
        "        #self.fc9 = nn.Linear(32, 16)\n",
        "        #self.fc10 = nn.Linear(16, 2)\n",
        "        self.fc_final = nn.Linear(16, 2)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropoutrate, inplace=False)\n",
        "    \n",
        "    def forward(self, batch):\n",
        "\n",
        "        #output=self.bert(input_ids=batch[\"input_ids\"],attention_mask=batch[\"attention_mask\"]).last_hidden_state[:,0]\n",
        "        output=self.distilbert(input_ids=batch[\"input_ids\"],attention_mask=batch[\"attention_mask\"]).last_hidden_state[:,0]\n",
        "\n",
        "        output = self.dropout(output)\n",
        "\n",
        "\n",
        "\n",
        "        output = self.fc1(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        output = self.fc2(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        #output = self.fc3(output)\n",
        "        #output = self.relu(output)\n",
        "        #output = self.dropout(output)\n",
        "\n",
        "        #output = self.fc4(output)\n",
        "        #output = self.t(output)\n",
        "        #output = self.dropout(output)\n",
        "\n",
        "        #output = self.fc5(output)\n",
        "        #output = self.t(output)\n",
        "        #output = self.dropout(output)\n",
        "\n",
        "        #output = self.fc6(output)\n",
        "        #output = self.t(output)\n",
        "        #output = self.dropout(output)\n",
        "\n",
        "        #output = self.fc7(output)\n",
        "        #output = self.t(output)\n",
        "        #output = self.dropout(output)\n",
        "\n",
        "        #output = self.fc8(output)\n",
        "        #output = self.t(output)\n",
        "        #output = self.dropout(output)\n",
        "\n",
        "        #output = self.fc9(output)\n",
        "        #output = self.t(output)\n",
        "        #output = self.dropout(output)\n",
        "\n",
        "        output = self.fc_final(output)\n",
        "        #output = self.sig(output)\n",
        "\n",
        "        #output = torch.squeeze(output)\n",
        "        return output #2 number between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y5dE0rKAUF8"
      },
      "outputs": [],
      "source": [
        "from torch.optim import lr_scheduler\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch import optim\n",
        "from torch.optim import Adam\n",
        "from tqdm.notebook import tqdm\n",
        "def train(train_ds, eval_ds, model, epochs, cfg, type, lr, loss=None):\n",
        "    if torch.cuda.is_available():  \n",
        "        dev = \"cuda:0\" \n",
        "    else:  \n",
        "        dev = \"cpu\" \n",
        "    device = torch.device(dev)\n",
        "    model = model.to(device)\n",
        "\n",
        "    #weights=torch.tensor([1., 3.]).cuda()\n",
        "    #criterion = nn.MSELoss()\n",
        "    criterion = CrossEntropyLoss(weight=LABEL_RATIO)\n",
        "\n",
        "    criterion.to(device)\n",
        "    #criterion = loss\n",
        "    \n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "    #optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=500, eta_min=1e-15)\n",
        "    #scheduler= optim.lr_scheduler.ExponentialLR(optimizer, 0.99, last_epoch=- 1, verbose=False)\n",
        "        \n",
        "    totalevalloss=0\n",
        "    totalcorrect=0\n",
        "    totaldata=0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for batch in eval_ds:\n",
        "            batch.to(device)\n",
        "            blabels=batch[\"labels\"]\n",
        "            outputs=model(batch)\n",
        "            eloss=criterion(outputs, blabels).item()\n",
        "            totalevalloss+=eloss\n",
        "            totalcorrect+=evaluate(blabels, outputs)\n",
        "            totaldata+=len(blabels)\n",
        "    totalcorrect_rate=(totalcorrect/(totaldata))\n",
        "    print(\"probability that our prediction of \", type, \" is correct: \", totalcorrect_rate)\n",
        "    #print(f'Initial Val Loss: {totalevalloss / len(eval_ds): .3f} ' ) \n",
        "    print(f'Initial Val Loss: {totalevalloss / len(eval_ds): .3f} | current lr: {scheduler.get_last_lr()}' ) \n",
        "    \n",
        "    for e in range(epochs):\n",
        "        totaltrainloss=0\n",
        "        totaltraincorrect=0\n",
        "        totaltraindata=0\n",
        "        for i,batch in enumerate(train_ds):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            batch.to(device)\n",
        "            labels=batch[\"labels\"]\n",
        "            outputs=model(batch)\n",
        "            bloss=criterion(outputs, labels)\n",
        "            bloss.backward()\n",
        "            optimizer.step()\n",
        "            totaltrainloss+=bloss.item()\n",
        "            totaltraincorrect+=evaluate(labels, outputs)\n",
        "            totaltraindata+=len(labels)\n",
        "        scheduler.step()\n",
        "        totalevalloss=0\n",
        "        totalcorrect=0\n",
        "        totaldata=0\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for batch in eval_ds:\n",
        "                batch.to(device)\n",
        "                blabels=batch[\"labels\"]\n",
        "                outputs=model(batch)\n",
        "                eloss=criterion(outputs, blabels).item()\n",
        "                totalevalloss+=eloss\n",
        "                totalcorrect+=evaluate(blabels, outputs)\n",
        "                totaldata+=len(blabels)\n",
        "        totalcorrect_rate=(totalcorrect/(totaldata))\n",
        "        totaltraincorrect_rate = (totaltraincorrect/(totaltraindata))\n",
        "        print(\"probability that our prediction of \", type, \" is correct: \", totalcorrect_rate)\n",
        "        print(\"probability that our prediction of \", type, \" is correct in training dataset: \", totaltraincorrect_rate)\n",
        "        #print(f'Epoch: {e+ 1} | Train Loss: {totaltrainloss / len(train_ds): .8f} | Val Loss: {totalevalloss / len(eval_ds): .3f}' ) \n",
        "        print(f'Epoch: {e+ 1} | Train Loss: {totaltrainloss / len(train_ds): .8f} | Val Loss: {totalevalloss / len(eval_ds): .8f} | current lr: {scheduler.get_last_lr()}' ) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pu6p8M6UAluq",
        "outputId": "bfecd44a-fbcc-442b-af39-a98a433aa15c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "probability that our prediction of  EI  is correct:  tensor(0.7662, device='cuda:0')\n",
            "Initial Val Loss:  0.696 | current lr: [0.0001]\n"
          ]
        }
      ],
      "source": [
        "#model=myModel(cfg, 0.2, bertmodel)\n",
        "model=myModel(cfg, 0.2, distilbertmodel)\n",
        "train(train_dl_EI, val_dl_EI, model, epochs=cfg.epoch, cfg=cfg, type=\"EI\", lr=1e-4, loss=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"new\")"
      ],
      "metadata": {
        "id": "rnMjX-2KsVJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "140LyAeeVWKX"
      },
      "outputs": [],
      "source": [
        "#m=BertForSequenceClassification.from_pretrained(cfg.model)\n",
        "m=DistilBertForSequenceClassification.from_pretrained(cfg.model)\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1YOSIsKWAEN"
      },
      "outputs": [],
      "source": [
        "#bertmodel\n",
        "distilbertmodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eh48V0cXgnxy"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i,batch in enumerate(val_dl_EI):\n",
        "            batch.to(\"cuda\")\n",
        "            batch_labels=batch[\"labels\"]\n",
        "            outputs=model(input_ids=batch[\"input_ids\"]).logits\n",
        "            print(outputs)\n",
        "            x=torch.argmax(outputs,dim=1)\n",
        "            print(x)\n",
        "            print(batch_labels)\n",
        "            print(torch.sum(x==batch_labels))\n",
        "            print((x==batch_labels).sum())\n",
        "            print(evaluate(batch_labels, outputs))\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWSOyn2fiTTP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}